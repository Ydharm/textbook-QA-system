{
    "0": "",
    "1": "",
    "2": "Data\tScience\tfrom\tScratch\n\nJoel\tGrus",
    "3": "",
    "4": "Data\tScience\tfrom\tScratch\n\nby\tJoel\tGrus\n\nCopyright\t\u00a9\t2015\tO\u2019Reilly\tMedia.\tAll\trights\treserved.\n\nPrinted\tin\tthe\tUnited\tStates\tof\tAmerica.\n\nPublished\tby\tO\u2019Reilly\tMedia,\tInc.,\t1005\tGravenstein\tHighway\tNorth,\tSebastopol,\tCA\n95472.\n\nO\u2019Reilly\tbooks\tmay\tbe\tpurchased\tfor\teducational,\tbusiness,\tor\tsales\tpromotional\tuse.\nOnline\teditions\tare\talso\tavailable\tfor\tmost\ttitles\t(http://safaribooksonline.com).\tFor\tmore\ninformation,\tcontact\tour\tcorporate/institutional\tsales\tdepartment:\t800-998-9938\tor\ncorporate@oreilly.com.\n\nEditor:\tMarie\tBeaugureau\n\nProduction\tEditor:\tMelanie\tYarbrough\n\nCopyeditor:\tNan\tReinhardt\n\nProofreader:\tEileen\tCohen\n\nIndexer:\tEllen\tTroutman-Zaig\n\nInterior\tDesigner:\tDavid\tFutato\n\nCover\tDesigner:\tKaren\tMontgomery\n\nIllustrator:\tRebecca\tDemarest\n\nApril\t2015:\tFirst\tEdition",
    "5": "Revision\tHistory\tfor\tthe\tFirst\tEdition\n\n2015-04-10:\tFirst\tRelease\n\nSee\thttp://oreilly.com/catalog/errata.csp?isbn=9781491901427\tfor\trelease\tdetails.\n\nThe\tO\u2019Reilly\tlogo\tis\ta\tregistered\ttrademark\tof\tO\u2019Reilly\tMedia,\tInc.\tData\tScience\tfrom\nScratch,\tthe\tcover\timage\tof\ta\tRock\tPtarmigan,\tand\trelated\ttrade\tdress\tare\ttrademarks\tof\nO\u2019Reilly\tMedia,\tInc.\n\nWhile\tthe\tpublisher\tand\tthe\tauthor\thave\tused\tgood\tfaith\tefforts\tto\tensure\tthat\tthe\ninformation\tand\tinstructions\tcontained\tin\tthis\twork\tare\taccurate,\tthe\tpublisher\tand\tthe\nauthor\tdisclaim\tall\tresponsibility\tfor\terrors\tor\tomissions,\tincluding\twithout\tlimitation\nresponsibility\tfor\tdamages\tresulting\tfrom\tthe\tuse\tof\tor\treliance\ton\tthis\twork.\tUse\tof\tthe\ninformation\tand\tinstructions\tcontained\tin\tthis\twork\tis\tat\tyour\town\trisk.\tIf\tany\tcode\nsamples\tor\tother\ttechnology\tthis\twork\tcontains\tor\tdescribes\tis\tsubject\tto\topen\tsource\nlicenses\tor\tthe\tintellectual\tproperty\trights\tof\tothers,\tit\tis\tyour\tresponsibility\tto\tensure\tthat\nyour\tuse\tthereof\tcomplies\twith\tsuch\tlicenses\tand/or\trights.\n\n978-1-491-90142-7\n\n[LSI]",
    "6": "",
    "7": "Preface",
    "8": "Data\tScience\n\nData\tscientist\thas\tbeen\tcalled\t\u201cthe\tsexiest\tjob\tof\tthe\t21st\tcentury,\u201d\tpresumably\tby\nsomeone\twho\thas\tnever\tvisited\ta\tfire\tstation.\tNonetheless,\tdata\tscience\tis\ta\thot\tand\ngrowing\tfield,\tand\tit\tdoesn\u2019t\ttake\ta\tgreat\tdeal\tof\tsleuthing\tto\tfind\tanalysts\tbreathlessly\nprognosticating\tthat\tover\tthe\tnext\t10\tyears,\twe\u2019ll\tneed\tbillions\tand\tbillions\tmore\tdata\nscientists\tthan\twe\tcurrently\thave.\n\nBut\twhat\tis\tdata\tscience?\tAfter\tall,\twe\tcan\u2019t\tproduce\tdata\tscientists\tif\twe\tdon\u2019t\tknow\twhat\ndata\tscience\tis.\tAccording\tto\ta\tVenn\tdiagram\tthat\tis\tsomewhat\tfamous\tin\tthe\tindustry,\tdata\nscience\tlies\tat\tthe\tintersection\tof:\n\nHacking\tskills\n\nMath\tand\tstatistics\tknowledge\n\nSubstantive\texpertise\n\nAlthough\tI\toriginally\tintended\tto\twrite\ta\tbook\tcovering\tall\tthree,\tI\tquickly\trealized\tthat\ta\nthorough\ttreatment\tof\t\u201csubstantive\texpertise\u201d\twould\trequire\ttens\tof\tthousands\tof\tpages.\tAt\nthat\tpoint,\tI\tdecided\tto\tfocus\ton\tthe\tfirst\ttwo.\tMy\tgoal\tis\tto\thelp\tyou\tdevelop\tthe\thacking\nskills\tthat\tyou\u2019ll\tneed\tto\tget\tstarted\tdoing\tdata\tscience.\tAnd\tmy\tgoal\tis\tto\thelp\tyou\tget\ncomfortable\twith\tthe\tmathematics\tand\tstatistics\tthat\tare\tat\tthe\tcore\tof\tdata\tscience.\n\nThis\tis\ta\tsomewhat\theavy\taspiration\tfor\ta\tbook.\tThe\tbest\tway\tto\tlearn\thacking\tskills\tis\tby\nhacking\ton\tthings.\tBy\treading\tthis\tbook,\tyou\twill\tget\ta\tgood\tunderstanding\tof\tthe\tway\tI\nhack\ton\tthings,\twhich\tmay\tnot\tnecessarily\tbe\tthe\tbest\tway\tfor\tyou\tto\thack\ton\tthings.\tYou\nwill\tget\ta\tgood\tunderstanding\tof\tsome\tof\tthe\ttools\tI\tuse,\twhich\twill\tnot\tnecessarily\tbe\tthe\nbest\ttools\tfor\tyou\tto\tuse.\tYou\twill\tget\ta\tgood\tunderstanding\tof\tthe\tway\tI\tapproach\tdata\nproblems,\twhich\tmay\tnot\tnecessarily\tbe\tthe\tbest\tway\tfor\tyou\tto\tapproach\tdata\tproblems.\nThe\tintent\t(and\tthe\thope)\tis\tthat\tmy\texamples\twill\tinspire\tyou\ttry\tthings\tyour\town\tway.\nAll\tthe\tcode\tand\tdata\tfrom\tthe\tbook\tis\tavailable\ton\tGitHub\tto\tget\tyou\tstarted.\n\nSimilarly,\tthe\tbest\tway\tto\tlearn\tmathematics\tis\tby\tdoing\tmathematics.\tThis\tis\temphatically\nnot\ta\tmath\tbook,\tand\tfor\tthe\tmost\tpart,\twe\twon\u2019t\tbe\t\u201cdoing\tmathematics.\u201d\tHowever,\tyou\ncan\u2019t\treally\tdo\tdata\tscience\twithout\tsome\tunderstanding\tof\tprobability\tand\tstatistics\tand\nlinear\talgebra.\tThis\tmeans\tthat,\twhere\tappropriate,\twe\twill\tdive\tinto\tmathematical\nequations,\tmathematical\tintuition,\tmathematical\taxioms,\tand\tcartoon\tversions\tof\tbig\nmathematical\tideas.\tI\thope\tthat\tyou\twon\u2019t\tbe\tafraid\tto\tdive\tin\twith\tme.\n\nThroughout\tit\tall,\tI\talso\thope\tto\tgive\tyou\ta\tsense\tthat\tplaying\twith\tdata\tis\tfun,\tbecause,\nwell,\tplaying\twith\tdata\tis\tfun!\t(Especially\tcompared\tto\tsome\tof\tthe\talternatives,\tlike\ttax\npreparation\tor\tcoal\tmining.)",
    "9": "From\tScratch\n\nThere\tare\tlots\tand\tlots\tof\tdata\tscience\tlibraries,\tframeworks,\tmodules,\tand\ttoolkits\tthat\nefficiently\timplement\tthe\tmost\tcommon\t(as\twell\tas\tthe\tleast\tcommon)\tdata\tscience\nalgorithms\tand\ttechniques.\tIf\tyou\tbecome\ta\tdata\tscientist,\tyou\twill\tbecome\tintimately\nfamiliar\twith\tNumPy,\twith\tscikit-learn,\twith\tpandas,\tand\twith\ta\tpanoply\tof\tother\tlibraries.\nThey\tare\tgreat\tfor\tdoing\tdata\tscience.\tBut\tthey\tare\talso\ta\tgood\tway\tto\tstart\tdoing\tdata\nscience\twithout\tactually\tunderstanding\tdata\tscience.\n\nIn\tthis\tbook,\twe\twill\tbe\tapproaching\tdata\tscience\tfrom\tscratch.\tThat\tmeans\twe\u2019ll\tbe\nbuilding\ttools\tand\timplementing\talgorithms\tby\thand\tin\torder\tto\tbetter\tunderstand\tthem.\tI\nput\ta\tlot\tof\tthought\tinto\tcreating\timplementations\tand\texamples\tthat\tare\tclear,\twell-\ncommented,\tand\treadable.\tIn\tmost\tcases,\tthe\ttools\twe\tbuild\twill\tbe\tilluminating\tbut\nimpractical.\tThey\twill\twork\twell\ton\tsmall\ttoy\tdata\tsets\tbut\tfall\tover\ton\t\u201cweb\tscale\u201d\tones.\n\nThroughout\tthe\tbook,\tI\twill\tpoint\tyou\tto\tlibraries\tyou\tmight\tuse\tto\tapply\tthese\ttechniques\nto\tlarger\tdata\tsets.\tBut\twe\twon\u2019t\tbe\tusing\tthem\there.\n\nThere\tis\ta\thealthy\tdebate\traging\tover\tthe\tbest\tlanguage\tfor\tlearning\tdata\tscience.\tMany\npeople\tbelieve\tit\u2019s\tthe\tstatistical\tprogramming\tlanguage\tR.\t(We\tcall\tthose\tpeople\twrong.)\nA\tfew\tpeople\tsuggest\tJava\tor\tScala.\tHowever,\tin\tmy\topinion,\tPython\tis\tthe\tobvious\nchoice.\n\nPython\thas\tseveral\tfeatures\tthat\tmake\tit\twell\tsuited\tfor\tlearning\t(and\tdoing)\tdata\tscience:\n\nIt\u2019s\tfree.\n\nIt\u2019s\trelatively\tsimple\tto\tcode\tin\t(and,\tin\tparticular,\tto\tunderstand).\n\nIt\thas\tlots\tof\tuseful\tdata\tscience\u2013related\tlibraries.\n\nI\tam\thesitant\tto\tcall\tPython\tmy\tfavorite\tprogramming\tlanguage.\tThere\tare\tother\tlanguages\nI\tfind\tmore\tpleasant,\tbetter-designed,\tor\tjust\tmore\tfun\tto\tcode\tin.\tAnd\tyet\tpretty\tmuch\nevery\ttime\tI\tstart\ta\tnew\tdata\tscience\tproject,\tI\tend\tup\tusing\tPython.\tEvery\ttime\tI\tneed\tto\nquickly\tprototype\tsomething\tthat\tjust\tworks,\tI\tend\tup\tusing\tPython.\tAnd\tevery\ttime\tI\nwant\tto\tdemonstrate\tdata\tscience\tconcepts\tin\ta\tclear,\teasy-to-understand\tway,\tI\tend\tup\nusing\tPython.\tAccordingly,\tthis\tbook\tuses\tPython.\n\nThe\tgoal\tof\tthis\tbook\tis\tnot\tto\tteach\tyou\tPython.\t(Although\tit\tis\tnearly\tcertain\tthat\tby\nreading\tthis\tbook\tyou\twill\tlearn\tsome\tPython.)\tI\u2019ll\ttake\tyou\tthrough\ta\tchapter-long\tcrash\ncourse\tthat\thighlights\tthe\tfeatures\tthat\tare\tmost\timportant\tfor\tour\tpurposes,\tbut\tif\tyou\nknow\tnothing\tabout\tprogramming\tin\tPython\t(or\tabout\tprogramming\tat\tall)\tthen\tyou\tmight\nwant\tto\tsupplement\tthis\tbook\twith\tsome\tsort\tof\t\u201cPython\tfor\tBeginners\u201d\ttutorial.\n\nThe\tremainder\tof\tour\tintroduction\tto\tdata\tscience\twill\ttake\tthis\tsame\tapproach\u2009\t\u2014\t\u2009going\ninto\tdetail\twhere\tgoing\tinto\tdetail\tseems\tcrucial\tor\tilluminating,\tat\tother\ttimes\tleaving\ndetails\tfor\tyou\tto\tfigure\tout\tyourself\t(or\tlook\tup\ton\tWikipedia).",
    "10": "Over\tthe\tyears,\tI\u2019ve\ttrained\ta\tnumber\tof\tdata\tscientists.\tWhile\tnot\tall\tof\tthem\thave\tgone\non\tto\tbecome\tworld-changing\tdata\tninja\trockstars,\tI\u2019ve\tleft\tthem\tall\tbetter\tdata\tscientists\nthan\tI\tfound\tthem.\tAnd\tI\u2019ve\tgrown\tto\tbelieve\tthat\tanyone\twho\thas\tsome\tamount\tof\nmathematical\taptitude\tand\tsome\tamount\tof\tprogramming\tskill\thas\tthe\tnecessary\traw\nmaterials\tto\tdo\tdata\tscience.\tAll\tshe\tneeds\tis\tan\tinquisitive\tmind,\ta\twillingness\tto\twork\nhard,\tand\tthis\tbook.\tHence\tthis\tbook.",
    "11": "Conventions\tUsed\tin\tThis\tBook\n\nThe\tfollowing\ttypographical\tconventions\tare\tused\tin\tthis\tbook:\n\nItalic\n\nIndicates\tnew\tterms,\tURLs,\temail\taddresses,\tfilenames,\tand\tfile\textensions.\n\nConstant\twidth\n\nUsed\tfor\tprogram\tlistings,\tas\twell\tas\twithin\tparagraphs\tto\trefer\tto\tprogram\telements\nsuch\tas\tvariable\tor\tfunction\tnames,\tdatabases,\tdata\ttypes,\tenvironment\tvariables,\nstatements,\tand\tkeywords.\n\nConstant\twidth\tbold\n\nShows\tcommands\tor\tother\ttext\tthat\tshould\tbe\ttyped\tliterally\tby\tthe\tuser.\n\nConstant\twidth\titalic\n\nShows\ttext\tthat\tshould\tbe\treplaced\twith\tuser-supplied\tvalues\tor\tby\tvalues\tdetermined\nby\tcontext.\n\nThis\telement\tsignifies\ta\ttip\tor\tsuggestion.\n\nThis\telement\tsignifies\ta\tgeneral\tnote.\n\nThis\telement\tindicates\ta\twarning\tor\tcaution.\n\nTIP\n\nNOTE\n\nWARNING",
    "12": "Using\tCode\tExamples\n\nSupplemental\tmaterial\t(code\texamples,\texercises,\tetc.)\tis\tavailable\tfor\tdownload\tat\nhttps://github.com/joelgrus/data-science-from-scratch.\n\nThis\tbook\tis\there\tto\thelp\tyou\tget\tyour\tjob\tdone.\tIn\tgeneral,\tif\texample\tcode\tis\toffered\nwith\tthis\tbook,\tyou\tmay\tuse\tit\tin\tyour\tprograms\tand\tdocumentation.\tYou\tdo\tnot\tneed\tto\ncontact\tus\tfor\tpermission\tunless\tyou\u2019re\treproducing\ta\tsignificant\tportion\tof\tthe\tcode.\tFor\nexample,\twriting\ta\tprogram\tthat\tuses\tseveral\tchunks\tof\tcode\tfrom\tthis\tbook\tdoes\tnot\nrequire\tpermission.\tSelling\tor\tdistributing\ta\tCD-ROM\tof\texamples\tfrom\tO\u2019Reilly\tbooks\ndoes\trequire\tpermission.\tAnswering\ta\tquestion\tby\tciting\tthis\tbook\tand\tquoting\texample\ncode\tdoes\tnot\trequire\tpermission.\tIncorporating\ta\tsignificant\tamount\tof\texample\tcode\nfrom\tthis\tbook\tinto\tyour\tproduct\u2019s\tdocumentation\tdoes\trequire\tpermission.\n\nWe\tappreciate,\tbut\tdo\tnot\trequire,\tattribution.\tAn\tattribution\tusually\tincludes\tthe\ttitle,\nauthor,\tpublisher,\tand\tISBN.\tFor\texample:\t\u201cData\tScience\tfrom\tScratch\tby\tJoel\tGrus\n(O\u2019Reilly).\tCopyright\t2015\tJoel\tGrus,\t978-1-4919-0142-7.\u201d\n\nIf\tyou\tfeel\tyour\tuse\tof\tcode\texamples\tfalls\toutside\tfair\tuse\tor\tthe\tpermission\tgiven\tabove,\nfeel\tfree\tto\tcontact\tus\tat\tpermissions@oreilly.com.",
    "13": "Safari\u00ae\tBooks\tOnline\n\nNOTE\n\nSafari\tBooks\tOnline\tis\tan\ton-demand\tdigital\tlibrary\tthat\tdelivers\texpert\tcontent\tin\tboth\nbook\tand\tvideo\tform\tfrom\tthe\tworld\u2019s\tleading\tauthors\tin\ttechnology\tand\tbusiness.\n\nTechnology\tprofessionals,\tsoftware\tdevelopers,\tweb\tdesigners,\tand\tbusiness\tand\tcreative\nprofessionals\tuse\tSafari\tBooks\tOnline\tas\ttheir\tprimary\tresource\tfor\tresearch,\tproblem\nsolving,\tlearning,\tand\tcertification\ttraining.\n\nSafari\tBooks\tOnline\toffers\ta\trange\tof\tplans\tand\tpricing\tfor\tenterprise,\tgovernment,\neducation,\tand\tindividuals.\n\nMembers\thave\taccess\tto\tthousands\tof\tbooks,\ttraining\tvideos,\tand\tprepublication\nmanuscripts\tin\tone\tfully\tsearchable\tdatabase\tfrom\tpublishers\tlike\tO\u2019Reilly\tMedia,\nPrentice\tHall\tProfessional,\tAddison-Wesley\tProfessional,\tMicrosoft\tPress,\tSams,\tQue,\nPeachpit\tPress,\tFocal\tPress,\tCisco\tPress,\tJohn\tWiley\t&\tSons,\tSyngress,\tMorgan\nKaufmann,\tIBM\tRedbooks,\tPackt,\tAdobe\tPress,\tFT\tPress,\tApress,\tManning,\tNew\tRiders,\nMcGraw-Hill,\tJones\t&\tBartlett,\tCourse\tTechnology,\tand\thundreds\tmore.\tFor\tmore\ninformation\tabout\tSafari\tBooks\tOnline,\tplease\tvisit\tus\tonline.",
    "14": "How\tto\tContact\tUs\n\nPlease\taddress\tcomments\tand\tquestions\tconcerning\tthis\tbook\tto\tthe\tpublisher:\n\nO\u2019Reilly\tMedia,\tInc.\n\n1005\tGravenstein\tHighway\tNorth\n\nSebastopol,\tCA\t95472\n\n800-998-9938\t(in\tthe\tUnited\tStates\tor\tCanada)\n\n707-829-0515\t(international\tor\tlocal)\n\n707-829-0104\t(fax)\n\nWe\thave\ta\tweb\tpage\tfor\tthis\tbook,\twhere\twe\tlist\terrata,\texamples,\tand\tany\tadditional\ninformation.\tYou\tcan\taccess\tthis\tpage\tat\thttp://bit.ly/data-science-from-scratch.\n\nTo\tcomment\tor\task\ttechnical\tquestions\tabout\tthis\tbook,\tsend\temail\tto\nbookquestions@oreilly.com.\n\nFor\tmore\tinformation\tabout\tour\tbooks,\tcourses,\tconferences,\tand\tnews,\tsee\tour\twebsite\tat\nhttp://www.oreilly.com.\n\nFind\tus\ton\tFacebook:\thttp://facebook.com/oreilly\n\nFollow\tus\ton\tTwitter:\thttp://twitter.com/oreillymedia\n\nWatch\tus\ton\tYouTube:\thttp://www.youtube.com/oreillymedia",
    "15": "Acknowledgments\n\nFirst,\tI\twould\tlike\tto\tthank\tMike\tLoukides\tfor\taccepting\tmy\tproposal\tfor\tthis\tbook\t(and\nfor\tinsisting\tthat\tI\tpare\tit\tdown\tto\ta\treasonable\tsize).\tIt\twould\thave\tbeen\tvery\teasy\tfor\thim\nto\tsay,\t\u201cWho\u2019s\tthis\tperson\twho\tkeeps\temailing\tme\tsample\tchapters,\tand\thow\tdo\tI\tget\thim\nto\tgo\taway?\u201d\tI\u2019m\tgrateful\the\tdidn\u2019t.\tI\u2019d\talso\tlike\tto\tthank\tmy\teditor,\tMarie\tBeaugureau,\nfor\tguiding\tme\tthrough\tthe\tpublishing\tprocess\tand\tgetting\tthe\tbook\tin\ta\tmuch\tbetter\tstate\nthan\tI\tever\twould\thave\tgotten\tit\ton\tmy\town.\n\nI\tcouldn\u2019t\thave\twritten\tthis\tbook\tif\tI\u2019d\tnever\tlearned\tdata\tscience,\tand\tI\tprobably\twouldn\u2019t\nhave\tlearned\tdata\tscience\tif\tnot\tfor\tthe\tinfluence\tof\tDave\tHsu,\tIgor\tTatarinov,\tJohn\nRauser,\tand\tthe\trest\tof\tthe\tFarecast\tgang.\t(So\tlong\tago\tthat\tit\twasn\u2019t\teven\tcalled\tdata\nscience\tat\tthe\ttime!)\tThe\tgood\tfolks\tat\tCoursera\tdeserve\ta\tlot\tof\tcredit,\ttoo.\n\nI\tam\talso\tgrateful\tto\tmy\tbeta\treaders\tand\treviewers.\tJay\tFundling\tfound\ta\tton\tof\tmistakes\nand\tpointed\tout\tmany\tunclear\texplanations,\tand\tthe\tbook\tis\tmuch\tbetter\t(and\tmuch\tmore\ncorrect)\tthanks\tto\thim.\tDebashis\tGhosh\tis\ta\thero\tfor\tsanity-checking\tall\tof\tmy\tstatistics.\nAndrew\tMusselman\tsuggested\ttoning\tdown\tthe\t\u201cpeople\twho\tprefer\tR\tto\tPython\tare\tmoral\nreprobates\u201d\taspect\tof\tthe\tbook,\twhich\tI\tthink\tended\tup\tbeing\tpretty\tgood\tadvice.\tTrey\nCausey,\tRyan\tMatthew\tBalfanz,\tLoris\tMularoni,\tN\u00faria\tPujol,\tRob\tJefferson,\tMary\tPat\nCampbell,\tZach\tGeary,\tand\tWendy\tGrus\talso\tprovided\tinvaluable\tfeedback.\tAny\terrors\nremaining\tare\tof\tcourse\tmy\tresponsibility.\n\nI\towe\ta\tlot\tto\tthe\tTwitter\t#datascience\tcommmunity,\tfor\texposing\tme\tto\ta\tton\tof\tnew\nconcepts,\tintroducing\tme\tto\ta\tlot\tof\tgreat\tpeople,\tand\tmaking\tme\tfeel\tlike\tenough\tof\tan\nunderachiever\tthat\tI\twent\tout\tand\twrote\ta\tbook\tto\tcompensate.\tSpecial\tthanks\tto\tTrey\nCausey\t(again),\tfor\t(inadvertently)\treminding\tme\tto\tinclude\ta\tchapter\ton\tlinear\talgebra,\nand\tto\tSean\tJ.\tTaylor,\tfor\t(inadvertently)\tpointing\tout\ta\tcouple\tof\thuge\tgaps\tin\tthe\n\u201cWorking\twith\tData\u201d\tchapter.\n\nAbove\tall,\tI\towe\timmense\tthanks\tto\tGanga\tand\tMadeline.\tThe\tonly\tthing\tharder\tthan\nwriting\ta\tbook\tis\tliving\twith\tsomeone\twho\u2019s\twriting\ta\tbook,\tand\tI\tcouldn\u2019t\thave\tpulled\tit\noff\twithout\ttheir\tsupport.",
    "16": "",
    "17": "Chapter\t1.\tIntroduction\n\n\u201cData!\tData!\tData!\u201d\the\tcried\timpatiently.\t\u201cI\tcan\u2019t\tmake\tbricks\twithout\tclay.\u201d\n\nArthur\tConan\tDoyle",
    "18": "The\tAscendance\tof\tData\n\nWe\tlive\tin\ta\tworld\tthat\u2019s\tdrowning\tin\tdata.\tWebsites\ttrack\tevery\tuser\u2019s\tevery\tclick.\tYour\nsmartphone\tis\tbuilding\tup\ta\trecord\tof\tyour\tlocation\tand\tspeed\tevery\tsecond\tof\tevery\tday.\n\u201cQuantified\tselfers\u201d\twear\tpedometers-on-steroids\tthat\tare\tever\trecording\ttheir\theart\trates,\nmovement\thabits,\tdiet,\tand\tsleep\tpatterns.\tSmart\tcars\tcollect\tdriving\thabits,\tsmart\thomes\ncollect\tliving\thabits,\tand\tsmart\tmarketers\tcollect\tpurchasing\thabits.\tThe\tInternet\titself\nrepresents\ta\thuge\tgraph\tof\tknowledge\tthat\tcontains\t(among\tother\tthings)\tan\tenormous\ncross-referenced\tencyclopedia;\tdomain-specific\tdatabases\tabout\tmovies,\tmusic,\tsports\nresults,\tpinball\tmachines,\tmemes,\tand\tcocktails;\tand\ttoo\tmany\tgovernment\tstatistics\n(some\tof\tthem\tnearly\ttrue!)\tfrom\ttoo\tmany\tgovernments\tto\twrap\tyour\thead\taround.\n\nBuried\tin\tthese\tdata\tare\tanswers\tto\tcountless\tquestions\tthat\tno\tone\u2019s\tever\tthought\tto\task.\nIn\tthis\tbook,\twe\u2019ll\tlearn\thow\tto\tfind\tthem.",
    "19": "What\tIs\tData\tScience?\n\nThere\u2019s\ta\tjoke\tthat\tsays\ta\tdata\tscientist\tis\tsomeone\twho\tknows\tmore\tstatistics\tthan\ta\ncomputer\tscientist\tand\tmore\tcomputer\tscience\tthan\ta\tstatistician.\t(I\tdidn\u2019t\tsay\tit\twas\ta\ngood\tjoke.)\tIn\tfact,\tsome\tdata\tscientists\tare\t\u2014\tfor\tall\tpractical\tpurposes\t\u2014\tstatisticians,\nwhile\tothers\tare\tpretty\tmuch\tindistinguishable\tfrom\tsoftware\tengineers.\tSome\tare\nmachine-learning\texperts,\twhile\tothers\tcouldn\u2019t\tmachine-learn\ttheir\tway\tout\tof\nkindergarten.\tSome\tare\tPhDs\twith\timpressive\tpublication\trecords,\twhile\tothers\thave\tnever\nread\tan\tacademic\tpaper\t(shame\ton\tthem,\tthough).\tIn\tshort,\tpretty\tmuch\tno\tmatter\thow\tyou\ndefine\tdata\tscience,\tyou\u2019ll\tfind\tpractitioners\tfor\twhom\tthe\tdefinition\tis\ttotally,\tabsolutely\nwrong.\n\nNonetheless,\twe\twon\u2019t\tlet\tthat\tstop\tus\tfrom\ttrying.\tWe\u2019ll\tsay\tthat\ta\tdata\tscientist\tis\nsomeone\twho\textracts\tinsights\tfrom\tmessy\tdata.\tToday\u2019s\tworld\tis\tfull\tof\tpeople\ttrying\tto\nturn\tdata\tinto\tinsight.\n\nFor\tinstance,\tthe\tdating\tsite\tOkCupid\tasks\tits\tmembers\tto\tanswer\tthousands\tof\tquestions\nin\torder\tto\tfind\tthe\tmost\tappropriate\tmatches\tfor\tthem.\tBut\tit\talso\tanalyzes\tthese\tresults\tto\nfigure\tout\tinnocuous-sounding\tquestions\tyou\tcan\task\tsomeone\tto\tfind\tout\thow\tlikely\nsomeone\tis\tto\tsleep\twith\tyou\ton\tthe\tfirst\tdate.\n\nFacebook\tasks\tyou\tto\tlist\tyour\thometown\tand\tyour\tcurrent\tlocation,\tostensibly\tto\tmake\tit\neasier\tfor\tyour\tfriends\tto\tfind\tand\tconnect\twith\tyou.\tBut\tit\talso\tanalyzes\tthese\tlocations\tto\nidentify\tglobal\tmigration\tpatterns\tand\twhere\tthe\tfanbases\tof\tdifferent\tfootball\tteams\tlive.\n\nAs\ta\tlarge\tretailer,\tTarget\ttracks\tyour\tpurchases\tand\tinteractions,\tboth\tonline\tand\tin-store.\nAnd\tit\tuses\tthe\tdata\tto\tpredictively\tmodel\twhich\tof\tits\tcustomers\tare\tpregnant,\tto\tbetter\nmarket\tbaby-related\tpurchases\tto\tthem.\n\nIn\t2012,\tthe\tObama\tcampaign\temployed\tdozens\tof\tdata\tscientists\twho\tdata-mined\tand\nexperimented\ttheir\tway\tto\tidentifying\tvoters\twho\tneeded\textra\tattention,\tchoosing\toptimal\ndonor-specific\tfundraising\tappeals\tand\tprograms,\tand\tfocusing\tget-out-the-vote\tefforts\nwhere\tthey\twere\tmost\tlikely\tto\tbe\tuseful.\tIt\tis\tgenerally\tagreed\tthat\tthese\tefforts\tplayed\tan\nimportant\trole\tin\tthe\tpresident\u2019s\tre-election,\twhich\tmeans\tit\tis\ta\tsafe\tbet\tthat\tpolitical\ncampaigns\tof\tthe\tfuture\twill\tbecome\tmore\tand\tmore\tdata-driven,\tresulting\tin\ta\tnever-\nending\tarms\trace\tof\tdata\tscience\tand\tdata\tcollection.\n\nNow,\tbefore\tyou\tstart\tfeeling\ttoo\tjaded:\tsome\tdata\tscientists\talso\toccasionally\tuse\ttheir\nskills\tfor\tgood\t\u2014\tusing\tdata\tto\tmake\tgovernment\tmore\teffective,\tto\thelp\tthe\thomeless,\nand\tto\timprove\tpublic\thealth.\tBut\tit\tcertainly\twon\u2019t\thurt\tyour\tcareer\tif\tyou\tlike\tfiguring\nout\tthe\tbest\tway\tto\tget\tpeople\tto\tclick\ton\tadvertisements.",
    "20": "Motivating\tHypothetical:\tDataSciencester\n\nCongratulations!\tYou\u2019ve\tjust\tbeen\thired\tto\tlead\tthe\tdata\tscience\tefforts\tat\tDataSciencester,\nthe\tsocial\tnetwork\tfor\tdata\tscientists.\n\nDespite\tbeing\tfor\tdata\tscientists,\tDataSciencester\thas\tnever\tactually\tinvested\tin\tbuilding\nits\town\tdata\tscience\tpractice.\t(In\tfairness,\tDataSciencester\thas\tnever\treally\tinvested\tin\nbuilding\tits\tproduct\teither.)\tThat\twill\tbe\tyour\tjob!\tThroughout\tthe\tbook,\twe\u2019ll\tbe\tlearning\nabout\tdata\tscience\tconcepts\tby\tsolving\tproblems\tthat\tyou\tencounter\tat\twork.\tSometimes\nwe\u2019ll\tlook\tat\tdata\texplicitly\tsupplied\tby\tusers,\tsometimes\twe\u2019ll\tlook\tat\tdata\tgenerated\nthrough\ttheir\tinteractions\twith\tthe\tsite,\tand\tsometimes\twe\u2019ll\teven\tlook\tat\tdata\tfrom\nexperiments\tthat\twe\u2019ll\tdesign.\n\nAnd\tbecause\tDataSciencester\thas\ta\tstrong\t\u201cnot-invented-here\u201d\tmentality,\twe\u2019ll\tbe\nbuilding\tour\town\ttools\tfrom\tscratch.\tAt\tthe\tend,\tyou\u2019ll\thave\ta\tpretty\tsolid\tunderstanding\nof\tthe\tfundamentals\tof\tdata\tscience.\tAnd\tyou\u2019ll\tbe\tready\tto\tapply\tyour\tskills\tat\ta\tcompany\nwith\ta\tless\tshaky\tpremise,\tor\tto\tany\tother\tproblems\tthat\thappen\tto\tinterest\tyou.\n\nWelcome\taboard,\tand\tgood\tluck!\t(You\u2019re\tallowed\tto\twear\tjeans\ton\tFridays,\tand\tthe\nbathroom\tis\tdown\tthe\thall\ton\tthe\tright.)",
    "21": "Finding\tKey\tConnectors\n\nIt\u2019s\tyour\tfirst\tday\ton\tthe\tjob\tat\tDataSciencester,\tand\tthe\tVP\tof\tNetworking\tis\tfull\tof\nquestions\tabout\tyour\tusers.\tUntil\tnow\the\u2019s\thad\tno\tone\tto\task,\tso\the\u2019s\tvery\texcited\tto\thave\nyou\taboard.\n\nIn\tparticular,\the\twants\tyou\tto\tidentify\twho\tthe\t\u201ckey\tconnectors\u201d\tare\tamong\tdata\tscientists.\nTo\tthis\tend,\the\tgives\tyou\ta\tdump\tof\tthe\tentire\tDataSciencester\tnetwork.\t(In\treal\tlife,\npeople\tdon\u2019t\ttypically\thand\tyou\tthe\tdata\tyou\tneed.\tChapter\t9\tis\tdevoted\tto\tgetting\tdata.)\n\nWhat\tdoes\tthis\tdata\tdump\tlook\tlike?\tIt\tconsists\tof\ta\tlist\tof\tusers,\teach\trepresented\tby\ta\ndict\tthat\tcontains\tfor\teach\tuser\this\tor\ther\tid\t(which\tis\ta\tnumber)\tand\tname\t(which,\tin\tone\nof\tthe\tgreat\tcosmic\tcoincidences,\trhymes\twith\tthe\tuser\u2019s\tid):\n\nusers\t=\t[\n\t\t\t\t{\t\"id\":\t0,\t\"name\":\t\"Hero\"\t},\n\t\t\t\t{\t\"id\":\t1,\t\"name\":\t\"Dunn\"\t},\n\t\t\t\t{\t\"id\":\t2,\t\"name\":\t\"Sue\"\t},\n\t\t\t\t{\t\"id\":\t3,\t\"name\":\t\"Chi\"\t},\n\t\t\t\t{\t\"id\":\t4,\t\"name\":\t\"Thor\"\t},\n\t\t\t\t{\t\"id\":\t5,\t\"name\":\t\"Clive\"\t},\n\t\t\t\t{\t\"id\":\t6,\t\"name\":\t\"Hicks\"\t},\n\t\t\t\t{\t\"id\":\t7,\t\"name\":\t\"Devin\"\t},\n\t\t\t\t{\t\"id\":\t8,\t\"name\":\t\"Kate\"\t},\n\t\t\t\t{\t\"id\":\t9,\t\"name\":\t\"Klein\"\t}\n]\n\nHe\talso\tgives\tyou\tthe\t\u201cfriendship\u201d\tdata,\trepresented\tas\ta\tlist\tof\tpairs\tof\tIDs:\n\nfriendships\t=\t[(0,\t1),\t(0,\t2),\t(1,\t2),\t(1,\t3),\t(2,\t3),\t(3,\t4),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(4,\t5),\t(5,\t6),\t(5,\t7),\t(6,\t8),\t(7,\t8),\t(8,\t9)]\n\nFor\texample,\tthe\ttuple\t(0,\t1)\tindicates\tthat\tthe\tdata\tscientist\twith\tid\t0\t(Hero)\tand\tthe\ndata\tscientist\twith\tid\t1\t(Dunn)\tare\tfriends.\tThe\tnetwork\tis\tillustrated\tin\tFigure\t1-1.\n\nFigure\t1-1.\tThe\tDataSciencester\tnetwork\n\nSince\twe\trepresented\tour\tusers\tas\tdicts,\tit\u2019s\teasy\tto\taugment\tthem\twith\textra\tdata.\n\nDon\u2019t\tget\ttoo\thung\tup\ton\tthe\tdetails\tof\tthe\tcode\tright\tnow.\tIn\tChapter\t2,\twe\u2019ll\ttake\tyou\tthrough\ta\tcrash\ncourse\tin\tPython.\tFor\tnow\tjust\ttry\tto\tget\tthe\tgeneral\tflavor\tof\twhat\twe\u2019re\tdoing.\n\nNOTE",
    "22": "For\texample,\twe\tmight\twant\tto\tadd\ta\tlist\tof\tfriends\tto\teach\tuser.\tFirst\twe\tset\teach\tuser\u2019s\nfriends\tproperty\tto\tan\tempty\tlist:\n\nfor\tuser\tin\tusers:\n\t\t\t\tuser[\"friends\"]\t=\t[]\n\nAnd\tthen\twe\tpopulate\tthe\tlists\tusing\tthe\tfriendships\tdata:\n\nfor\ti,\tj\tin\tfriendships:\n\t\t\t\t#\tthis\tworks\tbecause\tusers[i]\tis\tthe\tuser\twhose\tid\tis\ti\n\t\t\t\tusers[i][\"friends\"].append(users[j])\t#\tadd\ti\tas\ta\tfriend\tof\tj\n\t\t\t\tusers[j][\"friends\"].append(users[i])\t#\tadd\tj\tas\ta\tfriend\tof\ti\n\nOnce\teach\tuser\tdict\tcontains\ta\tlist\tof\tfriends,\twe\tcan\teasily\task\tquestions\tof\tour\tgraph,\nlike\t\u201cwhat\u2019s\tthe\taverage\tnumber\tof\tconnections?\u201d\n\nFirst\twe\tfind\tthe\ttotal\tnumber\tof\tconnections,\tby\tsumming\tup\tthe\tlengths\tof\tall\tthe\nfriends\tlists:\n\ndef\tnumber_of_friends(user):\n\t\t\t\t\"\"\"how\tmany\tfriends\tdoes\t_user_\thave?\"\"\"\n\t\t\t\treturn\tlen(user[\"friends\"])\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tlength\tof\tfriend_ids\tlist\n\ntotal_connections\t=\tsum(number_of_friends(user)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tuser\tin\tusers)\t\t\t\t\t\t\t\t#\t24\n\nAnd\tthen\twe\tjust\tdivide\tby\tthe\tnumber\tof\tusers:\n\nfrom\t__future__\timport\tdivision\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tinteger\tdivision\tis\tlame\nnum_users\t=\tlen(users)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tlength\tof\tthe\tusers\tlist\navg_connections\t=\ttotal_connections\t/\tnum_users\t\t\t#\t2.4\n\nIt\u2019s\talso\teasy\tto\tfind\tthe\tmost\tconnected\tpeople\t\u2014\tthey\u2019re\tthe\tpeople\twho\thave\tthe\tlargest\nnumber\tof\tfriends.\n\nSince\tthere\taren\u2019t\tvery\tmany\tusers,\twe\tcan\tsort\tthem\tfrom\t\u201cmost\tfriends\u201d\tto\t\u201cleast\nfriends\u201d:\n\n#\tcreate\ta\tlist\t(user_id,\tnumber_of_friends)\nnum_friends_by_id\t=\t[(user[\"id\"],\tnumber_of_friends(user))\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tuser\tin\tusers]\n\nsorted(num_friends_by_id,\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tget\tit\tsorted\n\t\t\t\t\t\t\tkey=lambda\t(user_id,\tnum_friends):\tnum_friends,\t\t\t#\tby\tnum_friends\n\t\t\t\t\t\t\treverse=True)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tlargest\tto\tsmallest\n\n#\teach\tpair\tis\t(user_id,\tnum_friends)\n#\t[(1,\t3),\t(2,\t3),\t(3,\t3),\t(5,\t3),\t(8,\t3),\n#\t\t(0,\t2),\t(4,\t2),\t(6,\t2),\t(7,\t2),\t(9,\t1)]\n\nOne\tway\tto\tthink\tof\twhat\twe\u2019ve\tdone\tis\tas\ta\tway\tof\tidentifying\tpeople\twho\tare\tsomehow\ncentral\tto\tthe\tnetwork.\tIn\tfact,\twhat\twe\u2019ve\tjust\tcomputed\tis\tthe\tnetwork\tmetric\tdegree\ncentrality\t(Figure\t1-2).",
    "23": "Figure\t1-2.\tThe\tDataSciencester\tnetwork\tsized\tby\tdegree\n\nThis\thas\tthe\tvirtue\tof\tbeing\tpretty\teasy\tto\tcalculate,\tbut\tit\tdoesn\u2019t\talways\tgive\tthe\tresults\nyou\u2019d\twant\tor\texpect.\tFor\texample,\tin\tthe\tDataSciencester\tnetwork\tThor\t(id\t4)\tonly\thas\ntwo\tconnections\twhile\tDunn\t(id\t1)\thas\tthree.\tYet\tlooking\tat\tthe\tnetwork\tit\tintuitively\nseems\tlike\tThor\tshould\tbe\tmore\tcentral.\tIn\tChapter\t21,\twe\u2019ll\tinvestigate\tnetworks\tin\tmore\ndetail,\tand\twe\u2019ll\tlook\tat\tmore\tcomplex\tnotions\tof\tcentrality\tthat\tmay\tor\tmay\tnot\taccord\nbetter\twith\tour\tintuition.",
    "24": "Data\tScientists\tYou\tMay\tKnow\n\nWhile\tyou\u2019re\tstill\tfilling\tout\tnew-hire\tpaperwork,\tthe\tVP\tof\tFraternization\tcomes\tby\tyour\ndesk.\tShe\twants\tto\tencourage\tmore\tconnections\tamong\tyour\tmembers,\tand\tshe\tasks\tyou\nto\tdesign\ta\t\u201cData\tScientists\tYou\tMay\tKnow\u201d\tsuggester.\n\nYour\tfirst\tinstinct\tis\tto\tsuggest\tthat\ta\tuser\tmight\tknow\tthe\tfriends\tof\tfriends.\tThese\tare\neasy\tto\tcompute:\tfor\teach\tof\ta\tuser\u2019s\tfriends,\titerate\tover\tthat\tperson\u2019s\tfriends,\tand\tcollect\nall\tthe\tresults:\n\ndef\tfriends_of_friend_ids_bad(user):\n\t\t\t\t#\t\"foaf\"\tis\tshort\tfor\t\"friend\tof\ta\tfriend\"\n\t\t\t\treturn\t[foaf[\"id\"]\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tfriend\tin\tuser[\"friends\"]\t\t\t\t\t#\tfor\teach\tof\tuser's\tfriends\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tfoaf\tin\tfriend[\"friends\"]]\t\t\t\t#\tget\teach\tof\t_their_\tfriends\n\nWhen\twe\tcall\tthis\ton\tusers[0]\t(Hero),\tit\tproduces:\n\n[0,\t2,\t3,\t0,\t1,\t3]\n\nIt\tincludes\tuser\t0\t(twice),\tsince\tHero\tis\tindeed\tfriends\twith\tboth\tof\this\tfriends.\tIt\tincludes\nusers\t1\tand\t2,\talthough\tthey\tare\tboth\tfriends\twith\tHero\talready.\tAnd\tit\tincludes\tuser\t3\ntwice,\tas\tChi\tis\treachable\tthrough\ttwo\tdifferent\tfriends:\n\nprint\t[friend[\"id\"]\tfor\tfriend\tin\tusers[0][\"friends\"]]\t\t#\t[1,\t2]\nprint\t[friend[\"id\"]\tfor\tfriend\tin\tusers[1][\"friends\"]]\t\t#\t[0,\t2,\t3]\nprint\t[friend[\"id\"]\tfor\tfriend\tin\tusers[2][\"friends\"]]\t\t#\t[0,\t1,\t3]\n\nKnowing\tthat\tpeople\tare\tfriends-of-friends\tin\tmultiple\tways\tseems\tlike\tinteresting\ninformation,\tso\tmaybe\tinstead\twe\tshould\tproduce\ta\tcount\tof\tmutual\tfriends.\tAnd\twe\ndefinitely\tshould\tuse\ta\thelper\tfunction\tto\texclude\tpeople\talready\tknown\tto\tthe\tuser:\n\nfrom\tcollections\timport\tCounter\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tnot\tloaded\tby\tdefault\n\ndef\tnot_the_same(user,\tother_user):\n\t\t\t\t\"\"\"two\tusers\tare\tnot\tthe\tsame\tif\tthey\thave\tdifferent\tids\"\"\"\n\t\t\t\treturn\tuser[\"id\"]\t!=\tother_user[\"id\"]\n\ndef\tnot_friends(user,\tother_user):\n\t\t\t\t\"\"\"other_user\tis\tnot\ta\tfriend\tif\the's\tnot\tin\tuser[\"friends\"];\n\t\t\t\tthat\tis,\tif\the's\tnot_the_same\tas\tall\tthe\tpeople\tin\tuser[\"friends\"]\"\"\"\n\t\t\t\treturn\tall(not_the_same(friend,\tother_user)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tfriend\tin\tuser[\"friends\"])\n\ndef\tfriends_of_friend_ids(user):\n\t\t\t\treturn\tCounter(foaf[\"id\"]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tfriend\tin\tuser[\"friends\"]\t\t\t\t#\tfor\teach\tof\tmy\tfriends\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tfoaf\tin\tfriend[\"friends\"]\t\t\t\t#\tcount\t*their*\tfriends\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tnot_the_same(user,\tfoaf)\t\t\t\t\t\t#\twho\taren't\tme\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tand\tnot_friends(user,\tfoaf))\t\t\t\t\t#\tand\taren't\tmy\tfriends\n\nprint\tfriends_of_friend_ids(users[3])\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tCounter({0:\t2,\t5:\t1})\n\nThis\tcorrectly\ttells\tChi\t(id\t3)\tthat\tshe\thas\ttwo\tmutual\tfriends\twith\tHero\t(id\t0)\tbut\tonly\none\tmutual\tfriend\twith\tClive\t(id\t5).\n\nAs\ta\tdata\tscientist,\tyou\tknow\tthat\tyou\talso\tmight\tenjoy\tmeeting\tusers\twith\tsimilar",
    "25": "interests.\t(This\tis\ta\tgood\texample\tof\tthe\t\u201csubstantive\texpertise\u201d\taspect\tof\tdata\tscience.)\nAfter\tasking\taround,\tyou\tmanage\tto\tget\tyour\thands\ton\tthis\tdata,\tas\ta\tlist\tof\tpairs\n(user_id,\tinterest):\n\ninterests\t=\t[\n\t\t\t\t(0,\t\"Hadoop\"),\t(0,\t\"Big\tData\"),\t(0,\t\"HBase\"),\t(0,\t\"Java\"),\n\t\t\t\t(0,\t\"Spark\"),\t(0,\t\"Storm\"),\t(0,\t\"Cassandra\"),\n\t\t\t\t(1,\t\"NoSQL\"),\t(1,\t\"MongoDB\"),\t(1,\t\"Cassandra\"),\t(1,\t\"HBase\"),\n\t\t\t\t(1,\t\"Postgres\"),\t(2,\t\"Python\"),\t(2,\t\"scikit-learn\"),\t(2,\t\"scipy\"),\n\t\t\t\t(2,\t\"numpy\"),\t(2,\t\"statsmodels\"),\t(2,\t\"pandas\"),\t(3,\t\"R\"),\t(3,\t\"Python\"),\n\t\t\t\t(3,\t\"statistics\"),\t(3,\t\"regression\"),\t(3,\t\"probability\"),\n\t\t\t\t(4,\t\"machine\tlearning\"),\t(4,\t\"regression\"),\t(4,\t\"decision\ttrees\"),\n\t\t\t\t(4,\t\"libsvm\"),\t(5,\t\"Python\"),\t(5,\t\"R\"),\t(5,\t\"Java\"),\t(5,\t\"C++\"),\n\t\t\t\t(5,\t\"Haskell\"),\t(5,\t\"programming\tlanguages\"),\t(6,\t\"statistics\"),\n\t\t\t\t(6,\t\"probability\"),\t(6,\t\"mathematics\"),\t(6,\t\"theory\"),\n\t\t\t\t(7,\t\"machine\tlearning\"),\t(7,\t\"scikit-learn\"),\t(7,\t\"Mahout\"),\n\t\t\t\t(7,\t\"neural\tnetworks\"),\t(8,\t\"neural\tnetworks\"),\t(8,\t\"deep\tlearning\"),\n\t\t\t\t(8,\t\"Big\tData\"),\t(8,\t\"artificial\tintelligence\"),\t(9,\t\"Hadoop\"),\n\t\t\t\t(9,\t\"Java\"),\t(9,\t\"MapReduce\"),\t(9,\t\"Big\tData\")\n]\n\nFor\texample,\tThor\t(id\t4)\thas\tno\tfriends\tin\tcommon\twith\tDevin\t(id\t7),\tbut\tthey\tshare\tan\ninterest\tin\tmachine\tlearning.\n\nIt\u2019s\teasy\tto\tbuild\ta\tfunction\tthat\tfinds\tusers\twith\ta\tcertain\tinterest:\n\ndef\tdata_scientists_who_like(target_interest):\n\t\t\t\treturn\t[user_id\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tuser_id,\tuser_interest\tin\tinterests\n\t\t\t\t\t\t\t\t\t\t\t\tif\tuser_interest\t==\ttarget_interest]\n\nThis\tworks,\tbut\tit\thas\tto\texamine\tthe\twhole\tlist\tof\tinterests\tfor\tevery\tsearch.\tIf\twe\thave\ta\nlot\tof\tusers\tand\tinterests\t(or\tif\twe\tjust\twant\tto\tdo\ta\tlot\tof\tsearches),\twe\u2019re\tprobably\tbetter\noff\tbuilding\tan\tindex\tfrom\tinterests\tto\tusers:\n\nfrom\tcollections\timport\tdefaultdict\n\n#\tkeys\tare\tinterests,\tvalues\tare\tlists\tof\tuser_ids\twith\tthat\tinterest\nuser_ids_by_interest\t=\tdefaultdict(list)\n\nfor\tuser_id,\tinterest\tin\tinterests:\n\t\t\t\tuser_ids_by_interest[interest].append(user_id)\n\nAnd\tanother\tfrom\tusers\tto\tinterests:\n\n#\tkeys\tare\tuser_ids,\tvalues\tare\tlists\tof\tinterests\tfor\tthat\tuser_id\ninterests_by_user_id\t=\tdefaultdict(list)\n\nfor\tuser_id,\tinterest\tin\tinterests:\n\t\t\t\tinterests_by_user_id[user_id].append(interest)\n\nNow\tit\u2019s\teasy\tto\tfind\twho\thas\tthe\tmost\tinterests\tin\tcommon\twith\ta\tgiven\tuser:\n\nIterate\tover\tthe\tuser\u2019s\tinterests.\n\nFor\teach\tinterest,\titerate\tover\tthe\tother\tusers\twith\tthat\tinterest.\n\nKeep\tcount\tof\thow\tmany\ttimes\twe\tsee\teach\tother\tuser.",
    "26": "def\tmost_common_interests_with(user):\n\t\t\t\treturn\tCounter(interested_user_id\n\t\t\t\t\t\t\t\tfor\tinterest\tin\tinterests_by_user_id[user[\"id\"]]\n\t\t\t\t\t\t\t\tfor\tinterested_user_id\tin\tuser_ids_by_interest[interest]\n\t\t\t\t\t\t\t\tif\tinterested_user_id\t!=\tuser[\"id\"])\n\nWe\tcould\tthen\tuse\tthis\tto\tbuild\ta\tricher\t\u201cData\tScientists\tYou\tShould\tKnow\u201d\tfeature\tbased\non\ta\tcombination\tof\tmutual\tfriends\tand\tmutual\tinterests.\tWe\u2019ll\texplore\tthese\tkinds\tof\napplications\tin\tChapter\t22.",
    "27": "Salaries\tand\tExperience\n\nRight\tas\tyou\u2019re\tabout\tto\thead\tto\tlunch,\tthe\tVP\tof\tPublic\tRelations\tasks\tif\tyou\tcan\tprovide\nsome\tfun\tfacts\tabout\thow\tmuch\tdata\tscientists\tearn.\tSalary\tdata\tis\tof\tcourse\tsensitive,\tbut\nhe\tmanages\tto\tprovide\tyou\tan\tanonymous\tdata\tset\tcontaining\teach\tuser\u2019s\tsalary\t(in\ndollars)\tand\ttenure\tas\ta\tdata\tscientist\t(in\tyears):\n\nsalaries_and_tenures\t=\t[(83000,\t8.7),\t(88000,\t8.1),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(48000,\t0.7),\t(76000,\t6),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(69000,\t6.5),\t(76000,\t7.5),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(60000,\t2.5),\t(83000,\t10),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(48000,\t1.9),\t(63000,\t4.2)]\n\nThe\tnatural\tfirst\tstep\tis\tto\tplot\tthe\tdata\t(which\twe\u2019ll\tsee\thow\tto\tdo\tin\tChapter\t3).\tYou\tcan\nsee\tthe\tresults\tin\tFigure\t1-3.\n\nFigure\t1-3.\tSalary\tby\tyears\tof\texperience\n\nIt\tseems\tpretty\tclear\tthat\tpeople\twith\tmore\texperience\ttend\tto\tearn\tmore.\tHow\tcan\tyou\nturn\tthis\tinto\ta\tfun\tfact?\tYour\tfirst\tidea\tis\tto\tlook\tat\tthe\taverage\tsalary\tfor\teach\ttenure:\n\n#\tkeys\tare\tyears,\tvalues\tare\tlists\tof\tthe\tsalaries\tfor\teach\ttenure\nsalary_by_tenure\t=\tdefaultdict(list)\n\nfor\tsalary,\ttenure\tin\tsalaries_and_tenures:\n\t\t\t\tsalary_by_tenure[tenure].append(salary)",
    "28": "#\tkeys\tare\tyears,\teach\tvalue\tis\taverage\tsalary\tfor\tthat\ttenure\naverage_salary_by_tenure\t=\t{\n\t\t\t\ttenure\t:\tsum(salaries)\t/\tlen(salaries)\n\t\t\t\tfor\ttenure,\tsalaries\tin\tsalary_by_tenure.items()\n}\n\nThis\tturns\tout\tto\tbe\tnot\tparticularly\tuseful,\tas\tnone\tof\tthe\tusers\thave\tthe\tsame\ttenure,\nwhich\tmeans\twe\u2019re\tjust\treporting\tthe\tindividual\tusers\u2019\tsalaries:\n\n{0.7:\t48000.0,\n\t1.9:\t48000.0,\n\t2.5:\t60000.0,\n\t4.2:\t63000.0,\n\t6:\t76000.0,\n\t6.5:\t69000.0,\n\t7.5:\t76000.0,\n\t8.1:\t88000.0,\n\t8.7:\t83000.0,\n\t10:\t83000.0}\n\nIt\tmight\tbe\tmore\thelpful\tto\tbucket\tthe\ttenures:\n\ndef\ttenure_bucket(tenure):\n\t\t\t\tif\ttenure\t<\t2:\n\t\t\t\t\t\t\t\treturn\t\"less\tthan\ttwo\"\n\t\t\t\telif\ttenure\t<\t5:\n\t\t\t\t\t\t\t\treturn\t\"between\ttwo\tand\tfive\"\n\t\t\t\telse:\n\t\t\t\t\t\t\t\treturn\t\"more\tthan\tfive\"\n\nThen\tgroup\ttogether\tthe\tsalaries\tcorresponding\tto\teach\tbucket:\n\n#\tkeys\tare\ttenure\tbuckets,\tvalues\tare\tlists\tof\tsalaries\tfor\tthat\tbucket\nsalary_by_tenure_bucket\t=\tdefaultdict(list)\n\nfor\tsalary,\ttenure\tin\tsalaries_and_tenures:\n\t\t\t\tbucket\t=\ttenure_bucket(tenure)\n\t\t\t\tsalary_by_tenure_bucket[bucket].append(salary)\n\nAnd\tfinally\tcompute\tthe\taverage\tsalary\tfor\teach\tgroup:\n\n#\tkeys\tare\ttenure\tbuckets,\tvalues\tare\taverage\tsalary\tfor\tthat\tbucket\naverage_salary_by_bucket\t=\t{\n\t\ttenure_bucket\t:\tsum(salaries)\t/\tlen(salaries)\n\t\tfor\ttenure_bucket,\tsalaries\tin\tsalary_by_tenure_bucket.iteritems()\n}\n\nwhich\tis\tmore\tinteresting:\n\n{'between\ttwo\tand\tfive':\t61500.0,\n\t'less\tthan\ttwo':\t48000.0,\n\t'more\tthan\tfive':\t79166.66666666667}\n\nAnd\tyou\thave\tyour\tsoundbite:\t\u201cData\tscientists\twith\tmore\tthan\tfive\tyears\texperience\tearn\n65%\tmore\tthan\tdata\tscientists\twith\tlittle\tor\tno\texperience!\u201d\n\nBut\twe\tchose\tthe\tbuckets\tin\ta\tpretty\tarbitrary\tway.\tWhat\twe\u2019d\treally\tlike\tis\tto\tmake\tsome\nsort\tof\tstatement\tabout\tthe\tsalary\teffect\t\u2014\ton\taverage\t\u2014\tof\thaving\tan\tadditional\tyear\tof",
    "29": "experience.\tIn\taddition\tto\tmaking\tfor\ta\tsnappier\tfun\tfact,\tthis\tallows\tus\tto\tmake\npredictions\tabout\tsalaries\tthat\twe\tdon\u2019t\tknow.\tWe\u2019ll\texplore\tthis\tidea\tin\tChapter\t14.",
    "30": "Paid\tAccounts\n\nWhen\tyou\tget\tback\tto\tyour\tdesk,\tthe\tVP\tof\tRevenue\tis\twaiting\tfor\tyou.\tShe\twants\tto\nbetter\tunderstand\twhich\tusers\tpay\tfor\taccounts\tand\twhich\tdon\u2019t.\t(She\tknows\ttheir\tnames,\nbut\tthat\u2019s\tnot\tparticularly\tactionable\tinformation.)\n\nYou\tnotice\tthat\tthere\tseems\tto\tbe\ta\tcorrespondence\tbetween\tyears\tof\texperience\tand\tpaid\naccounts:\n\n0.7\tpaid\n1.9\tunpaid\n2.5\tpaid\n4.2\tunpaid\n6\t\t\tunpaid\n6.5\tunpaid\n7.5\tunpaid\n8.1\tunpaid\n8.7\tpaid\n10\t\tpaid\n\nUsers\twith\tvery\tfew\tand\tvery\tmany\tyears\tof\texperience\ttend\tto\tpay;\tusers\twith\taverage\namounts\tof\texperience\tdon\u2019t.\n\nAccordingly,\tif\tyou\twanted\tto\tcreate\ta\tmodel\t\u2014\tthough\tthis\tis\tdefinitely\tnot\tenough\tdata\nto\tbase\ta\tmodel\ton\t\u2014\tyou\tmight\ttry\tto\tpredict\t\u201cpaid\u201d\tfor\tusers\twith\tvery\tfew\tand\tvery\nmany\tyears\tof\texperience,\tand\t\u201cunpaid\u201d\tfor\tusers\twith\tmiddling\tamounts\tof\texperience:\n\ndef\tpredict_paid_or_unpaid(years_experience):\n\t\tif\tyears_experience\t<\t3.0:\n\t\t\t\treturn\t\"paid\"\n\t\telif\tyears_experience\t<\t8.5:\n\t\t\t\treturn\t\"unpaid\"\n\t\telse:\n\t\t\t\treturn\t\"paid\"\n\nOf\tcourse,\twe\ttotally\teyeballed\tthe\tcutoffs.\n\nWith\tmore\tdata\t(and\tmore\tmathematics),\twe\tcould\tbuild\ta\tmodel\tpredicting\tthe\tlikelihood\nthat\ta\tuser\twould\tpay,\tbased\ton\this\tyears\tof\texperience.\tWe\u2019ll\tinvestigate\tthis\tsort\tof\nproblem\tin\tChapter\t16.",
    "31": "Topics\tof\tInterest\n\nAs\tyou\u2019re\twrapping\tup\tyour\tfirst\tday,\tthe\tVP\tof\tContent\tStrategy\tasks\tyou\tfor\tdata\tabout\nwhat\ttopics\tusers\tare\tmost\tinterested\tin,\tso\tthat\tshe\tcan\tplan\tout\ther\tblog\tcalendar\naccordingly.\tYou\talready\thave\tthe\traw\tdata\tfrom\tthe\tfriend-suggester\tproject:\n\ninterests\t=\t[\n\t\t\t\t(0,\t\"Hadoop\"),\t(0,\t\"Big\tData\"),\t(0,\t\"HBase\"),\t(0,\t\"Java\"),\n\t\t\t\t(0,\t\"Spark\"),\t(0,\t\"Storm\"),\t(0,\t\"Cassandra\"),\n\t\t\t\t(1,\t\"NoSQL\"),\t(1,\t\"MongoDB\"),\t(1,\t\"Cassandra\"),\t(1,\t\"HBase\"),\n\t\t\t\t(1,\t\"Postgres\"),\t(2,\t\"Python\"),\t(2,\t\"scikit-learn\"),\t(2,\t\"scipy\"),\n\t\t\t\t(2,\t\"numpy\"),\t(2,\t\"statsmodels\"),\t(2,\t\"pandas\"),\t(3,\t\"R\"),\t(3,\t\"Python\"),\n\t\t\t\t(3,\t\"statistics\"),\t(3,\t\"regression\"),\t(3,\t\"probability\"),\n\t\t\t\t(4,\t\"machine\tlearning\"),\t(4,\t\"regression\"),\t(4,\t\"decision\ttrees\"),\n\t\t\t\t(4,\t\"libsvm\"),\t(5,\t\"Python\"),\t(5,\t\"R\"),\t(5,\t\"Java\"),\t(5,\t\"C++\"),\n\t\t\t\t(5,\t\"Haskell\"),\t(5,\t\"programming\tlanguages\"),\t(6,\t\"statistics\"),\n\t\t\t\t(6,\t\"probability\"),\t(6,\t\"mathematics\"),\t(6,\t\"theory\"),\n\t\t\t\t(7,\t\"machine\tlearning\"),\t(7,\t\"scikit-learn\"),\t(7,\t\"Mahout\"),\n\t\t\t\t(7,\t\"neural\tnetworks\"),\t(8,\t\"neural\tnetworks\"),\t(8,\t\"deep\tlearning\"),\n\t\t\t\t(8,\t\"Big\tData\"),\t(8,\t\"artificial\tintelligence\"),\t(9,\t\"Hadoop\"),\n\t\t\t\t(9,\t\"Java\"),\t(9,\t\"MapReduce\"),\t(9,\t\"Big\tData\")\n]\n\nOne\tsimple\t(if\tnot\tparticularly\texciting)\tway\tto\tfind\tthe\tmost\tpopular\tinterests\tis\tsimply\tto\ncount\tthe\twords:\n\n1.\t Lowercase\teach\tinterest\t(since\tdifferent\tusers\tmay\tor\tmay\tnot\tcapitalize\ttheir\n\ninterests).\n\n2.\t Split\tit\tinto\twords.\n\n3.\t Count\tthe\tresults.\n\nIn\tcode:\n\nwords_and_counts\t=\tCounter(word\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tuser,\tinterest\tin\tinterests\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tword\tin\tinterest.lower().split())\n\nThis\tmakes\tit\teasy\tto\tlist\tout\tthe\twords\tthat\toccur\tmore\tthan\tonce:\n\nfor\tword,\tcount\tin\twords_and_counts.most_common():\n\t\t\t\tif\tcount\t>\t1:\n\t\t\t\t\t\t\t\tprint\tword,\tcount\n\nwhich\tgives\tthe\tresults\tyou\u2019d\texpect\t(unless\tyou\texpect\t\u201cscikit-learn\u201d\tto\tget\tsplit\tinto\ttwo\nwords,\tin\twhich\tcase\tit\tdoesn\u2019t\tgive\tthe\tresults\tyou\texpect):\n\nlearning\t3\njava\t3\npython\t3\nbig\t3\ndata\t3\nhbase\t2\nregression\t2\ncassandra\t2\nstatistics\t2\nprobability\t2\nhadoop\t2",
    "32": "networks\t2\nmachine\t2\nneural\t2\nscikit-learn\t2\nr\t2\n\nWe\u2019ll\tlook\tat\tmore\tsophisticated\tways\tto\textract\ttopics\tfrom\tdata\tin\tChapter\t20.",
    "33": "Onward\n\nIt\u2019s\tbeen\ta\tsuccessful\tfirst\tday!\tExhausted,\tyou\tslip\tout\tof\tthe\tbuilding\tbefore\tanyone\telse\ncan\task\tyou\tfor\tanything\telse.\tGet\ta\tgood\tnight\u2019s\trest,\tbecause\ttomorrow\tis\tnew\temployee\norientation.\t(Yes,\tyou\twent\tthrough\ta\tfull\tday\tof\twork\tbefore\tnew\temployee\torientation.\nTake\tit\tup\twith\tHR.)",
    "34": "",
    "35": "Chapter\t2.\tA\tCrash\tCourse\tin\tPython\n\nPeople\tare\tstill\tcrazy\tabout\tPython\tafter\ttwenty-five\tyears,\twhich\tI\tfind\thard\tto\tbelieve.\n\nMichael\tPalin\n\nAll\tnew\temployees\tat\tDataSciencester\tare\trequired\tto\tgo\tthrough\tnew\temployee\norientation,\tthe\tmost\tinteresting\tpart\tof\twhich\tis\ta\tcrash\tcourse\tin\tPython.\n\nThis\tis\tnot\ta\tcomprehensive\tPython\ttutorial\tbut\tinstead\tis\tintended\tto\thighlight\tthe\tparts\tof\nthe\tlanguage\tthat\twill\tbe\tmost\timportant\tto\tus\t(some\tof\twhich\tare\toften\tnot\tthe\tfocus\tof\nPython\ttutorials).",
    "36": "The\tBasics",
    "37": "Getting\tPython\n\nYou\tcan\tdownload\tPython\tfrom\tpython.org.\tBut\tif\tyou\tdon\u2019t\talready\thave\tPython,\tI\nrecommend\tinstead\tinstalling\tthe\tAnaconda\tdistribution,\twhich\talready\tincludes\tmost\tof\nthe\tlibraries\tthat\tyou\tneed\tto\tdo\tdata\tscience.\n\nAs\tI\twrite\tthis,\tthe\tlatest\tversion\tof\tPython\tis\t3.4.\tAt\tDataSciencester,\thowever,\twe\tuse\nold,\treliable\tPython\t2.7.\tPython\t3\tis\tnot\tbackward-compatible\twith\tPython\t2,\tand\tmany\nimportant\tlibraries\tonly\twork\twell\twith\t2.7.\tThe\tdata\tscience\tcommunity\tis\tstill\tfirmly\nstuck\ton\t2.7,\twhich\tmeans\twe\twill\tbe,\ttoo.\tMake\tsure\tto\tget\tthat\tversion.\n\nIf\tyou\tdon\u2019t\tget\tAnaconda,\tmake\tsure\tto\tinstall\tpip,\twhich\tis\ta\tPython\tpackage\tmanager\nthat\tallows\tyou\tto\teasily\tinstall\tthird-party\tpackages\t(some\tof\twhich\twe\u2019ll\tneed).\tIt\u2019s\talso\nworth\tgetting\tIPython,\twhich\tis\ta\tmuch\tnicer\tPython\tshell\tto\twork\twith.\n\n(If\tyou\tinstalled\tAnaconda\tthen\tit\tshould\thave\tcome\twith\tpip\tand\tIPython.)\n\nJust\trun:\n\npip\tinstall\tipython\n\nand\tthen\tsearch\tthe\tInternet\tfor\tsolutions\tto\twhatever\tcryptic\terror\tmessages\tthat\tcauses.",
    "38": "The\tZen\tof\tPython\n\nPython\thas\ta\tsomewhat\tZen\tdescription\tof\tits\tdesign\tprinciples,\twhich\tyou\tcan\talso\tfind\ninside\tthe\tPython\tinterpreter\titself\tby\ttyping\timport\tthis.\n\nOne\tof\tthe\tmost\tdiscussed\tof\tthese\tis:\n\nThere\tshould\tbe\tone\t\u2014\tand\tpreferably\tonly\tone\t\u2014\tobvious\tway\tto\tdo\tit.\n\nCode\twritten\tin\taccordance\twith\tthis\t\u201cobvious\u201d\tway\t(which\tmay\tnot\tbe\tobvious\tat\tall\tto\ta\nnewcomer)\tis\toften\tdescribed\tas\t\u201cPythonic.\u201d\tAlthough\tthis\tis\tnot\ta\tbook\tabout\tPython,\twe\nwill\toccasionally\tcontrast\tPythonic\tand\tnon-Pythonic\tways\tof\taccomplishing\tthe\tsame\nthings,\tand\twe\twill\tgenerally\tfavor\tPythonic\tsolutions\tto\tour\tproblems.",
    "39": "Whitespace\tFormatting\n\nMany\tlanguages\tuse\tcurly\tbraces\tto\tdelimit\tblocks\tof\tcode.\tPython\tuses\tindentation:\n\nfor\ti\tin\t[1,\t2,\t3,\t4,\t5]:\n\t\t\t\tprint\ti\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tfirst\tline\tin\t\"for\ti\"\tblock\n\t\t\t\tfor\tj\tin\t[1,\t2,\t3,\t4,\t5]:\n\t\t\t\t\t\t\t\tprint\tj\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tfirst\tline\tin\t\"for\tj\"\tblock\n\t\t\t\t\t\t\t\tprint\ti\t+\tj\t\t\t\t\t\t\t\t\t\t\t\t\t#\tlast\tline\tin\t\"for\tj\"\tblock\n\t\t\t\tprint\ti\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tlast\tline\tin\t\"for\ti\"\tblock\nprint\t\"done\tlooping\"\n\nThis\tmakes\tPython\tcode\tvery\treadable,\tbut\tit\talso\tmeans\tthat\tyou\thave\tto\tbe\tvery\tcareful\nwith\tyour\tformatting.\tWhitespace\tis\tignored\tinside\tparentheses\tand\tbrackets,\twhich\tcan\tbe\nhelpful\tfor\tlong-winded\tcomputations:\n\nlong_winded_computation\t=\t(1\t+\t2\t+\t3\t+\t4\t+\t5\t+\t6\t+\t7\t+\t8\t+\t9\t+\t10\t+\t11\t+\t12\t+\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t13\t+\t14\t+\t15\t+\t16\t+\t17\t+\t18\t+\t19\t+\t20)\n\nand\tfor\tmaking\tcode\teasier\tto\tread:\n\nlist_of_lists\t=\t[[1,\t2,\t3],\t[4,\t5,\t6],\t[7,\t8,\t9]]\n\neasier_to_read_list_of_lists\t=\t[\t[1,\t2,\t3],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[4,\t5,\t6],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[7,\t8,\t9]\t]\n\nYou\tcan\talso\tuse\ta\tbackslash\tto\tindicate\tthat\ta\tstatement\tcontinues\tonto\tthe\tnext\tline,\nalthough\twe\u2019ll\trarely\tdo\tthis:\n\ntwo_plus_three\t=\t2\t+\t\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t3\n\nOne\tconsequence\tof\twhitespace\tformatting\tis\tthat\tit\tcan\tbe\thard\tto\tcopy\tand\tpaste\tcode\ninto\tthe\tPython\tshell.\tFor\texample,\tif\tyou\ttried\tto\tpaste\tthe\tcode:\n\nfor\ti\tin\t[1,\t2,\t3,\t4,\t5]:\n\n\t\t\t\t#\tnotice\tthe\tblank\tline\n\t\t\t\tprint\ti\n\ninto\tthe\tordinary\tPython\tshell,\tyou\twould\tget\ta:\n\nIndentationError:\texpected\tan\tindented\tblock\n\nbecause\tthe\tinterpreter\tthinks\tthe\tblank\tline\tsignals\tthe\tend\tof\tthe\tfor\tloop\u2019s\tblock.\n\nIPython\thas\ta\tmagic\tfunction\t%paste,\twhich\tcorrectly\tpastes\twhatever\tis\ton\tyour\nclipboard,\twhitespace\tand\tall.\tThis\talone\tis\ta\tgood\treason\tto\tuse\tIPython.",
    "40": "Modules\n\nCertain\tfeatures\tof\tPython\tare\tnot\tloaded\tby\tdefault.\tThese\tinclude\tboth\tfeatures\tincluded\nas\tpart\tof\tthe\tlanguage\tas\twell\tas\tthird-party\tfeatures\tthat\tyou\tdownload\tyourself.\tIn\torder\nto\tuse\tthese\tfeatures,\tyou\u2019ll\tneed\tto\timport\tthe\tmodules\tthat\tcontain\tthem.\n\nOne\tapproach\tis\tto\tsimply\timport\tthe\tmodule\titself:\n\nimport\tre\nmy_regex\t=\tre.compile(\"[0-9]+\",\tre.I)\n\nHere\tre\tis\tthe\tmodule\tcontaining\tfunctions\tand\tconstants\tfor\tworking\twith\tregular\nexpressions.\tAfter\tthis\ttype\tof\timport\tyou\tcan\tonly\taccess\tthose\tfunctions\tby\tprefixing\nthem\twith\tre..\n\nIf\tyou\talready\thad\ta\tdifferent\tre\tin\tyour\tcode\tyou\tcould\tuse\tan\talias:\n\nimport\tre\tas\tregex\nmy_regex\t=\tregex.compile(\"[0-9]+\",\tregex.I)\n\nYou\tmight\talso\tdo\tthis\tif\tyour\tmodule\thas\tan\tunwieldy\tname\tor\tif\tyou\u2019re\tgoing\tto\tbe\ntyping\tit\ta\tlot.\tFor\texample,\twhen\tvisualizing\tdata\twith\tmatplotlib,\ta\tstandard\nconvention\tis:\n\nimport\tmatplotlib.pyplot\tas\tplt\n\nIf\tyou\tneed\ta\tfew\tspecific\tvalues\tfrom\ta\tmodule,\tyou\tcan\timport\tthem\texplicitly\tand\tuse\nthem\twithout\tqualification:\n\nfrom\tcollections\timport\tdefaultdict,\tCounter\nlookup\t=\tdefaultdict(int)\nmy_counter\t=\tCounter()\n\nIf\tyou\twere\ta\tbad\tperson,\tyou\tcould\timport\tthe\tentire\tcontents\tof\ta\tmodule\tinto\tyour\nnamespace,\twhich\tmight\tinadvertently\toverwrite\tvariables\tyou\u2019ve\talready\tdefined:\n\nmatch\t=\t10\nfrom\tre\timport\t*\t\t\t\t#\tuh\toh,\tre\thas\ta\tmatch\tfunction\nprint\tmatch\t\t\t\t\t\t\t\t\t#\t\"<function\tre.match>\"\n\nHowever,\tsince\tyou\tare\tnot\ta\tbad\tperson,\tyou\twon\u2019t\tever\tdo\tthis.",
    "41": "Arithmetic\n\nPython\t2.7\tuses\tinteger\tdivision\tby\tdefault,\tso\tthat\t5\t/\t2\tequals\t2.\tAlmost\talways\tthis\tis\nnot\twhat\twe\twant,\tso\twe\twill\talways\tstart\tour\tfiles\twith:\n\nfrom\t__future__\timport\tdivision\n\nafter\twhich\t5\t/\t2\tequals\t2.5.\tEvery\tcode\texample\tin\tthis\tbook\tuses\tthis\tnew-style\ndivision.\tIn\tthe\thandful\tof\tcases\twhere\twe\tneed\tinteger\tdivision,\twe\tcan\tget\tit\twith\ta\ndouble\tslash:\t5\t//\t2.",
    "42": "Functions\n\nA\tfunction\tis\ta\trule\tfor\ttaking\tzero\tor\tmore\tinputs\tand\treturning\ta\tcorresponding\toutput.\tIn\nPython,\twe\ttypically\tdefine\tfunctions\tusing\tdef:\n\ndef\tdouble(x):\n\t\t\t\t\"\"\"this\tis\twhere\tyou\tput\tan\toptional\tdocstring\n\t\t\t\tthat\texplains\twhat\tthe\tfunction\tdoes.\n\t\t\t\tfor\texample,\tthis\tfunction\tmultiplies\tits\tinput\tby\t2\"\"\"\n\t\t\t\treturn\tx\t*\t2\n\nPython\tfunctions\tare\tfirst-class,\twhich\tmeans\tthat\twe\tcan\tassign\tthem\tto\tvariables\tand\npass\tthem\tinto\tfunctions\tjust\tlike\tany\tother\targuments:\n\ndef\tapply_to_one(f):\n\t\t\t\t\"\"\"calls\tthe\tfunction\tf\twith\t1\tas\tits\targument\"\"\"\n\t\t\t\treturn\tf(1)\n\nmy_double\t=\tdouble\t\t\t\t\t\t\t\t\t\t\t\t\t#\trefers\tto\tthe\tpreviously\tdefined\tfunction\nx\t=\tapply_to_one(my_double)\t\t\t\t#\tequals\t2\n\nIt\tis\talso\teasy\tto\tcreate\tshort\tanonymous\tfunctions,\tor\tlambdas:\n\ny\t=\tapply_to_one(lambda\tx:\tx\t+\t4)\t\t\t\t\t\t#\tequals\t5\n\nYou\tcan\tassign\tlambdas\tto\tvariables,\talthough\tmost\tpeople\twill\ttell\tyou\tthat\tyou\tshould\njust\tuse\tdef\tinstead:\n\nanother_double\t=\tlambda\tx:\t2\t*\tx\t\t\t\t\t\t\t#\tdon't\tdo\tthis\ndef\tanother_double(x):\treturn\t2\t*\tx\t\t\t\t#\tdo\tthis\tinstead\n\nFunction\tparameters\tcan\talso\tbe\tgiven\tdefault\targuments,\twhich\tonly\tneed\tto\tbe\tspecified\nwhen\tyou\twant\ta\tvalue\tother\tthan\tthe\tdefault:\n\ndef\tmy_print(message=\"my\tdefault\tmessage\"):\n\t\t\t\tprint\tmessage\n\nmy_print(\"hello\")\t\t\t#\tprints\t'hello'\nmy_print()\t\t\t\t\t\t\t\t\t\t#\tprints\t'my\tdefault\tmessage'\n\nIt\tis\tsometimes\tuseful\tto\tspecify\targuments\tby\tname:\n\ndef\tsubtract(a=0,\tb=0):\n\t\t\t\treturn\ta\t-\tb\n\nsubtract(10,\t5)\t#\treturns\t5\nsubtract(0,\t5)\t\t#\treturns\t-5\nsubtract(b=5)\t\t\t#\tsame\tas\tprevious\n\nWe\twill\tbe\tcreating\tmany,\tmany\tfunctions.",
    "43": "Strings\n\nStrings\tcan\tbe\tdelimited\tby\tsingle\tor\tdouble\tquotation\tmarks\t(but\tthe\tquotes\thave\tto\nmatch):\n\nsingle_quoted_string\t=\t'data\tscience'\ndouble_quoted_string\t=\t\"data\tscience\"\n\nPython\tuses\tbackslashes\tto\tencode\tspecial\tcharacters.\tFor\texample:\n\ntab_string\t=\t\"\\t\"\t\t\t\t\t\t\t#\trepresents\tthe\ttab\tcharacter\nlen(tab_string)\t\t\t\t\t\t\t\t\t#\tis\t1\n\nIf\tyou\twant\tbackslashes\tas\tbackslashes\t(which\tyou\tmight\tin\tWindows\tdirectory\tnames\tor\nin\tregular\texpressions),\tyou\tcan\tcreate\traw\tstrings\tusing\tr\"\":\n\nnot_tab_string\t=\tr\"\\t\"\t\t#\trepresents\tthe\tcharacters\t'\\'\tand\t't'\nlen(not_tab_string)\t\t\t\t\t#\tis\t2\n\nYou\tcan\tcreate\tmultiline\tstrings\tusing\ttriple-[double-]-quotes:\n\nmulti_line_string\t=\t\"\"\"This\tis\tthe\tfirst\tline.\nand\tthis\tis\tthe\tsecond\tline\nand\tthis\tis\tthe\tthird\tline\"\"\"",
    "44": "Exceptions\n\nWhen\tsomething\tgoes\twrong,\tPython\traises\tan\texception.\tUnhandled,\tthese\twill\tcause\nyour\tprogram\tto\tcrash.\tYou\tcan\thandle\tthem\tusing\ttry\tand\texcept:\n\ntry:\n\t\t\t\tprint\t0\t/\t0\nexcept\tZeroDivisionError:\n\t\t\t\tprint\t\"cannot\tdivide\tby\tzero\"\n\nAlthough\tin\tmany\tlanguages\texceptions\tare\tconsidered\tbad,\tin\tPython\tthere\tis\tno\tshame\nin\tusing\tthem\tto\tmake\tyour\tcode\tcleaner,\tand\twe\twill\toccasionally\tdo\tso.",
    "45": "Lists\n\nProbably\tthe\tmost\tfundamental\tdata\tstructure\tin\tPython\tis\tthe\tlist.\tA\tlist\tis\tsimply\tan\nordered\tcollection.\t(It\tis\tsimilar\tto\twhat\tin\tother\tlanguages\tmight\tbe\tcalled\tan\tarray,\tbut\nwith\tsome\tadded\tfunctionality.)\n\ninteger_list\t=\t[1,\t2,\t3]\nheterogeneous_list\t=\t[\"string\",\t0.1,\tTrue]\nlist_of_lists\t=\t[\tinteger_list,\theterogeneous_list,\t[]\t]\n\nlist_length\t=\tlen(integer_list)\t\t\t\t\t#\tequals\t3\nlist_sum\t\t\t\t=\tsum(integer_list)\t\t\t\t\t#\tequals\t6\n\nYou\tcan\tget\tor\tset\tthe\tnth\telement\tof\ta\tlist\twith\tsquare\tbrackets:\n\nx\t=\trange(10)\t\t\t#\tis\tthe\tlist\t[0,\t1,\t...,\t9]\nzero\t=\tx[0]\t\t\t\t\t#\tequals\t0,\tlists\tare\t0-indexed\none\t=\tx[1]\t\t\t\t\t\t#\tequals\t1\nnine\t=\tx[-1]\t\t\t\t#\tequals\t9,\t'Pythonic'\tfor\tlast\telement\neight\t=\tx[-2]\t\t\t#\tequals\t8,\t'Pythonic'\tfor\tnext-to-last\telement\nx[0]\t=\t-1\t\t\t\t\t\t\t#\tnow\tx\tis\t[-1,\t1,\t2,\t3,\t...,\t9]\n\nYou\tcan\talso\tuse\tsquare\tbrackets\tto\t\u201cslice\u201d\tlists:\n\nfirst_three\t\t\t=\tx[:3]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t[-1,\t1,\t2]\nthree_to_end\t=\tx[3:]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t[3,\t4,\t...,\t9]\none_to_four\t=\tx[1:5]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t[1,\t2,\t3,\t4]\nlast_three\t=\tx[-3:]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t[7,\t8,\t9]\nwithout_first_and_last\t=\tx[1:-1]\t\t\t\t#\t[1,\t2,\t...,\t8]\ncopy_of_x\t=\tx[:]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t[-1,\t1,\t2,\t...,\t9]\n\nPython\thas\tan\tin\toperator\tto\tcheck\tfor\tlist\tmembership:\n\n1\tin\t[1,\t2,\t3]\t\t\t\t#\tTrue\n0\tin\t[1,\t2,\t3]\t\t\t\t#\tFalse\n\nThis\tcheck\tinvolves\texamining\tthe\telements\tof\tthe\tlist\tone\tat\ta\ttime,\twhich\tmeans\tthat\nyou\tprobably\tshouldn\u2019t\tuse\tit\tunless\tyou\tknow\tyour\tlist\tis\tpretty\tsmall\t(or\tunless\tyou\ndon\u2019t\tcare\thow\tlong\tthe\tcheck\ttakes).\n\nIt\tis\teasy\tto\tconcatenate\tlists\ttogether:\n\nx\t=\t[1,\t2,\t3]\nx.extend([4,\t5,\t6])\t\t\t\t\t#\tx\tis\tnow\t[1,2,3,4,5,6]\n\nIf\tyou\tdon\u2019t\twant\tto\tmodify\tx\tyou\tcan\tuse\tlist\taddition:\n\nx\t=\t[1,\t2,\t3]\ny\t=\tx\t+\t[4,\t5,\t6]\t\t\t\t\t\t\t#\ty\tis\t[1,\t2,\t3,\t4,\t5,\t6];\tx\tis\tunchanged\n\nMore\tfrequently\twe\twill\tappend\tto\tlists\tone\titem\tat\ta\ttime:\n\nx\t=\t[1,\t2,\t3]\nx.append(0)\t\t\t\t\t\t#\tx\tis\tnow\t[1,\t2,\t3,\t0]\ny\t=\tx[-1]\t\t\t\t\t\t\t\t#\tequals\t0\nz\t=\tlen(x)\t\t\t\t\t\t\t#\tequals\t4",
    "46": "It\tis\toften\tconvenient\tto\tunpack\tlists\tif\tyou\tknow\thow\tmany\telements\tthey\tcontain:\n\nx,\ty\t=\t[1,\t2]\t\t\t\t#\tnow\tx\tis\t1,\ty\tis\t2\n\nalthough\tyou\twill\tget\ta\tValueError\tif\tyou\tdon\u2019t\thave\tthe\tsame\tnumbers\tof\telements\ton\nboth\tsides.\n\nIt\u2019s\tcommon\tto\tuse\tan\tunderscore\tfor\ta\tvalue\tyou\u2019re\tgoing\tto\tthrow\taway:\n\n_,\ty\t=\t[1,\t2]\t\t\t\t#\tnow\ty\t==\t2,\tdidn't\tcare\tabout\tthe\tfirst\telement",
    "47": "Tuples\n\nTuples\tare\tlists\u2019\timmutable\tcousins.\tPretty\tmuch\tanything\tyou\tcan\tdo\tto\ta\tlist\tthat\tdoesn\u2019t\ninvolve\tmodifying\tit,\tyou\tcan\tdo\tto\ta\ttuple.\tYou\tspecify\ta\ttuple\tby\tusing\tparentheses\t(or\nnothing)\tinstead\tof\tsquare\tbrackets:\n\nmy_list\t=\t[1,\t2]\nmy_tuple\t=\t(1,\t2)\nother_tuple\t=\t3,\t4\nmy_list[1]\t=\t3\t\t\t\t\t\t#\tmy_list\tis\tnow\t[1,\t3]\n\ntry:\n\t\t\t\tmy_tuple[1]\t=\t3\nexcept\tTypeError:\n\t\t\t\tprint\t\"cannot\tmodify\ta\ttuple\"\n\nTuples\tare\ta\tconvenient\tway\tto\treturn\tmultiple\tvalues\tfrom\tfunctions:\n\ndef\tsum_and_product(x,\ty):\n\t\t\t\treturn\t(x\t+\ty),(x\t*\ty)\n\nsp\t=\tsum_and_product(2,\t3)\t\t\t\t#\tequals\t(5,\t6)\ns,\tp\t=\tsum_and_product(5,\t10)\t#\ts\tis\t15,\tp\tis\t50\n\nTuples\t(and\tlists)\tcan\talso\tbe\tused\tfor\tmultiple\tassignment:\n\nx,\ty\t=\t1,\t2\t\t\t\t\t#\tnow\tx\tis\t1,\ty\tis\t2\nx,\ty\t=\ty,\tx\t\t\t\t\t#\tPythonic\tway\tto\tswap\tvariables;\tnow\tx\tis\t2,\ty\tis\t1",
    "48": "Dictionaries\n\nAnother\tfundamental\tdata\tstructure\tis\ta\tdictionary,\twhich\tassociates\tvalues\twith\tkeys\tand\nallows\tyou\tto\tquickly\tretrieve\tthe\tvalue\tcorresponding\tto\ta\tgiven\tkey:\n\nempty_dict\t=\t{}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tPythonic\nempty_dict2\t=\tdict()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tless\tPythonic\ngrades\t=\t{\t\"Joel\"\t:\t80,\t\"Tim\"\t:\t95\t}\t\t\t\t#\tdictionary\tliteral\n\nYou\tcan\tlook\tup\tthe\tvalue\tfor\ta\tkey\tusing\tsquare\tbrackets:\n\njoels_grade\t=\tgrades[\"Joel\"]\t\t\t\t\t\t\t\t\t\t\t\t#\tequals\t80\n\nBut\tyou\u2019ll\tget\ta\tKeyError\tif\tyou\task\tfor\ta\tkey\tthat\u2019s\tnot\tin\tthe\tdictionary:\n\ntry:\n\t\t\t\tkates_grade\t=\tgrades[\"Kate\"]\nexcept\tKeyError:\n\t\t\t\tprint\t\"no\tgrade\tfor\tKate!\"\n\nYou\tcan\tcheck\tfor\tthe\texistence\tof\ta\tkey\tusing\tin:\n\njoel_has_grade\t=\t\"Joel\"\tin\tgrades\t\t\t\t\t#\tTrue\nkate_has_grade\t=\t\"Kate\"\tin\tgrades\t\t\t\t\t#\tFalse\n\nDictionaries\thave\ta\tget\tmethod\tthat\treturns\ta\tdefault\tvalue\t(instead\tof\traising\tan\nexception)\twhen\tyou\tlook\tup\ta\tkey\tthat\u2019s\tnot\tin\tthe\tdictionary:\n\njoels_grade\t=\tgrades.get(\"Joel\",\t0)\t\t\t#\tequals\t80\nkates_grade\t=\tgrades.get(\"Kate\",\t0)\t\t\t#\tequals\t0\nno_ones_grade\t=\tgrades.get(\"No\tOne\")\t\t#\tdefault\tdefault\tis\tNone\n\nYou\tassign\tkey-value\tpairs\tusing\tthe\tsame\tsquare\tbrackets:\n\ngrades[\"Tim\"]\t=\t99\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\treplaces\tthe\told\tvalue\ngrades[\"Kate\"]\t=\t100\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tadds\ta\tthird\tentry\nnum_students\t=\tlen(grades)\t\t\t\t\t\t\t\t\t\t\t\t#\tequals\t3\n\nWe\twill\tfrequently\tuse\tdictionaries\tas\ta\tsimple\tway\tto\trepresent\tstructured\tdata:\n\ntweet\t=\t{\n\t\t\t\t\"user\"\t:\t\"joelgrus\",\n\t\t\t\t\"text\"\t:\t\"Data\tScience\tis\tAwesome\",\n\t\t\t\t\"retweet_count\"\t:\t100,\n\t\t\t\t\"hashtags\"\t:\t[\"#data\",\t\"#science\",\t\"#datascience\",\t\"#awesome\",\t\"#yolo\"]\n}\n\nBesides\tlooking\tfor\tspecific\tkeys\twe\tcan\tlook\tat\tall\tof\tthem:\n\ntweet_keys\t\t\t=\ttweet.keys()\t\t\t\t\t#\tlist\tof\tkeys\ntweet_values\t=\ttweet.values()\t\t\t#\tlist\tof\tvalues\ntweet_items\t\t=\ttweet.items()\t\t\t\t#\tlist\tof\t(key,\tvalue)\ttuples\n\n\"user\"\tin\ttweet_keys\t\t\t\t\t\t\t\t\t\t\t\t#\tTrue,\tbut\tuses\ta\tslow\tlist\tin\n\"user\"\tin\ttweet\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tmore\tPythonic,\tuses\tfaster\tdict\tin",
    "49": "\"joelgrus\"\tin\ttweet_values\t\t\t\t\t\t#\tTrue\n\nDictionary\tkeys\tmust\tbe\timmutable;\tin\tparticular,\tyou\tcannot\tuse\tlists\tas\tkeys.\tIf\tyou\nneed\ta\tmultipart\tkey,\tyou\tshould\tuse\ta\ttuple\tor\tfigure\tout\ta\tway\tto\tturn\tthe\tkey\tinto\ta\nstring.\n\ndefaultdict\n\nImagine\tthat\tyou\u2019re\ttrying\tto\tcount\tthe\twords\tin\ta\tdocument.\tAn\tobvious\tapproach\tis\tto\ncreate\ta\tdictionary\tin\twhich\tthe\tkeys\tare\twords\tand\tthe\tvalues\tare\tcounts.\tAs\tyou\tcheck\neach\tword,\tyou\tcan\tincrement\tits\tcount\tif\tit\u2019s\talready\tin\tthe\tdictionary\tand\tadd\tit\tto\tthe\ndictionary\tif\tit\u2019s\tnot:\n\nword_counts\t=\t{}\nfor\tword\tin\tdocument:\n\t\t\t\tif\tword\tin\tword_counts:\n\t\t\t\t\t\t\t\tword_counts[word]\t+=\t1\n\t\t\t\telse:\n\t\t\t\t\t\t\t\tword_counts[word]\t=\t1\n\nYou\tcould\talso\tuse\tthe\t\u201cforgiveness\tis\tbetter\tthan\tpermission\u201d\tapproach\tand\tjust\thandle\nthe\texception\tfrom\ttrying\tto\tlook\tup\ta\tmissing\tkey:\n\nword_counts\t=\t{}\nfor\tword\tin\tdocument:\n\t\t\t\ttry:\n\t\t\t\t\t\t\t\tword_counts[word]\t+=\t1\n\t\t\t\texcept\tKeyError:\n\t\t\t\t\t\t\t\tword_counts[word]\t=\t1\n\nA\tthird\tapproach\tis\tto\tuse\tget,\twhich\tbehaves\tgracefully\tfor\tmissing\tkeys:\n\nword_counts\t=\t{}\nfor\tword\tin\tdocument:\n\t\t\t\tprevious_count\t=\tword_counts.get(word,\t0)\n\t\t\t\tword_counts[word]\t=\tprevious_count\t+\t1\n\nEvery\tone\tof\tthese\tis\tslightly\tunwieldy,\twhich\tis\twhy\tdefaultdict\tis\tuseful.\tA\ndefaultdict\tis\tlike\ta\tregular\tdictionary,\texcept\tthat\twhen\tyou\ttry\tto\tlook\tup\ta\tkey\tit\ndoesn\u2019t\tcontain,\tit\tfirst\tadds\ta\tvalue\tfor\tit\tusing\ta\tzero-argument\tfunction\tyou\tprovided\nwhen\tyou\tcreated\tit.\tIn\torder\tto\tuse\tdefaultdicts,\tyou\thave\tto\timport\tthem\tfrom\ncollections:\n\nfrom\tcollections\timport\tdefaultdict\n\nword_counts\t=\tdefaultdict(int)\t\t\t\t\t\t\t\t\t\t#\tint()\tproduces\t0\nfor\tword\tin\tdocument:\n\t\t\t\tword_counts[word]\t+=\t1\n\nThey\tcan\talso\tbe\tuseful\twith\tlist\tor\tdict\tor\teven\tyour\town\tfunctions:\n\ndd_list\t=\tdefaultdict(list)\t\t\t\t\t\t\t\t\t\t\t\t\t#\tlist()\tproduces\tan\tempty\tlist\ndd_list[2].append(1)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tnow\tdd_list\tcontains\t{2:\t[1]}",
    "50": "dd_dict\t=\tdefaultdict(dict)\t\t\t\t\t\t\t\t\t\t\t\t\t#\tdict()\tproduces\tan\tempty\tdict\ndd_dict[\"Joel\"][\"City\"]\t=\t\"Seattle\"\t\t\t\t\t#\t{\t\"Joel\"\t:\t{\t\"City\"\t:\tSeattle\"}}\n\ndd_pair\t=\tdefaultdict(lambda:\t[0,\t0])\ndd_pair[2][1]\t=\t1\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tnow\tdd_pair\tcontains\t{2:\t[0,1]}\n\nThese\twill\tbe\tuseful\twhen\twe\u2019re\tusing\tdictionaries\tto\t\u201ccollect\u201d\tresults\tby\tsome\tkey\tand\ndon\u2019t\twant\tto\thave\tto\tcheck\tevery\ttime\tto\tsee\tif\tthe\tkey\texists\tyet.\n\nCounter\n\nA\tCounter\tturns\ta\tsequence\tof\tvalues\tinto\ta\tdefaultdict(int)-like\tobject\tmapping\tkeys\nto\tcounts.\tWe\twill\tprimarily\tuse\tit\tto\tcreate\thistograms:\n\nfrom\tcollections\timport\tCounter\nc\t=\tCounter([0,\t1,\t2,\t0])\t\t\t\t\t\t\t\t\t\t#\tc\tis\t(basically)\t{\t0\t:\t2,\t1\t:\t1,\t2\t:\t1\t}\n\nThis\tgives\tus\ta\tvery\tsimple\tway\tto\tsolve\tour\tword_counts\tproblem:\n\nword_counts\t=\tCounter(document)\n\nA\tCounter\tinstance\thas\ta\tmost_common\tmethod\tthat\tis\tfrequently\tuseful:\n\n#\tprint\tthe\t10\tmost\tcommon\twords\tand\ttheir\tcounts\nfor\tword,\tcount\tin\tword_counts.most_common(10):\n\t\t\t\tprint\tword,\tcount",
    "51": "Sets\n\nAnother\tdata\tstructure\tis\tset,\twhich\trepresents\ta\tcollection\tof\tdistinct\telements:\n\ns\t=\tset()\ns.add(1)\t\t\t\t\t\t\t#\ts\tis\tnow\t{\t1\t}\ns.add(2)\t\t\t\t\t\t\t#\ts\tis\tnow\t{\t1,\t2\t}\ns.add(2)\t\t\t\t\t\t\t#\ts\tis\tstill\t{\t1,\t2\t}\nx\t=\tlen(s)\t\t\t\t\t#\tequals\t2\ny\t=\t2\tin\ts\t\t\t\t\t#\tequals\tTrue\nz\t=\t3\tin\ts\t\t\t\t\t#\tequals\tFalse\n\nWe\u2019ll\tuse\tsets\tfor\ttwo\tmain\treasons.\tThe\tfirst\tis\tthat\tin\tis\ta\tvery\tfast\toperation\ton\tsets.\tIf\nwe\thave\ta\tlarge\tcollection\tof\titems\tthat\twe\twant\tto\tuse\tfor\ta\tmembership\ttest,\ta\tset\tis\tmore\nappropriate\tthan\ta\tlist:\n\nstopwords_list\t=\t[\"a\",\"an\",\"at\"]\t+\thundreds_of_other_words\t+\t[\"yet\",\t\"you\"]\n\n\"zip\"\tin\tstopwords_list\t\t\t\t\t#\tFalse,\tbut\thave\tto\tcheck\tevery\telement\n\nstopwords_set\t=\tset(stopwords_list)\n\"zip\"\tin\tstopwords_set\t\t\t\t\t\t#\tvery\tfast\tto\tcheck\n\nThe\tsecond\treason\tis\tto\tfind\tthe\tdistinct\titems\tin\ta\tcollection:\n\nitem_list\t=\t[1,\t2,\t3,\t1,\t2,\t3]\nnum_items\t=\tlen(item_list)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t6\nitem_set\t=\tset(item_list)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t{1,\t2,\t3}\nnum_distinct_items\t=\tlen(item_set)\t\t\t\t\t\t\t\t#\t3\ndistinct_item_list\t=\tlist(item_set)\t\t\t\t\t\t\t#\t[1,\t2,\t3]\n\nWe\u2019ll\tuse\tsets\tmuch\tless\tfrequently\tthan\tdicts\tand\tlists.",
    "52": "Control\tFlow\n\nAs\tin\tmost\tprogramming\tlanguages,\tyou\tcan\tperform\tan\taction\tconditionally\tusing\tif:\n\nif\t1\t>\t2:\n\t\t\t\tmessage\t=\t\"if\tonly\t1\twere\tgreater\tthan\ttwo\u2026\"\nelif\t1\t>\t3:\n\t\t\t\tmessage\t=\t\"elif\tstands\tfor\t'else\tif'\"\nelse:\n\t\t\t\tmessage\t=\t\"when\tall\telse\tfails\tuse\telse\t(if\tyou\twant\tto)\"\n\nYou\tcan\talso\twrite\ta\tternary\tif-then-else\ton\tone\tline,\twhich\twe\twill\tdo\toccasionally:\n\nparity\t=\t\"even\"\tif\tx\t%\t2\t==\t0\telse\t\"odd\"\n\nPython\thas\ta\twhile\tloop:\n\nx\t=\t0\nwhile\tx\t<\t10:\n\t\t\t\tprint\tx,\t\"is\tless\tthan\t10\"\n\t\t\t\tx\t+=\t1\n\nalthough\tmore\toften\twe\u2019ll\tuse\tfor\tand\tin:\n\nfor\tx\tin\trange(10):\n\t\t\t\tprint\tx,\t\"is\tless\tthan\t10\"\n\nIf\tyou\tneed\tmore-complex\tlogic,\tyou\tcan\tuse\tcontinue\tand\tbreak:\n\nfor\tx\tin\trange(10):\n\t\t\t\tif\tx\t==\t3:\n\t\t\t\t\t\t\t\tcontinue\t\t#\tgo\timmediately\tto\tthe\tnext\titeration\n\t\t\t\tif\tx\t==\t5:\n\t\t\t\t\t\t\t\tbreak\t\t\t\t\t#\tquit\tthe\tloop\tentirely\n\t\t\t\tprint\tx\n\nThis\twill\tprint\t0,\t1,\t2,\tand\t4.",
    "53": "Truthiness\n\nBooleans\tin\tPython\twork\tas\tin\tmost\tother\tlanguages,\texcept\tthat\tthey\u2019re\tcapitalized:\n\none_is_less_than_two\t=\t1\t<\t2\t\t\t\t\t\t\t\t\t\t#\tequals\tTrue\ntrue_equals_false\t=\tTrue\t==\tFalse\t\t\t\t\t#\tequals\tFalse\n\nPython\tuses\tthe\tvalue\tNone\tto\tindicate\ta\tnonexistent\tvalue.\tIt\tis\tsimilar\tto\tother\tlanguages\u2019\nnull:\n\nx\t=\tNone\nprint\tx\t==\tNone\t\t\t\t#\tprints\tTrue,\tbut\tis\tnot\tPythonic\nprint\tx\tis\tNone\t\t\t\t#\tprints\tTrue,\tand\tis\tPythonic\n\nPython\tlets\tyou\tuse\tany\tvalue\twhere\tit\texpects\ta\tBoolean.\tThe\tfollowing\tare\tall\t\u201cFalsy\u201d:\n\nFalse\n\nNone\n\n[]\t(an\tempty\tlist)\n\n{}\t(an\tempty\tdict)\n\n\"\"\n\nset()\n\n0\n\n0.0\n\nPretty\tmuch\tanything\telse\tgets\ttreated\tas\tTrue.\tThis\tallows\tyou\tto\teasily\tuse\tif\tstatements\nto\ttest\tfor\tempty\tlists\tor\tempty\tstrings\tor\tempty\tdictionaries\tor\tso\ton.\tIt\talso\tsometimes\ncauses\ttricky\tbugs\tif\tyou\u2019re\tnot\texpecting\tthis\tbehavior:\n\ns\t=\tsome_function_that_returns_a_string()\nif\ts:\n\t\t\t\tfirst_char\t=\ts[0]\nelse:\n\t\t\t\tfirst_char\t=\t\"\"\n\nA\tsimpler\tway\tof\tdoing\tthe\tsame\tis:\n\nfirst_char\t=\ts\tand\ts[0]\n\nsince\tand\treturns\tits\tsecond\tvalue\twhen\tthe\tfirst\tis\t\u201ctruthy,\u201d\tthe\tfirst\tvalue\twhen\tit\u2019s\tnot.\nSimilarly,\tif\tx\tis\teither\ta\tnumber\tor\tpossibly\tNone:\n\nsafe_x\t=\tx\tor\t0",
    "54": "is\tdefinitely\ta\tnumber.\n\nPython\thas\tan\tall\tfunction,\twhich\ttakes\ta\tlist\tand\treturns\tTrue\tprecisely\twhen\tevery\nelement\tis\ttruthy,\tand\tan\tany\tfunction,\twhich\treturns\tTrue\twhen\tat\tleast\tone\telement\tis\ntruthy:\n\nall([True,\t1,\t{\t3\t}])\t\t\t#\tTrue\nall([True,\t1,\t{}])\t\t\t\t\t\t#\tFalse,\t{}\tis\tfalsy\nany([True,\t1,\t{}])\t\t\t\t\t\t#\tTrue,\tTrue\tis\ttruthy\nall([])\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tTrue,\tno\tfalsy\telements\tin\tthe\tlist\nany([])\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tFalse,\tno\ttruthy\telements\tin\tthe\tlist",
    "55": "The\tNot-So-Basics\n\nHere\twe\u2019ll\tlook\tat\tsome\tmore-advanced\tPython\tfeatures\tthat\twe\u2019ll\tfind\tuseful\tfor\tworking\nwith\tdata.",
    "56": "Sorting\n\nEvery\tPython\tlist\thas\ta\tsort\tmethod\tthat\tsorts\tit\tin\tplace.\tIf\tyou\tdon\u2019t\twant\tto\tmess\tup\nyour\tlist,\tyou\tcan\tuse\tthe\tsorted\tfunction,\twhich\treturns\ta\tnew\tlist:\n\nx\t=\t[4,1,2,3]\ny\t=\tsorted(x)\t\t\t\t\t#\tis\t[1,2,3,4],\tx\tis\tunchanged\nx.sort()\t\t\t\t\t\t\t\t\t\t#\tnow\tx\tis\t[1,2,3,4]\n\nBy\tdefault,\tsort\t(and\tsorted)\tsort\ta\tlist\tfrom\tsmallest\tto\tlargest\tbased\ton\tnaively\ncomparing\tthe\telements\tto\tone\tanother.\n\nIf\tyou\twant\telements\tsorted\tfrom\tlargest\tto\tsmallest,\tyou\tcan\tspecify\ta\treverse=True\nparameter.\tAnd\tinstead\tof\tcomparing\tthe\telements\tthemselves,\tyou\tcan\tcompare\tthe\nresults\tof\ta\tfunction\tthat\tyou\tspecify\twith\tkey:\n\n#\tsort\tthe\tlist\tby\tabsolute\tvalue\tfrom\tlargest\tto\tsmallest\nx\t=\tsorted([-4,1,-2,3],\tkey=abs,\treverse=True)\t\t#\tis\t[-4,3,-2,1]\n\n#\tsort\tthe\twords\tand\tcounts\tfrom\thighest\tcount\tto\tlowest\nwc\t=\tsorted(word_counts.items(),\n\t\t\t\t\t\t\t\t\t\t\t\tkey=lambda\t(word,\tcount):\tcount,\n\t\t\t\t\t\t\t\t\t\t\t\treverse=True)",
    "57": "List\tComprehensions\n\nFrequently,\tyou\u2019ll\twant\tto\ttransform\ta\tlist\tinto\tanother\tlist,\tby\tchoosing\tonly\tcertain\nelements,\tor\tby\ttransforming\telements,\tor\tboth.\tThe\tPythonic\tway\tof\tdoing\tthis\tis\tlist\ncomprehensions:\n\neven_numbers\t=\t[x\tfor\tx\tin\trange(5)\tif\tx\t%\t2\t==\t0]\t\t#\t[0,\t2,\t4]\nsquares\t\t\t\t\t\t=\t[x\t*\tx\tfor\tx\tin\trange(5)]\t\t\t\t\t\t\t\t\t\t\t\t#\t[0,\t1,\t4,\t9,\t16]\neven_squares\t=\t[x\t*\tx\tfor\tx\tin\teven_numbers]\t\t\t\t\t\t\t\t#\t[0,\t4,\t16]\n\nYou\tcan\tsimilarly\tturn\tlists\tinto\tdictionaries\tor\tsets:\n\nsquare_dict\t=\t{\tx\t:\tx\t*\tx\tfor\tx\tin\trange(5)\t}\t\t#\t{\t0:0,\t1:1,\t2:4,\t3:9,\t4:16\t}\nsquare_set\t\t=\t{\tx\t*\tx\tfor\tx\tin\t[1,\t-1]\t}\t\t\t\t\t\t\t#\t{\t1\t}\n\nIf\tyou\tdon\u2019t\tneed\tthe\tvalue\tfrom\tthe\tlist,\tit\u2019s\tconventional\tto\tuse\tan\tunderscore\tas\tthe\nvariable:\n\nzeroes\t=\t[0\tfor\t_\tin\teven_numbers]\t\t\t\t\t\t#\thas\tthe\tsame\tlength\tas\teven_numbers\n\nA\tlist\tcomprehension\tcan\tinclude\tmultiple\tfors:\n\npairs\t=\t[(x,\ty)\n\t\t\t\t\t\t\t\t\tfor\tx\tin\trange(10)\n\t\t\t\t\t\t\t\t\tfor\ty\tin\trange(10)]\t\t\t#\t100\tpairs\t(0,0)\t(0,1)\t...\t(9,8),\t(9,9)\n\nand\tlater\tfors\tcan\tuse\tthe\tresults\tof\tearlier\tones:\n\nincreasing_pairs\t=\t[(x,\ty)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tonly\tpairs\twith\tx\t<\ty,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tx\tin\trange(10)\t\t\t\t\t\t\t\t\t\t\t#\trange(lo,\thi)\tequals\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\ty\tin\trange(x\t+\t1,\t10)]\t\t\t#\t[lo,\tlo\t+\t1,\t...,\thi\t-\t1]\n\nWe\twill\tuse\tlist\tcomprehensions\ta\tlot.",
    "58": "Generators\tand\tIterators\n\nA\tproblem\twith\tlists\tis\tthat\tthey\tcan\teasily\tgrow\tvery\tbig.\trange(1000000)\tcreates\tan\nactual\tlist\tof\t1\tmillion\telements.\tIf\tyou\tonly\tneed\tto\tdeal\twith\tthem\tone\tat\ta\ttime,\tthis\tcan\nbe\ta\thuge\tsource\tof\tinefficiency\t(or\tof\trunning\tout\tof\tmemory).\tIf\tyou\tpotentially\tonly\nneed\tthe\tfirst\tfew\tvalues,\tthen\tcalculating\tthem\tall\tis\ta\twaste.\n\nA\tgenerator\tis\tsomething\tthat\tyou\tcan\titerate\tover\t(for\tus,\tusually\tusing\tfor)\tbut\twhose\nvalues\tare\tproduced\tonly\tas\tneeded\t(lazily).\n\nOne\tway\tto\tcreate\tgenerators\tis\twith\tfunctions\tand\tthe\tyield\toperator:\n\ndef\tlazy_range(n):\n\t\t\t\t\"\"\"a\tlazy\tversion\tof\trange\"\"\"\n\t\t\t\ti\t=\t0\n\t\t\t\twhile\ti\t<\tn:\n\t\t\t\t\t\t\t\tyield\ti\n\t\t\t\t\t\t\t\ti\t+=\t1\n\nThe\tfollowing\tloop\twill\tconsume\tthe\tyielded\tvalues\tone\tat\ta\ttime\tuntil\tnone\tare\tleft:\n\nfor\ti\tin\tlazy_range(10):\n\t\t\t\tdo_something_with(i)\n\n(Python\tactually\tcomes\twith\ta\tlazy_range\tfunction\tcalled\txrange,\tand\tin\tPython\t3,\trange\nitself\tis\tlazy.)\tThis\tmeans\tyou\tcould\teven\tcreate\tan\tinfinite\tsequence:\n\ndef\tnatural_numbers():\n\t\t\t\t\"\"\"returns\t1,\t2,\t3,\t...\"\"\"\n\t\t\t\tn\t=\t1\n\t\t\t\twhile\tTrue:\n\t\t\t\t\t\t\t\tyield\tn\n\t\t\t\t\t\t\t\tn\t+=\t1\n\nalthough\tyou\tprobably\tshouldn\u2019t\titerate\tover\tit\twithout\tusing\tsome\tkind\tof\tbreak\tlogic.\n\nTIP\n\nThe\tflip\tside\tof\tlaziness\tis\tthat\tyou\tcan\tonly\titerate\tthrough\ta\tgenerator\tonce.\tIf\tyou\tneed\tto\titerate\tthrough\nsomething\tmultiple\ttimes,\tyou\u2019ll\tneed\tto\teither\trecreate\tthe\tgenerator\teach\ttime\tor\tuse\ta\tlist.\n\nA\tsecond\tway\tto\tcreate\tgenerators\tis\tby\tusing\tfor\tcomprehensions\twrapped\tin\nparentheses:\n\nlazy_evens_below_20\t=\t(i\tfor\ti\tin\tlazy_range(20)\tif\ti\t%\t2\t==\t0)\n\nRecall\talso\tthat\tevery\tdict\thas\tan\titems()\tmethod\tthat\treturns\ta\tlist\tof\tits\tkey-value\tpairs.\nMore\tfrequently\twe\u2019ll\tuse\tthe\titeritems()\tmethod,\twhich\tlazily\tyields\tthe\tkey-value\npairs\tone\tat\ta\ttime\tas\twe\titerate\tover\tit.",
    "59": "Randomness\n\nAs\twe\tlearn\tdata\tscience,\twe\twill\tfrequently\tneed\tto\tgenerate\trandom\tnumbers,\twhich\twe\ncan\tdo\twith\tthe\trandom\tmodule:\n\nimport\trandom\n\nfour_uniform_randoms\t=\t[random.random()\tfor\t_\tin\trange(4)]\n\n#\t\t[0.8444218515250481,\t\t\t\t\t\t#\trandom.random()\tproduces\tnumbers\n#\t\t\t0.7579544029403025,\t\t\t\t\t\t#\tuniformly\tbetween\t0\tand\t1\n#\t\t\t0.420571580830845,\t\t\t\t\t\t\t#\tit's\tthe\trandom\tfunction\twe'll\tuse\n#\t\t\t0.25891675029296335]\t\t\t\t\t#\tmost\toften\n\nThe\trandom\tmodule\tactually\tproduces\tpseudorandom\t(that\tis,\tdeterministic)\tnumbers\nbased\ton\tan\tinternal\tstate\tthat\tyou\tcan\tset\twith\trandom.seed\tif\tyou\twant\tto\tget\nreproducible\tresults:\n\nrandom.seed(10)\t\t\t\t\t\t\t\t\t#\tset\tthe\tseed\tto\t10\nprint\trandom.random()\t\t\t#\t0.57140259469\nrandom.seed(10)\t\t\t\t\t\t\t\t\t#\treset\tthe\tseed\tto\t10\nprint\trandom.random()\t\t\t#\t0.57140259469\tagain\n\nWe\u2019ll\tsometimes\tuse\trandom.randrange,\twhich\ttakes\teither\t1\tor\t2\targuments\tand\treturns\nan\telement\tchosen\trandomly\tfrom\tthe\tcorresponding\trange():\n\nrandom.randrange(10)\t\t\t\t#\tchoose\trandomly\tfrom\trange(10)\t=\t[0,\t1,\t...,\t9]\nrandom.randrange(3,\t6)\t\t#\tchoose\trandomly\tfrom\trange(3,\t6)\t=\t[3,\t4,\t5]\n\nThere\tare\ta\tfew\tmore\tmethods\tthat\twe\u2019ll\tsometimes\tfind\tconvenient.\trandom.shuffle\nrandomly\treorders\tthe\telements\tof\ta\tlist:\n\nup_to_ten\t=\trange(10)\nrandom.shuffle(up_to_ten)\nprint\tup_to_ten\n#\t[2,\t5,\t1,\t9,\t7,\t3,\t8,\t6,\t4,\t0]\t\t\t(your\tresults\twill\tprobably\tbe\tdifferent)\n\nIf\tyou\tneed\tto\trandomly\tpick\tone\telement\tfrom\ta\tlist\tyou\tcan\tuse\trandom.choice:\n\nmy_best_friend\t=\trandom.choice([\"Alice\",\t\"Bob\",\t\"Charlie\"])\t\t\t\t\t#\t\"Bob\"\tfor\tme\n\nAnd\tif\tyou\tneed\tto\trandomly\tchoose\ta\tsample\tof\telements\twithout\treplacement\t(i.e.,\twith\nno\tduplicates),\tyou\tcan\tuse\trandom.sample:\n\nlottery_numbers\t=\trange(60)\nwinning_numbers\t=\trandom.sample(lottery_numbers,\t6)\t\t#\t[16,\t36,\t10,\t6,\t25,\t9]\n\nTo\tchoose\ta\tsample\tof\telements\twith\treplacement\t(i.e.,\tallowing\tduplicates),\tyou\tcan\tjust\nmake\tmultiple\tcalls\tto\trandom.choice:\n\nfour_with_replacement\t=\t[random.choice(range(10))\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\t_\tin\trange(4)]\n#\t[9,\t4,\t4,\t2]",
    "60": "Regular\tExpressions\n\nRegular\texpressions\tprovide\ta\tway\tof\tsearching\ttext.\tThey\tare\tincredibly\tuseful\tbut\talso\nfairly\tcomplicated,\tso\tmuch\tso\tthat\tthere\tare\tentire\tbooks\twritten\tabout\tthem.\tWe\twill\nexplain\ttheir\tdetails\tthe\tfew\ttimes\twe\tencounter\tthem;\there\tare\ta\tfew\texamples\tof\thow\tto\nuse\tthem\tin\tPython:\n\nimport\tre\n\nprint\tall([\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tall\tof\tthese\tare\ttrue,\tbecause\n\t\t\t\tnot\tre.match(\"a\",\t\"cat\"),\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t*\t'cat'\tdoesn't\tstart\twith\t'a'\n\t\t\t\tre.search(\"a\",\t\"cat\"),\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t*\t'cat'\thas\tan\t'a'\tin\tit\n\t\t\t\tnot\tre.search(\"c\",\t\"dog\"),\t\t\t\t\t\t\t\t\t\t\t\t\t#\t*\t'dog'\tdoesn't\thave\ta\t'c'\tin\tit\n\t\t\t\t3\t==\tlen(re.split(\"[ab]\",\t\"carbs\")),\t\t\t#\t*\tsplit\ton\ta\tor\tb\tto\t['c','r','s']\n\t\t\t\t\"R-D-\"\t==\tre.sub(\"[0-9]\",\t\"-\",\t\"R2D2\")\t#\t*\treplace\tdigits\twith\tdashes\n\t\t\t\t])\t\t#\tprints\tTrue",
    "61": "Object-Oriented\tProgramming\n\nLike\tmany\tlanguages,\tPython\tallows\tyou\tto\tdefine\tclasses\tthat\tencapsulate\tdata\tand\tthe\nfunctions\tthat\toperate\ton\tthem.\tWe\u2019ll\tuse\tthem\tsometimes\tto\tmake\tour\tcode\tcleaner\tand\nsimpler.\tIt\u2019s\tprobably\tsimplest\tto\texplain\tthem\tby\tconstructing\ta\theavily\tannotated\nexample.\n\nImagine\twe\tdidn\u2019t\thave\tthe\tbuilt-in\tPython\tset.\tThen\twe\tmight\twant\tto\tcreate\tour\town\nSet\tclass.\n\nWhat\tbehavior\tshould\tour\tclass\thave?\tGiven\tan\tinstance\tof\tSet,\twe\u2019ll\tneed\tto\tbe\table\tto\nadd\titems\tto\tit,\tremove\titems\tfrom\tit,\tand\tcheck\twhether\tit\tcontains\ta\tcertain\tvalue.\tWe\u2019ll\ncreate\tall\tof\tthese\tas\tmember\tfunctions,\twhich\tmeans\twe\u2019ll\taccess\tthem\twith\ta\tdot\tafter\ta\nSet\tobject:\n\n#\tby\tconvention,\twe\tgive\tclasses\tPascalCase\tnames\nclass\tSet:\n\n\t\t\t\t#\tthese\tare\tthe\tmember\tfunctions\n\t\t\t\t#\tevery\tone\ttakes\ta\tfirst\tparameter\t\"self\"\t(another\tconvention)\n\t\t\t\t#\tthat\trefers\tto\tthe\tparticular\tSet\tobject\tbeing\tused\n\n\t\t\t\tdef\t__init__(self,\tvalues=None):\n\t\t\t\t\t\t\t\t\"\"\"This\tis\tthe\tconstructor.\n\t\t\t\t\t\t\t\tIt\tgets\tcalled\twhen\tyou\tcreate\ta\tnew\tSet.\n\t\t\t\t\t\t\t\tYou\twould\tuse\tit\tlike\n\t\t\t\t\t\t\t\ts1\t=\tSet()\t\t\t\t\t\t\t\t\t\t#\tempty\tset\n\t\t\t\t\t\t\t\ts2\t=\tSet([1,2,2,3])\t#\tinitialize\twith\tvalues\"\"\"\n\n\t\t\t\t\t\t\t\tself.dict\t=\t{}\t#\teach\tinstance\tof\tSet\thas\tits\town\tdict\tproperty\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\twhich\tis\twhat\twe'll\tuse\tto\ttrack\tmemberships\n\t\t\t\t\t\t\t\tif\tvalues\tis\tnot\tNone:\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tvalue\tin\tvalues:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.add(value)\n\n\t\t\t\tdef\t__repr__(self):\n\t\t\t\t\t\t\t\t\"\"\"this\tis\tthe\tstring\trepresentation\tof\ta\tSet\tobject\n\t\t\t\t\t\t\t\tif\tyou\ttype\tit\tat\tthe\tPython\tprompt\tor\tpass\tit\tto\tstr()\"\"\"\n\t\t\t\t\t\t\t\treturn\t\"Set:\t\"\t+\tstr(self.dict.keys())\n\n\t\t\t\t#\twe'll\trepresent\tmembership\tby\tbeing\ta\tkey\tin\tself.dict\twith\tvalue\tTrue\n\t\t\t\tdef\tadd(self,\tvalue):\n\t\t\t\t\t\t\t\tself.dict[value]\t=\tTrue\n\n\t\t\t\t#\tvalue\tis\tin\tthe\tSet\tif\tit's\ta\tkey\tin\tthe\tdictionary\n\t\t\t\tdef\tcontains(self,\tvalue):\n\t\t\t\t\t\t\t\treturn\tvalue\tin\tself.dict\n\n\t\t\t\tdef\tremove(self,\tvalue):\n\t\t\t\t\t\t\t\tdel\tself.dict[value]\n\nWhich\twe\tcould\tthen\tuse\tlike:\n\ns\t=\tSet([1,2,3])\ns.add(4)\nprint\ts.contains(4)\t\t\t\t\t#\tTrue\ns.remove(3)\nprint\ts.contains(3)\t\t\t\t\t#\tFalse",
    "62": "Functional\tTools\n\nWhen\tpassing\tfunctions\taround,\tsometimes\twe\u2019ll\twant\tto\tpartially\tapply\t(or\tcurry)\nfunctions\tto\tcreate\tnew\tfunctions.\tAs\ta\tsimple\texample,\timagine\tthat\twe\thave\ta\tfunction\nof\ttwo\tvariables:\n\ndef\texp(base,\tpower):\n\t\t\t\treturn\tbase\t**\tpower\n\nand\twe\twant\tto\tuse\tit\tto\tcreate\ta\tfunction\tof\tone\tvariable\ttwo_to_the\twhose\tinput\tis\ta\npower\tand\twhose\toutput\tis\tthe\tresult\tof\texp(2,\tpower).\n\nWe\tcan,\tof\tcourse,\tdo\tthis\twith\tdef,\tbut\tthis\tcan\tsometimes\tget\tunwieldy:\n\ndef\ttwo_to_the(power):\n\t\t\t\treturn\texp(2,\tpower)\n\nA\tdifferent\tapproach\tis\tto\tuse\tfunctools.partial:\n\nfrom\tfunctools\timport\tpartial\ntwo_to_the\t=\tpartial(exp,\t2)\t\t\t\t\t#\tis\tnow\ta\tfunction\tof\tone\tvariable\nprint\ttwo_to_the(3)\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t8\n\nYou\tcan\talso\tuse\tpartial\tto\tfill\tin\tlater\targuments\tif\tyou\tspecify\ttheir\tnames:\n\nsquare_of\t=\tpartial(exp,\tpower=2)\nprint\tsquare_of(3)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t9\n\nIt\tstarts\tto\tget\tmessy\tif\tyou\tcurry\targuments\tin\tthe\tmiddle\tof\tthe\tfunction,\tso\twe\u2019ll\ttry\tto\navoid\tdoing\tthat.\n\nWe\twill\talso\toccasionally\tuse\tmap,\treduce,\tand\tfilter,\twhich\tprovide\tfunctional\nalternatives\tto\tlist\tcomprehensions:\n\ndef\tdouble(x):\n\t\t\t\treturn\t2\t*\tx\n\nxs\t=\t[1,\t2,\t3,\t4]\ntwice_xs\t=\t[double(x)\tfor\tx\tin\txs]\t\t\t\t\t\t\t\t#\t[2,\t4,\t6,\t8]\ntwice_xs\t=\tmap(double,\txs)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tsame\tas\tabove\nlist_doubler\t=\tpartial(map,\tdouble)\t\t\t\t\t\t\t#\t*function*\tthat\tdoubles\ta\tlist\ntwice_xs\t=\tlist_doubler(xs)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tagain\t[2,\t4,\t6,\t8]\n\nYou\tcan\tuse\tmap\twith\tmultiple-argument\tfunctions\tif\tyou\tprovide\tmultiple\tlists:\n\ndef\tmultiply(x,\ty):\treturn\tx\t*\ty\n\nproducts\t=\tmap(multiply,\t[1,\t2],\t[4,\t5])\t#\t[1\t*\t4,\t2\t*\t5]\t=\t[4,\t10]\n\nSimilarly,\tfilter\tdoes\tthe\twork\tof\ta\tlist-comprehension\tif:\n\ndef\tis_even(x):\n\t\t\t\t\"\"\"True\tif\tx\tis\teven,\tFalse\tif\tx\tis\todd\"\"\"\n\t\t\t\treturn\tx\t%\t2\t==\t0",
    "63": "x_evens\t=\t[x\tfor\tx\tin\txs\tif\tis_even(x)]\t\t\t\t#\t[2,\t4]\nx_evens\t=\tfilter(is_even,\txs)\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tsame\tas\tabove\nlist_evener\t=\tpartial(filter,\tis_even)\t\t\t\t\t#\t*function*\tthat\tfilters\ta\tlist\nx_evens\t=\tlist_evener(xs)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tagain\t[2,\t4]\n\nAnd\treduce\tcombines\tthe\tfirst\ttwo\telements\tof\ta\tlist,\tthen\tthat\tresult\twith\tthe\tthird,\tthat\nresult\twith\tthe\tfourth,\tand\tso\ton,\tproducing\ta\tsingle\tresult:\n\nx_product\t=\treduce(multiply,\txs)\t\t\t\t\t\t\t\t\t\t\t#\t=\t1\t*\t2\t*\t3\t*\t4\t=\t24\nlist_product\t=\tpartial(reduce,\tmultiply)\t\t\t#\t*function*\tthat\treduces\ta\tlist\nx_product\t=\tlist_product(xs)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tagain\t=\t24",
    "64": "enumerate\n\nNot\tinfrequently,\tyou\u2019ll\twant\tto\titerate\tover\ta\tlist\tand\tuse\tboth\tits\telements\tand\ttheir\nindexes:\n\n#\tnot\tPythonic\nfor\ti\tin\trange(len(documents)):\n\t\t\t\tdocument\t=\tdocuments[i]\n\t\t\t\tdo_something(i,\tdocument)\n\n#\talso\tnot\tPythonic\ni\t=\t0\nfor\tdocument\tin\tdocuments:\n\t\t\t\tdo_something(i,\tdocument)\n\t\t\t\ti\t+=\t1\n\nThe\tPythonic\tsolution\tis\tenumerate,\twhich\tproduces\ttuples\t(index,\telement):\n\nfor\ti,\tdocument\tin\tenumerate(documents):\n\t\t\t\tdo_something(i,\tdocument)\n\nSimilarly,\tif\twe\tjust\twant\tthe\tindexes:\n\nfor\ti\tin\trange(len(documents)):\tdo_something(i)\t\t\t\t\t#\tnot\tPythonic\nfor\ti,\t_\tin\tenumerate(documents):\tdo_something(i)\t\t\t#\tPythonic\n\nWe\u2019ll\tuse\tthis\ta\tlot.",
    "65": "zip\tand\tArgument\tUnpacking\n\nOften\twe\twill\tneed\tto\tzip\ttwo\tor\tmore\tlists\ttogether.\tzip\ttransforms\tmultiple\tlists\tinto\ta\nsingle\tlist\tof\ttuples\tof\tcorresponding\telements:\n\nlist1\t=\t['a',\t'b',\t'c']\nlist2\t=\t[1,\t2,\t3]\nzip(list1,\tlist2)\t\t\t\t\t\t\t\t#\tis\t[('a',\t1),\t('b',\t2),\t('c',\t3)]\n\nIf\tthe\tlists\tare\tdifferent\tlengths,\tzip\tstops\tas\tsoon\tas\tthe\tfirst\tlist\tends.\n\nYou\tcan\talso\t\u201cunzip\u201d\ta\tlist\tusing\ta\tstrange\ttrick:\n\npairs\t=\t[('a',\t1),\t('b',\t2),\t('c',\t3)]\nletters,\tnumbers\t=\tzip(*pairs)\n\nThe\tasterisk\tperforms\targument\tunpacking,\twhich\tuses\tthe\telements\tof\tpairs\tas\nindividual\targuments\tto\tzip.\tIt\tends\tup\tthe\tsame\tas\tif\tyou\u2019d\tcalled:\n\nzip(('a',\t1),\t('b',\t2),\t('c',\t3))\n\nwhich\treturns\t[('a','b','c'),\t('1','2','3')].\n\nYou\tcan\tuse\targument\tunpacking\twith\tany\tfunction:\n\ndef\tadd(a,\tb):\treturn\ta\t+\tb\n\nadd(1,\t2)\t\t\t\t\t\t#\treturns\t3\nadd([1,\t2])\t\t\t\t#\tTypeError!\nadd(*[1,\t2])\t\t\t#\treturns\t3\n\nIt\tis\trare\tthat\twe\u2019ll\tfind\tthis\tuseful,\tbut\twhen\twe\tdo\tit\u2019s\ta\tneat\ttrick.",
    "66": "args\tand\tkwargs\n\nLet\u2019s\tsay\twe\twant\tto\tcreate\ta\thigher-order\tfunction\tthat\ttakes\tas\tinput\tsome\tfunction\tf\tand\nreturns\ta\tnew\tfunction\tthat\tfor\tany\tinput\treturns\ttwice\tthe\tvalue\tof\tf:\n\ndef\tdoubler(f):\n\t\t\t\tdef\tg(x):\n\t\t\t\t\t\t\t\treturn\t2\t*\tf(x)\n\t\t\t\treturn\tg\n\nThis\tworks\tin\tsome\tcases:\n\ndef\tf1(x):\n\t\t\t\treturn\tx\t+\t1\n\ng\t=\tdoubler(f1)\nprint\tg(3)\t\t\t\t\t\t\t\t\t\t#\t8\t(==\t(\t3\t+\t1)\t*\t2)\nprint\tg(-1)\t\t\t\t\t\t\t\t\t#\t0\t(==\t(-1\t+\t1)\t*\t2)\n\nHowever,\tit\tbreaks\tdown\twith\tfunctions\tthat\ttake\tmore\tthan\ta\tsingle\targument:\n\ndef\tf2(x,\ty):\n\t\t\t\treturn\tx\t+\ty\n\ng\t=\tdoubler(f2)\nprint\tg(1,\t2)\t\t\t\t#\tTypeError:\tg()\ttakes\texactly\t1\targument\t(2\tgiven)\n\nWhat\twe\tneed\tis\ta\tway\tto\tspecify\ta\tfunction\tthat\ttakes\tarbitrary\targuments.\tWe\tcan\tdo\tthis\nwith\targument\tunpacking\tand\ta\tlittle\tbit\tof\tmagic:\n\ndef\tmagic(*args,\t**kwargs):\n\t\t\t\tprint\t\"unnamed\targs:\",\targs\n\t\t\t\tprint\t\"keyword\targs:\",\tkwargs\n\nmagic(1,\t2,\tkey=\"word\",\tkey2=\"word2\")\n\n#\tprints\n#\t\tunnamed\targs:\t(1,\t2)\n#\t\tkeyword\targs:\t{'key2':\t'word2',\t'key':\t'word'}\n\nThat\tis,\twhen\twe\tdefine\ta\tfunction\tlike\tthis,\targs\tis\ta\ttuple\tof\tits\tunnamed\targuments\tand\nkwargs\tis\ta\tdict\tof\tits\tnamed\targuments.\tIt\tworks\tthe\tother\tway\ttoo,\tif\tyou\twant\tto\tuse\ta\nlist\t(or\ttuple)\tand\tdict\tto\tsupply\targuments\tto\ta\tfunction:\n\ndef\tother_way_magic(x,\ty,\tz):\n\t\t\t\treturn\tx\t+\ty\t+\tz\n\nx_y_list\t=\t[1,\t2]\nz_dict\t=\t{\t\"z\"\t:\t3\t}\nprint\tother_way_magic(*x_y_list,\t**z_dict)\t\t\t#\t6\n\nYou\tcould\tdo\tall\tsorts\tof\tstrange\ttricks\twith\tthis;\twe\twill\tonly\tuse\tit\tto\tproduce\thigher-\norder\tfunctions\twhose\tinputs\tcan\taccept\tarbitrary\targuments:\n\ndef\tdoubler_correct(f):\n\t\t\t\t\"\"\"works\tno\tmatter\twhat\tkind\tof\tinputs\tf\texpects\"\"\"\n\t\t\t\tdef\tg(*args,\t**kwargs):",
    "67": "\"\"\"whatever\targuments\tg\tis\tsupplied,\tpass\tthem\tthrough\tto\tf\"\"\"\n\t\t\t\t\t\t\t\treturn\t2\t*\tf(*args,\t**kwargs)\n\t\t\t\treturn\tg\n\ng\t=\tdoubler_correct(f2)\nprint\tg(1,\t2)\t#\t6",
    "68": "Welcome\tto\tDataSciencester!\n\nThis\tconcludes\tnew-employee\torientation.\tOh,\tand\talso,\ttry\tnot\tto\tembezzle\tanything.",
    "69": "For\tFurther\tExploration\n\nThere\tis\tno\tshortage\tof\tPython\ttutorials\tin\tthe\tworld.\tThe\tofficial\tone\tis\tnot\ta\tbad\tplace\nto\tstart.\n\nThe\tofficial\tIPython\ttutorial\tis\tnot\tquite\tas\tgood.\tYou\tmight\tbe\tbetter\toff\twith\ttheir\nvideos\tand\tpresentations.\tAlternatively,\tWes\tMcKinney\u2019s\tPython\tfor\tData\tAnalysis\n(O\u2019Reilly)\thas\ta\treally\tgood\tIPython\tchapter.",
    "70": "",
    "71": "Chapter\t3.\tVisualizing\tData\n\nI\tbelieve\tthat\tvisualization\tis\tone\tof\tthe\tmost\tpowerful\tmeans\tof\tachieving\tpersonal\ngoals.\n\nHarvey\tMackay\n\nA\tfundamental\tpart\tof\tthe\tdata\tscientist\u2019s\ttoolkit\tis\tdata\tvisualization.\tAlthough\tit\tis\tvery\neasy\tto\tcreate\tvisualizations,\tit\u2019s\tmuch\tharder\tto\tproduce\tgood\tones.\n\nThere\tare\ttwo\tprimary\tuses\tfor\tdata\tvisualization:\n\nTo\texplore\tdata\n\nTo\tcommunicate\tdata\n\nIn\tthis\tchapter,\twe\twill\tconcentrate\ton\tbuilding\tthe\tskills\tthat\tyou\u2019ll\tneed\tto\tstart\texploring\nyour\town\tdata\tand\tto\tproduce\tthe\tvisualizations\twe\u2019ll\tbe\tusing\tthroughout\tthe\trest\tof\tthe\nbook.\tLike\tmost\tof\tour\tchapter\ttopics,\tdata\tvisualization\tis\ta\trich\tfield\tof\tstudy\tthat\ndeserves\tits\town\tbook.\tNonetheless,\twe\u2019ll\ttry\tto\tgive\tyou\ta\tsense\tof\twhat\tmakes\tfor\ta\ngood\tvisualization\tand\twhat\tdoesn\u2019t.",
    "72": "matplotlib\n\nA\twide\tvariety\tof\ttools\texists\tfor\tvisualizing\tdata.\tWe\twill\tbe\tusing\tthe\tmatplotlib\nlibrary,\twhich\tis\twidely\tused\t(although\tsort\tof\tshowing\tits\tage).\tIf\tyou\tare\tinterested\tin\nproducing\telaborate\tinteractive\tvisualizations\tfor\tthe\tWeb,\tit\tis\tlikely\tnot\tthe\tright\tchoice,\nbut\tfor\tsimple\tbar\tcharts,\tline\tcharts,\tand\tscatterplots,\tit\tworks\tpretty\twell.\n\nIn\tparticular,\twe\twill\tbe\tusing\tthe\tmatplotlib.pyplot\tmodule.\tIn\tits\tsimplest\tuse,\tpyplot\nmaintains\tan\tinternal\tstate\tin\twhich\tyou\tbuild\tup\ta\tvisualization\tstep\tby\tstep.\tOnce\tyou\u2019re\ndone,\tyou\tcan\tsave\tit\t(with\tsavefig())\tor\tdisplay\tit\t(with\tshow()).\n\nFor\texample,\tmaking\tsimple\tplots\t(like\tFigure\t3-1)\tis\tpretty\tsimple:\n\nfrom\tmatplotlib\timport\tpyplot\tas\tplt\n\nyears\t=\t[1950,\t1960,\t1970,\t1980,\t1990,\t2000,\t2010]\ngdp\t=\t[300.2,\t543.3,\t1075.9,\t2862.5,\t5979.6,\t10289.7,\t14958.3]\n\n#\tcreate\ta\tline\tchart,\tyears\ton\tx-axis,\tgdp\ton\ty-axis\nplt.plot(years,\tgdp,\tcolor='green',\tmarker='o',\tlinestyle='solid')\n\n#\tadd\ta\ttitle\nplt.title(\"Nominal\tGDP\")\n\n#\tadd\ta\tlabel\tto\tthe\ty-axis\nplt.ylabel(\"Billions\tof\t$\")\nplt.show()\n\nFigure\t3-1.\tA\tsimple\tline\tchart",
    "73": "Making\tplots\tthat\tlook\tpublication-quality\tgood\tis\tmore\tcomplicated\tand\tbeyond\tthe\nscope\tof\tthis\tchapter.\tThere\tare\tmany\tways\tyou\tcan\tcustomize\tyour\tcharts\twith\t(for\nexample)\taxis\tlabels,\tline\tstyles,\tand\tpoint\tmarkers.\tRather\tthan\tattempt\ta\tcomprehensive\ntreatment\tof\tthese\toptions,\twe\u2019ll\tjust\tuse\t(and\tcall\tattention\tto)\tsome\tof\tthem\tin\tour\nexamples.\n\nAlthough\twe\twon\u2019t\tbe\tusing\tmuch\tof\tthis\tfunctionality,\tmatplotlib\tis\tcapable\tof\tproducing\tcomplicated\nplots\twithin\tplots,\tsophisticated\tformatting,\tand\tinteractive\tvisualizations.\tCheck\tout\tits\tdocumentation\tif\nyou\twant\tto\tgo\tdeeper\tthan\twe\tdo\tin\tthis\tbook.\n\nNOTE",
    "74": "Bar\tCharts\n\nA\tbar\tchart\tis\ta\tgood\tchoice\twhen\tyou\twant\tto\tshow\thow\tsome\tquantity\tvaries\tamong\nsome\tdiscrete\tset\tof\titems.\tFor\tinstance,\tFigure\t3-2\tshows\thow\tmany\tAcademy\tAwards\nwere\twon\tby\teach\tof\ta\tvariety\tof\tmovies:\n\nmovies\t=\t[\"Annie\tHall\",\t\"Ben-Hur\",\t\"Casablanca\",\t\"Gandhi\",\t\"West\tSide\tStory\"]\nnum_oscars\t=\t[5,\t11,\t3,\t8,\t10]\n\n#\tbars\tare\tby\tdefault\twidth\t0.8,\tso\twe'll\tadd\t0.1\tto\tthe\tleft\tcoordinates\n#\tso\tthat\teach\tbar\tis\tcentered\nxs\t=\t[i\t+\t0.1\tfor\ti,\t_\tin\tenumerate(movies)]\n\n#\tplot\tbars\twith\tleft\tx-coordinates\t[xs],\theights\t[num_oscars]\nplt.bar(xs,\tnum_oscars)\n\nplt.ylabel(\"#\tof\tAcademy\tAwards\")\nplt.title(\"My\tFavorite\tMovies\")\n\n#\tlabel\tx-axis\twith\tmovie\tnames\tat\tbar\tcenters\nplt.xticks([i\t+\t0.5\tfor\ti,\t_\tin\tenumerate(movies)],\tmovies)\n\nplt.show()\n\nFigure\t3-2.\tA\tsimple\tbar\tchart\n\nA\tbar\tchart\tcan\talso\tbe\ta\tgood\tchoice\tfor\tplotting\thistograms\tof\tbucketed\tnumeric\tvalues,\nin\torder\tto\tvisually\texplore\thow\tthe\tvalues\tare\tdistributed,\tas\tin\tFigure\t3-3:",
    "75": "grades\t=\t[83,95,91,87,70,0,85,82,100,67,73,77,0]\ndecile\t=\tlambda\tgrade:\tgrade\t//\t10\t*\t10\nhistogram\t=\tCounter(decile(grade)\tfor\tgrade\tin\tgrades)\n\nplt.bar([x\t-\t4\tfor\tx\tin\thistogram.keys()],\t#\tshift\teach\tbar\tto\tthe\tleft\tby\t4\n\t\t\t\t\t\t\t\thistogram.values(),\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tgive\teach\tbar\tits\tcorrect\theight\n\t\t\t\t\t\t\t\t8)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tgive\teach\tbar\ta\twidth\tof\t8\n\nplt.axis([-5,\t105,\t0,\t5])\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tx-axis\tfrom\t-5\tto\t105,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\ty-axis\tfrom\t0\tto\t5\n\nplt.xticks([10\t*\ti\tfor\ti\tin\trange(11)])\t\t\t\t#\tx-axis\tlabels\tat\t0,\t10,\t...,\t100\nplt.xlabel(\"Decile\")\nplt.ylabel(\"#\tof\tStudents\")\nplt.title(\"Distribution\tof\tExam\t1\tGrades\")\nplt.show()\n\nFigure\t3-3.\tUsing\ta\tbar\tchart\tfor\ta\thistogram\n\nThe\tthird\targument\tto\tplt.bar\tspecifies\tthe\tbar\twidth.\tHere\twe\tchose\ta\twidth\tof\t8\t(which\nleaves\ta\tsmall\tgap\tbetween\tbars,\tsince\tour\tbuckets\thave\twidth\t10).\tAnd\twe\tshifted\tthe\tbar\nleft\tby\t4,\tso\tthat\t(for\texample)\tthe\t\u201c80\u201d\tbar\thas\tits\tleft\tand\tright\tsides\tat\t76\tand\t84,\tand\n(hence)\tits\tcenter\tat\t80.\n\nThe\tcall\tto\tplt.axis\tindicates\tthat\twe\twant\tthe\tx-axis\tto\trange\tfrom\t-5\tto\t105\t(so\tthat\tthe\n\u201c0\u201d\tand\t\u201c100\u201d\tbars\tare\tfully\tshown),\tand\tthat\tthe\ty-axis\tshould\trange\tfrom\t0\tto\t5.\tAnd\tthe\ncall\tto\tplt.xticks\tputs\tx-axis\tlabels\tat\t0,\t10,\t20,\t\u2026,\t100.",
    "76": "Be\tjudicious\twhen\tusing\tplt.axis().\tWhen\tcreating\tbar\tcharts\tit\tis\tconsidered\tespecially\nbad\tform\tfor\tyour\ty-axis\tnot\tto\tstart\tat\t0,\tsince\tthis\tis\tan\teasy\tway\tto\tmislead\tpeople\n(Figure\t3-4):\n\nmentions\t=\t[500,\t505]\nyears\t=\t[2013,\t2014]\n\nplt.bar([2012.6,\t2013.6],\tmentions,\t0.8)\nplt.xticks(years)\nplt.ylabel(\"#\tof\ttimes\tI\theard\tsomeone\tsay\t'data\tscience'\")\n\n#\tif\tyou\tdon't\tdo\tthis,\tmatplotlib\twill\tlabel\tthe\tx-axis\t0,\t1\n#\tand\tthen\tadd\ta\t+2.013e3\toff\tin\tthe\tcorner\t(bad\tmatplotlib!)\nplt.ticklabel_format(useOffset=False)\n\n#\tmisleading\ty-axis\tonly\tshows\tthe\tpart\tabove\t500\nplt.axis([2012.5,2014.5,499,506])\nplt.title(\"Look\tat\tthe\t'Huge'\tIncrease!\")\nplt.show()\n\nIn\tFigure\t3-5,\twe\tuse\tmore-sensible\taxes,\tand\tit\tlooks\tfar\tless\timpressive:\n\nFigure\t3-4.\tA\tchart\twith\ta\tmisleading\ty-axis\n\nplt.axis([2012.5,2014.5,0,550])\nplt.title(\"Not\tSo\tHuge\tAnymore\")\nplt.show()",
    "77": "Figure\t3-5.\tThe\tsame\tchart\twith\ta\tnonmisleading\ty-axis",
    "78": "Line\tCharts\n\nAs\twe\tsaw\talready,\twe\tcan\tmake\tline\tcharts\tusing\tplt.plot().\tThese\tare\ta\tgood\tchoice\nfor\tshowing\ttrends,\tas\tillustrated\tin\tFigure\t3-6:\n\nvariance\t\t\t\t\t=\t[1,\t2,\t4,\t8,\t16,\t32,\t64,\t128,\t256]\nbias_squared\t=\t[256,\t128,\t64,\t32,\t16,\t8,\t4,\t2,\t1]\ntotal_error\t\t=\t[x\t+\ty\tfor\tx,\ty\tin\tzip(variance,\tbias_squared)]\nxs\t=\t[i\tfor\ti,\t_\tin\tenumerate(variance)]\n\n#\twe\tcan\tmake\tmultiple\tcalls\tto\tplt.plot\n#\tto\tshow\tmultiple\tseries\ton\tthe\tsame\tchart\nplt.plot(xs,\tvariance,\t\t\t\t\t'g-',\t\tlabel='variance')\t\t\t\t#\tgreen\tsolid\tline\nplt.plot(xs,\tbias_squared,\t'r-.',\tlabel='bias^2')\t\t\t\t\t\t#\tred\tdot-dashed\tline\nplt.plot(xs,\ttotal_error,\t\t'b:',\t\tlabel='total\terror')\t#\tblue\tdotted\tline\n\n#\tbecause\twe've\tassigned\tlabels\tto\teach\tseries\n#\twe\tcan\tget\ta\tlegend\tfor\tfree\n#\tloc=9\tmeans\t\"top\tcenter\"\nplt.legend(loc=9)\nplt.xlabel(\"model\tcomplexity\")\nplt.title(\"The\tBias-Variance\tTradeoff\")\nplt.show()\n\nFigure\t3-6.\tSeveral\tline\tcharts\twith\ta\tlegend",
    "79": "Scatterplots\n\nA\tscatterplot\tis\tthe\tright\tchoice\tfor\tvisualizing\tthe\trelationship\tbetween\ttwo\tpaired\tsets\tof\ndata.\tFor\texample,\tFigure\t3-7\tillustrates\tthe\trelationship\tbetween\tthe\tnumber\tof\tfriends\nyour\tusers\thave\tand\tthe\tnumber\tof\tminutes\tthey\tspend\ton\tthe\tsite\tevery\tday:\n\nfriends\t=\t[\t70,\t\t65,\t\t72,\t\t63,\t\t71,\t\t64,\t\t60,\t\t64,\t\t67]\nminutes\t=\t[175,\t170,\t205,\t120,\t220,\t130,\t105,\t145,\t190]\nlabels\t=\t\t['a',\t'b',\t'c',\t'd',\t'e',\t'f',\t'g',\t'h',\t'i']\n\nplt.scatter(friends,\tminutes)\n\n#\tlabel\teach\tpoint\nfor\tlabel,\tfriend_count,\tminute_count\tin\tzip(labels,\tfriends,\tminutes):\n\t\t\t\tplt.annotate(label,\n\t\t\t\t\t\t\t\txy=(friend_count,\tminute_count),\t#\tput\tthe\tlabel\twith\tits\tpoint\n\t\t\t\t\t\t\t\txytext=(5,\t-5),\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tbut\tslightly\toffset\n\t\t\t\t\t\t\t\ttextcoords='offset\tpoints')\n\nplt.title(\"Daily\tMinutes\tvs.\tNumber\tof\tFriends\")\nplt.xlabel(\"#\tof\tfriends\")\nplt.ylabel(\"daily\tminutes\tspent\ton\tthe\tsite\")\nplt.show()\n\nFigure\t3-7.\tA\tscatterplot\tof\tfriends\tand\ttime\ton\tthe\tsite\n\nIf\tyou\u2019re\tscattering\tcomparable\tvariables,\tyou\tmight\tget\ta\tmisleading\tpicture\tif\tyou\tlet\nmatplotlib\tchoose\tthe\tscale,\tas\tin\tFigure\t3-8:",
    "80": "test_1_grades\t=\t[\t99,\t90,\t85,\t97,\t80]\ntest_2_grades\t=\t[100,\t85,\t60,\t90,\t70]\n\nplt.scatter(test_1_grades,\ttest_2_grades)\nplt.title(\"Axes\tAren't\tComparable\")\nplt.xlabel(\"test\t1\tgrade\")\nplt.ylabel(\"test\t2\tgrade\")\nplt.show()\n\nFigure\t3-8.\tA\tscatterplot\twith\tuncomparable\taxes\n\nIf\twe\tinclude\ta\tcall\tto\tplt.axis(\"equal\"),\tthe\tplot\t(Figure\t3-9)\tmore\taccurately\tshows\nthat\tmost\tof\tthe\tvariation\toccurs\ton\ttest\t2.\n\nThat\u2019s\tenough\tto\tget\tyou\tstarted\tdoing\tvisualization.\tWe\u2019ll\tlearn\tmuch\tmore\tabout\nvisualization\tthroughout\tthe\tbook.",
    "81": "Figure\t3-9.\tThe\tsame\tscatterplot\twith\tequal\taxes",
    "82": "For\tFurther\tExploration\n\nseaborn\tis\tbuilt\ton\ttop\tof\tmatplotlib\tand\tallows\tyou\tto\teasily\tproduce\tprettier\t(and\nmore\tcomplex)\tvisualizations.\n\nD3.js\tis\ta\tJavaScript\tlibrary\tfor\tproducing\tsophisticated\tinteractive\tvisualizations\tfor\nthe\tweb.\tAlthough\tit\tis\tnot\tin\tPython,\tit\tis\tboth\ttrendy\tand\twidely\tused,\tand\tit\tis\twell\nworth\tyour\twhile\tto\tbe\tfamiliar\twith\tit.\n\nBokeh\tis\ta\tnewer\tlibrary\tthat\tbrings\tD3-style\tvisualizations\tinto\tPython.\n\nggplot\tis\ta\tPython\tport\tof\tthe\tpopular\tR\tlibrary\tggplot2,\twhich\tis\twidely\tused\tfor\ncreating\t\u201cpublication\tquality\u201d\tcharts\tand\tgraphics.\tIt\u2019s\tprobably\tmost\tinteresting\tif\nyou\u2019re\talready\tan\tavid\tggplot2\tuser,\tand\tpossibly\ta\tlittle\topaque\tif\tyou\u2019re\tnot.",
    "83": "",
    "84": "Chapter\t4.\tLinear\tAlgebra\n\nIs\tthere\tanything\tmore\tuseless\tor\tless\tuseful\tthan\tAlgebra?\n\nBilly\tConnolly\n\nLinear\talgebra\tis\tthe\tbranch\tof\tmathematics\tthat\tdeals\twith\tvector\tspaces.\tAlthough\tI\ncan\u2019t\thope\tto\tteach\tyou\tlinear\talgebra\tin\ta\tbrief\tchapter,\tit\tunderpins\ta\tlarge\tnumber\tof\ndata\tscience\tconcepts\tand\ttechniques,\twhich\tmeans\tI\towe\tit\tto\tyou\tto\tat\tleast\ttry.\tWhat\twe\nlearn\tin\tthis\tchapter\twe\u2019ll\tuse\theavily\tthroughout\tthe\trest\tof\tthe\tbook.",
    "85": "Vectors\n\nAbstractly,\tvectors\tare\tobjects\tthat\tcan\tbe\tadded\ttogether\t(to\tform\tnew\tvectors)\tand\tthat\ncan\tbe\tmultiplied\tby\tscalars\t(i.e.,\tnumbers),\talso\tto\tform\tnew\tvectors.\n\nConcretely\t(for\tus),\tvectors\tare\tpoints\tin\tsome\tfinite-dimensional\tspace.\tAlthough\tyou\nmight\tnot\tthink\tof\tyour\tdata\tas\tvectors,\tthey\tare\ta\tgood\tway\tto\trepresent\tnumeric\tdata.\n\nFor\texample,\tif\tyou\thave\tthe\theights,\tweights,\tand\tages\tof\ta\tlarge\tnumber\tof\tpeople,\tyou\ncan\ttreat\tyour\tdata\tas\tthree-dimensional\tvectors\t(height,\tweight,\tage).\tIf\tyou\u2019re\nteaching\ta\tclass\twith\tfour\texams,\tyou\tcan\ttreat\tstudent\tgrades\tas\tfour-dimensional\tvectors\n(exam1,\texam2,\texam3,\texam4).\n\nThe\tsimplest\tfrom-scratch\tapproach\tis\tto\trepresent\tvectors\tas\tlists\tof\tnumbers.\tA\tlist\tof\nthree\tnumbers\tcorresponds\tto\ta\tvector\tin\tthree-dimensional\tspace,\tand\tvice\tversa:\n\nheight_weight_age\t=\t[70,\t\t#\tinches,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t170,\t#\tpounds,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t40\t]\t#\tyears\n\ngrades\t=\t[95,\t\t\t#\texam1\n\t\t\t\t\t\t\t\t\t\t80,\t\t\t#\texam2\n\t\t\t\t\t\t\t\t\t\t75,\t\t\t#\texam3\n\t\t\t\t\t\t\t\t\t\t62\t]\t\t#\texam4\n\nOne\tproblem\twith\tthis\tapproach\tis\tthat\twe\twill\twant\tto\tperform\tarithmetic\ton\tvectors.\nBecause\tPython\tlists\taren\u2019t\tvectors\t(and\thence\tprovide\tno\tfacilities\tfor\tvector\tarithmetic),\nwe\u2019ll\tneed\tto\tbuild\tthese\tarithmetic\ttools\tourselves.\tSo\tlet\u2019s\tstart\twith\tthat.\n\nTo\tbegin\twith,\twe\u2019ll\tfrequently\tneed\tto\tadd\ttwo\tvectors.\tVectors\tadd\tcomponentwise.\tThis\nmeans\tthat\tif\ttwo\tvectors\tv\tand\tw\tare\tthe\tsame\tlength,\ttheir\tsum\tis\tjust\tthe\tvector\twhose\nfirst\telement\tis\tv[0]\t+\tw[0],\twhose\tsecond\telement\tis\tv[1]\t+\tw[1],\tand\tso\ton.\t(If\tthey\u2019re\nnot\tthe\tsame\tlength,\tthen\twe\u2019re\tnot\tallowed\tto\tadd\tthem.)\n\nFor\texample,\tadding\tthe\tvectors\t[1,\t2]\tand\t[2,\t1]\tresults\tin\t[1\t+\t2,\t2\t+\t1]\tor\t[3,\t3],\nas\tshown\tin\tFigure\t4-1.",
    "86": "Figure\t4-1.\tAdding\ttwo\tvectors\n\nWe\tcan\teasily\timplement\tthis\tby\tzip-ing\tthe\tvectors\ttogether\tand\tusing\ta\tlist\ncomprehension\tto\tadd\tthe\tcorresponding\telements:\n\ndef\tvector_add(v,\tw):\n\t\t\t\t\"\"\"adds\tcorresponding\telements\"\"\"\n\t\t\t\treturn\t[v_i\t+\tw_i\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tv_i,\tw_i\tin\tzip(v,\tw)]\n\nSimilarly,\tto\tsubtract\ttwo\tvectors\twe\tjust\tsubtract\tcorresponding\telements:\n\ndef\tvector_subtract(v,\tw):\n\t\t\t\t\"\"\"subtracts\tcorresponding\telements\"\"\"\n\t\t\t\treturn\t[v_i\t-\tw_i\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tv_i,\tw_i\tin\tzip(v,\tw)]\n\nWe\u2019ll\talso\tsometimes\twant\tto\tcomponentwise\tsum\ta\tlist\tof\tvectors.\tThat\tis,\tcreate\ta\tnew\nvector\twhose\tfirst\telement\tis\tthe\tsum\tof\tall\tthe\tfirst\telements,\twhose\tsecond\telement\tis\tthe\nsum\tof\tall\tthe\tsecond\telements,\tand\tso\ton.\tThe\teasiest\tway\tto\tdo\tthis\tis\tby\tadding\tone\nvector\tat\ta\ttime:\n\ndef\tvector_sum(vectors):\n\t\t\t\t\"\"\"sums\tall\tcorresponding\telements\"\"\"\n\t\t\t\tresult\t=\tvectors[0]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tstart\twith\tthe\tfirst\tvector\n\t\t\t\tfor\tvector\tin\tvectors[1:]:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tthen\tloop\tover\tthe\tothers\n\t\t\t\t\t\t\t\tresult\t=\tvector_add(result,\tvector)\t\t\t\t\t#\tand\tadd\tthem\tto\tthe\tresult\n\t\t\t\treturn\tresult",
    "87": "If\tyou\tthink\tabout\tit,\twe\tare\tjust\treduce-ing\tthe\tlist\tof\tvectors\tusing\tvector_add,\twhich\nmeans\twe\tcan\trewrite\tthis\tmore\tbriefly\tusing\thigher-order\tfunctions:\n\ndef\tvector_sum(vectors):\n\t\t\t\treturn\treduce(vector_add,\tvectors)\n\nor\teven:\n\nvector_sum\t=\tpartial(reduce,\tvector_add)\n\nalthough\tthis\tlast\tone\tis\tprobably\tmore\tclever\tthan\thelpful.\n\nWe\u2019ll\talso\tneed\tto\tbe\table\tto\tmultiply\ta\tvector\tby\ta\tscalar,\twhich\twe\tdo\tsimply\tby\nmultiplying\teach\telement\tof\tthe\tvector\tby\tthat\tnumber:\n\ndef\tscalar_multiply(c,\tv):\n\t\t\t\t\"\"\"c\tis\ta\tnumber,\tv\tis\ta\tvector\"\"\"\n\t\t\t\treturn\t[c\t*\tv_i\tfor\tv_i\tin\tv]\n\nThis\tallows\tus\tto\tcompute\tthe\tcomponentwise\tmeans\tof\ta\tlist\tof\t(same-sized)\tvectors:\n\ndef\tvector_mean(vectors):\n\t\t\t\t\"\"\"compute\tthe\tvector\twhose\tith\telement\tis\tthe\tmean\tof\tthe\n\t\t\t\tith\telements\tof\tthe\tinput\tvectors\"\"\"\n\t\t\t\tn\t=\tlen(vectors)\n\t\t\t\treturn\tscalar_multiply(1/n,\tvector_sum(vectors))\n\nA\tless\tobvious\ttool\tis\tthe\tdot\tproduct.\tThe\tdot\tproduct\tof\ttwo\tvectors\tis\tthe\tsum\tof\ttheir\ncomponentwise\tproducts:\n\ndef\tdot(v,\tw):\n\t\t\t\t\"\"\"v_1\t*\tw_1\t+\t...\t+\tv_n\t*\tw_n\"\"\"\n\t\t\t\treturn\tsum(v_i\t*\tw_i\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tv_i,\tw_i\tin\tzip(v,\tw))\n\nThe\tdot\tproduct\tmeasures\thow\tfar\tthe\tvector\tv\textends\tin\tthe\tw\tdirection.\tFor\texample,\tif\nw\t=\t[1,\t0]\tthen\tdot(v,\tw)\tis\tjust\tthe\tfirst\tcomponent\tof\tv.\tAnother\tway\tof\tsaying\tthis\tis\nthat\tit\u2019s\tthe\tlength\tof\tthe\tvector\tyou\u2019d\tget\tif\tyou\tprojected\tv\tonto\tw\t(Figure\t4-2).",
    "88": "Figure\t4-2.\tThe\tdot\tproduct\tas\tvector\tprojection\n\nUsing\tthis,\tit\u2019s\teasy\tto\tcompute\ta\tvector\u2019s\tsum\tof\tsquares:\n\ndef\tsum_of_squares(v):\n\t\t\t\t\"\"\"v_1\t*\tv_1\t+\t...\t+\tv_n\t*\tv_n\"\"\"\n\t\t\t\treturn\tdot(v,\tv)\n\nWhich\twe\tcan\tuse\tto\tcompute\tits\tmagnitude\t(or\tlength):\n\nimport\tmath\n\ndef\tmagnitude(v):\n\t\t\t\treturn\tmath.sqrt(sum_of_squares(v))\t\t\t#\tmath.sqrt\tis\tsquare\troot\tfunction\n\nWe\tnow\thave\tall\tthe\tpieces\twe\tneed\tto\tcompute\tthe\tdistance\tbetween\ttwo\tvectors,\tdefined\nas:\n\ndef\tsquared_distance(v,\tw):\n\t\t\t\t\"\"\"(v_1\t-\tw_1)\t**\t2\t+\t...\t+\t(v_n\t-\tw_n)\t**\t2\"\"\"\n\t\t\t\treturn\tsum_of_squares(vector_subtract(v,\tw))\n\ndef\tdistance(v,\tw):",
    "89": "return\tmath.sqrt(squared_distance(v,\tw))\n\nWhich\tis\tpossibly\tclearer\tif\twe\twrite\tit\tas\t(the\tequivalent):\n\ndef\tdistance(v,\tw):\n\t\t\t\treturn\tmagnitude(vector_subtract(v,\tw))\n\nThat\tshould\tbe\tplenty\tto\tget\tus\tstarted.\tWe\u2019ll\tbe\tusing\tthese\tfunctions\theavily\tthroughout\nthe\tbook.\n\nNOTE\n\nUsing\tlists\tas\tvectors\tis\tgreat\tfor\texposition\tbut\tterrible\tfor\tperformance.\n\nIn\tproduction\tcode,\tyou\twould\twant\tto\tuse\tthe\tNumPy\tlibrary,\twhich\tincludes\ta\thigh-performance\tarray\nclass\twith\tall\tsorts\tof\tarithmetic\toperations\tincluded.",
    "90": "Matrices\n\nA\tmatrix\tis\ta\ttwo-dimensional\tcollection\tof\tnumbers.\tWe\twill\trepresent\tmatrices\tas\tlists\nof\tlists,\twith\teach\tinner\tlist\thaving\tthe\tsame\tsize\tand\trepresenting\ta\trow\tof\tthe\tmatrix.\tIf\nA\tis\ta\tmatrix,\tthen\tA[i][j]\tis\tthe\telement\tin\tthe\tith\trow\tand\tthe\tjth\tcolumn.\tPer\nmathematical\tconvention,\twe\twill\ttypically\tuse\tcapital\tletters\tto\trepresent\tmatrices.\tFor\nexample:\n\nA\t=\t[[1,\t2,\t3],\t\t#\tA\thas\t2\trows\tand\t3\tcolumns\n\t\t\t\t\t[4,\t5,\t6]]\n\nB\t=\t[[1,\t2],\t\t\t\t\t#\tB\thas\t3\trows\tand\t2\tcolumns\n\t\t\t\t\t[3,\t4],\n\t\t\t\t\t[5,\t6]]\n\nNOTE\n\nIn\tmathematics,\tyou\twould\tusually\tname\tthe\tfirst\trow\tof\tthe\tmatrix\t\u201crow\t1\u201d\tand\tthe\tfirst\tcolumn\t\u201ccolumn\n1.\u201d\tBecause\twe\u2019re\trepresenting\tmatrices\twith\tPython\tlists,\twhich\tare\tzero-indexed,\twe\u2019ll\tcall\tthe\tfirst\trow\nof\ta\tmatrix\t\u201crow\t0\u201d\tand\tthe\tfirst\tcolumn\t\u201ccolumn\t0.\u201d\n\nGiven\tthis\tlist-of-lists\trepresentation,\tthe\tmatrix\tA\thas\tlen(A)\trows\tand\tlen(A[0])\ncolumns,\twhich\twe\tconsider\tits\tshape:\n\ndef\tshape(A):\n\t\t\t\tnum_rows\t=\tlen(A)\n\t\t\t\tnum_cols\t=\tlen(A[0])\tif\tA\telse\t0\t\t\t#\tnumber\tof\telements\tin\tfirst\trow\n\t\t\t\treturn\tnum_rows,\tnum_cols\n\nIf\ta\tmatrix\thas\tn\trows\tand\tk\tcolumns,\twe\twill\trefer\tto\tit\tas\ta\t\nsometimes\twill)\tthink\tof\teach\trow\tof\ta\t\ncolumn\tas\ta\tvector\tof\tlength\tn:\n\n\tmatrix\tas\ta\tvector\tof\tlength\tk,\tand\teach\n\n\tmatrix.\tWe\tcan\t(and\n\ndef\tget_row(A,\ti):\n\t\t\t\treturn\tA[i]\t\t\t\t\t\t\t\t\t\t\t\t\t#\tA[i]\tis\talready\tthe\tith\trow\n\ndef\tget_column(A,\tj):\n\t\t\t\treturn\t[A_i[j]\t\t\t\t\t\t\t\t\t\t#\tjth\telement\tof\trow\tA_i\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tA_i\tin\tA]\t\t\t#\tfor\teach\trow\tA_i\n\nWe\u2019ll\talso\twant\tto\tbe\table\tto\tcreate\ta\tmatrix\tgiven\tits\tshape\tand\ta\tfunction\tfor\tgenerating\nits\telements.\tWe\tcan\tdo\tthis\tusing\ta\tnested\tlist\tcomprehension:\n\ndef\tmake_matrix(num_rows,\tnum_cols,\tentry_fn):\n\t\t\t\t\"\"\"returns\ta\tnum_rows\tx\tnum_cols\tmatrix\n\t\t\t\twhose\t(i,j)th\tentry\tis\tentry_fn(i,\tj)\"\"\"\n\t\t\t\treturn\t[[entry_fn(i,\tj)\t\t\t\t\t\t\t\t\t\t\t\t\t#\tgiven\ti,\tcreate\ta\tlist\n\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tj\tin\trange(num_cols)]\t\t#\t\t\t[entry_fn(i,\t0),\t...\t]\n\t\t\t\t\t\t\t\t\t\t\t\tfor\ti\tin\trange(num_rows)]\t\t\t#\tcreate\tone\tlist\tfor\teach\ti\n\nGiven\tthis\tfunction,\tyou\tcould\tmake\ta\t5\t\u00d7\t5\tidentity\tmatrix\t(with\t1s\ton\tthe\tdiagonal\tand\n0s\telsewhere)\twith:\n\ndef\tis_diagonal(i,\tj):",
    "91": "\"\"\"1's\ton\tthe\t'diagonal',\t0's\teverywhere\telse\"\"\"\n\t\t\t\treturn\t1\tif\ti\t==\tj\telse\t0\n\nidentity_matrix\t=\tmake_matrix(5,\t5,\tis_diagonal)\n\n#\t[[1,\t0,\t0,\t0,\t0],\n#\t\t[0,\t1,\t0,\t0,\t0],\n#\t\t[0,\t0,\t1,\t0,\t0],\n#\t\t[0,\t0,\t0,\t1,\t0],\n#\t\t[0,\t0,\t0,\t0,\t1]]\n\nMatrices\twill\tbe\timportant\tto\tus\tfor\tseveral\treasons.\n\nFirst,\twe\tcan\tuse\ta\tmatrix\tto\trepresent\ta\tdata\tset\tconsisting\tof\tmultiple\tvectors,\tsimply\tby\nconsidering\teach\tvector\tas\ta\trow\tof\tthe\tmatrix.\tFor\texample,\tif\tyou\thad\tthe\theights,\n\nweights,\tand\tages\tof\t1,000\tpeople\tyou\tcould\tput\tthem\tin\ta\t\n\n\tmatrix:\n\ndata\t=\t[[70,\t170,\t40],\n\t\t\t\t\t\t\t\t[65,\t120,\t26],\n\t\t\t\t\t\t\t\t[77,\t250,\t19],\n\t\t\t\t\t\t\t\t#\t....\n\t\t\t\t\t\t\t]\n\nSecond,\tas\twe\u2019ll\tsee\tlater,\twe\tcan\tuse\tan\t\nmaps\tk-dimensional\tvectors\tto\tn-dimensional\tvectors.\tSeveral\tof\tour\ttechniques\tand\nconcepts\twill\tinvolve\tsuch\tfunctions.\n\n\tmatrix\tto\trepresent\ta\tlinear\tfunction\tthat\n\nThird,\tmatrices\tcan\tbe\tused\tto\trepresent\tbinary\trelationships.\tIn\tChapter\t1,\twe\trepresented\nthe\tedges\tof\ta\tnetwork\tas\ta\tcollection\tof\tpairs\t(i,\tj).\tAn\talternative\trepresentation\twould\nbe\tto\tcreate\ta\tmatrix\tA\tsuch\tthat\tA[i][j]\tis\t1\tif\tnodes\ti\tand\tj\tare\tconnected\tand\t0\notherwise.\n\nRecall\tthat\tbefore\twe\thad:\n\nfriendships\t=\t[(0,\t1),\t(0,\t2),\t(1,\t2),\t(1,\t3),\t(2,\t3),\t(3,\t4),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(4,\t5),\t(5,\t6),\t(5,\t7),\t(6,\t8),\t(7,\t8),\t(8,\t9)]\n\nWe\tcould\talso\trepresent\tthis\tas:\n\n\t\t\t\t\t#\t\t\t\t\tuser\t0\t\t1\t\t2\t\t3\t\t4\t\t5\t\t6\t\t7\t\t8\t\t9\n\t\t\t\t\t#\nfriendships\t=\t[[0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0],\t#\tuser\t0\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[1,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0,\t0],\t#\tuser\t1\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[1,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0],\t#\tuser\t2\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[0,\t1,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t0],\t#\tuser\t3\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t0],\t#\tuser\t4\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[0,\t0,\t0,\t0,\t1,\t0,\t1,\t1,\t0,\t0],\t#\tuser\t5\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0],\t#\tuser\t6\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t1,\t0],\t#\tuser\t7\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t1],\t#\tuser\t8\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0]]\t#\tuser\t9\n\nIf\tthere\tare\tvery\tfew\tconnections,\tthis\tis\ta\tmuch\tmore\tinefficient\trepresentation,\tsince\tyou\nend\tup\thaving\tto\tstore\ta\tlot\tof\tzeroes.\tHowever,\twith\tthe\tmatrix\trepresentation\tit\tis\tmuch\nquicker\tto\tcheck\twhether\ttwo\tnodes\tare\tconnected\t\u2014\tyou\tjust\thave\tto\tdo\ta\tmatrix\tlookup\ninstead\tof\t(potentially)\tinspecting\tevery\tedge:",
    "92": "friendships[0][2]\t==\t1\t\t\t#\tTrue,\t0\tand\t2\tare\tfriends\nfriendships[0][8]\t==\t1\t\t\t#\tFalse,\t0\tand\t8\tare\tnot\tfriends\n\nSimilarly,\tto\tfind\tthe\tconnections\ta\tnode\thas,\tyou\tonly\tneed\tto\tinspect\tthe\tcolumn\t(or\tthe\nrow)\tcorresponding\tto\tthat\tnode:\n\nfriends_of_five\t=\t[i\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tonly\tneed\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\ti,\tis_friend\tin\tenumerate(friendships[5])\t\t#\tto\tlook\tat\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tis_friend]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tone\trow\n\nPreviously\twe\tadded\ta\tlist\tof\tconnections\tto\teach\tnode\tobject\tto\tspeed\tup\tthis\tprocess,\tbut\nfor\ta\tlarge,\tevolving\tgraph\tthat\twould\tprobably\tbe\ttoo\texpensive\tand\tdifficult\tto\tmaintain.\n\nWe\u2019ll\trevisit\tmatrices\tthroughout\tthe\tbook.",
    "93": "For\tFurther\tExploration\n\nLinear\talgebra\tis\twidely\tused\tby\tdata\tscientists\t(frequently\timplicitly,\tand\tnot\ninfrequently\tby\tpeople\twho\tdon\u2019t\tunderstand\tit).\tIt\twouldn\u2019t\tbe\ta\tbad\tidea\tto\tread\ta\ntextbook.\tYou\tcan\tfind\tseveral\tfreely\tavailable\tonline:\n\nLinear\tAlgebra,\tfrom\tUC\tDavis\n\nLinear\tAlgebra,\tfrom\tSaint\tMichael\u2019s\tCollege\n\nIf\tyou\tare\tfeeling\tadventurous,\tLinear\tAlgebra\tDone\tWrong\tis\ta\tmore\tadvanced\nintroduction\n\nAll\tof\tthe\tmachinery\twe\tbuilt\there\tyou\tget\tfor\tfree\tif\tyou\tuse\tNumPy.\t(You\tget\ta\tlot\nmore\ttoo.)",
    "94": "",
    "95": "Chapter\t5.\tStatistics\n\nFacts\tare\tstubborn,\tbut\tstatistics\tare\tmore\tpliable.\n\nMark\tTwain\n\nStatistics\trefers\tto\tthe\tmathematics\tand\ttechniques\twith\twhich\twe\tunderstand\tdata.\tIt\tis\ta\nrich,\tenormous\tfield,\tmore\tsuited\tto\ta\tshelf\t(or\troom)\tin\ta\tlibrary\trather\tthan\ta\tchapter\tin\ta\nbook,\tand\tso\tour\tdiscussion\twill\tnecessarily\tnot\tbe\ta\tdeep\tone.\tInstead,\tI\u2019ll\ttry\tto\tteach\nyou\tjust\tenough\tto\tbe\tdangerous,\tand\tpique\tyour\tinterest\tjust\tenough\tthat\tyou\u2019ll\tgo\toff\tand\nlearn\tmore.",
    "96": "Describing\ta\tSingle\tSet\tof\tData\n\nThrough\ta\tcombination\tof\tword-of-mouth\tand\tluck,\tDataSciencester\thas\tgrown\tto\tdozens\nof\tmembers,\tand\tthe\tVP\tof\tFundraising\tasks\tyou\tfor\tsome\tsort\tof\tdescription\tof\thow\tmany\nfriends\tyour\tmembers\thave\tthat\the\tcan\tinclude\tin\this\televator\tpitches.\n\nUsing\ttechniques\tfrom\tChapter\t1,\tyou\tare\teasily\table\tto\tproduce\tthis\tdata.\tBut\tnow\tyou\nare\tfaced\twith\tthe\tproblem\tof\thow\tto\tdescribe\tit.\n\nOne\tobvious\tdescription\tof\tany\tdata\tset\tis\tsimply\tthe\tdata\titself:\n\nnum_friends\t=\t[100,\t49,\t41,\t40,\t25,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t...\tand\tlots\tmore\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t]\n\nFor\ta\tsmall\tenough\tdata\tset\tthis\tmight\teven\tbe\tthe\tbest\tdescription.\tBut\tfor\ta\tlarger\tdata\nset,\tthis\tis\tunwieldy\tand\tprobably\topaque.\t(Imagine\tstaring\tat\ta\tlist\tof\t1\tmillion\tnumbers.)\nFor\tthat\treason\twe\tuse\tstatistics\tto\tdistill\tand\tcommunicate\trelevant\tfeatures\tof\tour\tdata.\n\nAs\ta\tfirst\tapproach\tyou\tput\tthe\tfriend\tcounts\tinto\ta\thistogram\tusing\tCounter\tand\nplt.bar()\t(Figure\t5-1):\n\nfriend_counts\t=\tCounter(num_friends)\nxs\t=\trange(101)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tlargest\tvalue\tis\t100\nys\t=\t[friend_counts[x]\tfor\tx\tin\txs]\t\t\t\t\t#\theight\tis\tjust\t#\tof\tfriends\nplt.bar(xs,\tys)\nplt.axis([0,\t101,\t0,\t25])\nplt.title(\"Histogram\tof\tFriend\tCounts\")\nplt.xlabel(\"#\tof\tfriends\")\nplt.ylabel(\"#\tof\tpeople\")\nplt.show()",
    "97": "Figure\t5-1.\tA\thistogram\tof\tfriend\tcounts\n\nUnfortunately,\tthis\tchart\tis\tstill\ttoo\tdifficult\tto\tslip\tinto\tconversations.\tSo\tyou\tstart\ngenerating\tsome\tstatistics.\tProbably\tthe\tsimplest\tstatistic\tis\tsimply\tthe\tnumber\tof\tdata\npoints:\n\nnum_points\t=\tlen(num_friends)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t204\n\nYou\u2019re\tprobably\talso\tinterested\tin\tthe\tlargest\tand\tsmallest\tvalues:\n\nlargest_value\t=\tmax(num_friends)\t\t\t\t\t\t\t\t\t\t\t\t#\t100\nsmallest_value\t=\tmin(num_friends)\t\t\t\t\t\t\t\t\t\t\t#\t1\n\nwhich\tare\tjust\tspecial\tcases\tof\twanting\tto\tknow\tthe\tvalues\tin\tspecific\tpositions:\n\nsorted_values\t=\tsorted(num_friends)\nsmallest_value\t=\tsorted_values[0]\t\t\t\t\t\t\t\t\t\t\t#\t1\nsecond_smallest_value\t=\tsorted_values[1]\t\t\t\t#\t1\nsecond_largest_value\t=\tsorted_values[-2]\t\t\t\t#\t49\n\nBut\twe\u2019re\tonly\tgetting\tstarted.",
    "98": "Central\tTendencies\n\nUsually,\twe\u2019ll\twant\tsome\tnotion\tof\twhere\tour\tdata\tis\tcentered.\tMost\tcommonly\twe\u2019ll\tuse\nthe\tmean\t(or\taverage),\twhich\tis\tjust\tthe\tsum\tof\tthe\tdata\tdivided\tby\tits\tcount:\n\n#\tthis\tisn't\tright\tif\tyou\tdon't\tfrom\t__future__\timport\tdivision\ndef\tmean(x):\n\t\t\t\treturn\tsum(x)\t/\tlen(x)\n\nmean(num_friends)\t\t\t#\t7.333333\n\nIf\tyou\thave\ttwo\tdata\tpoints,\tthe\tmean\tis\tsimply\tthe\tpoint\thalfway\tbetween\tthem.\tAs\tyou\nadd\tmore\tpoints,\tthe\tmean\tshifts\taround,\tbut\tit\talways\tdepends\ton\tthe\tvalue\tof\tevery\npoint.\n\nWe\u2019ll\talso\tsometimes\tbe\tinterested\tin\tthe\tmedian,\twhich\tis\tthe\tmiddle-most\tvalue\t(if\tthe\nnumber\tof\tdata\tpoints\tis\todd)\tor\tthe\taverage\tof\tthe\ttwo\tmiddle-most\tvalues\t(if\tthe\tnumber\nof\tdata\tpoints\tis\teven).\n\nFor\tinstance,\tif\twe\thave\tfive\tdata\tpoints\tin\ta\tsorted\tvector\tx,\tthe\tmedian\tis\tx[5\t//\t2]\tor\nx[2].\tIf\twe\thave\tsix\tdata\tpoints,\twe\twant\tthe\taverage\tof\tx[2]\t(the\tthird\tpoint)\tand\tx[3]\n(the\tfourth\tpoint).\n\nNotice\tthat\t\u2014\tunlike\tthe\tmean\t\u2014\tthe\tmedian\tdoesn\u2019t\tdepend\ton\tevery\tvalue\tin\tyour\tdata.\nFor\texample,\tif\tyou\tmake\tthe\tlargest\tpoint\tlarger\t(or\tthe\tsmallest\tpoint\tsmaller),\tthe\nmiddle\tpoints\tremain\tunchanged,\twhich\tmeans\tso\tdoes\tthe\tmedian.\n\nThe\tmedian\tfunction\tis\tslightly\tmore\tcomplicated\tthan\tyou\tmight\texpect,\tmostly\tbecause\nof\tthe\t\u201ceven\u201d\tcase:\n\ndef\tmedian(v):\n\t\t\t\t\"\"\"finds\tthe\t'middle-most'\tvalue\tof\tv\"\"\"\n\t\t\t\tn\t=\tlen(v)\n\t\t\t\tsorted_v\t=\tsorted(v)\n\t\t\t\tmidpoint\t=\tn\t//\t2\n\n\t\t\t\tif\tn\t%\t2\t==\t1:\n\t\t\t\t\t\t\t\t#\tif\todd,\treturn\tthe\tmiddle\tvalue\n\t\t\t\t\t\t\t\treturn\tsorted_v[midpoint]\n\t\t\t\telse:\n\t\t\t\t\t\t\t\t#\tif\teven,\treturn\tthe\taverage\tof\tthe\tmiddle\tvalues\n\t\t\t\t\t\t\t\tlo\t=\tmidpoint\t-\t1\n\t\t\t\t\t\t\t\thi\t=\tmidpoint\n\t\t\t\t\t\t\t\treturn\t(sorted_v[lo]\t+\tsorted_v[hi])\t/\t2\n\nmedian(num_friends)\t#\t6.0\n\nClearly,\tthe\tmean\tis\tsimpler\tto\tcompute,\tand\tit\tvaries\tsmoothly\tas\tour\tdata\tchanges.\tIf\twe\nhave\tn\tdata\tpoints\tand\tone\tof\tthem\tincreases\tby\tsome\tsmall\tamount\te,\tthen\tnecessarily\tthe\nmean\twill\tincrease\tby\te\t/\tn.\t(This\tmakes\tthe\tmean\tamenable\tto\tall\tsorts\tof\tcalculus\ttricks.)\nWhereas\tin\torder\tto\tfind\tthe\tmedian,\twe\thave\tto\tsort\tour\tdata.\tAnd\tchanging\tone\tof\tour\ndata\tpoints\tby\ta\tsmall\tamount\te\tmight\tincrease\tthe\tmedian\tby\te,\tby\tsome\tnumber\tless\tthan\ne,\tor\tnot\tat\tall\t(depending\ton\tthe\trest\tof\tthe\tdata).",
    "99": "NOTE\n\nThere\tare,\tin\tfact,\tnonobvious\ttricks\tto\tefficiently\tcompute\tmedians\twithout\tsorting\tthe\tdata.\tHowever,\tthey\nare\tbeyond\tthe\tscope\tof\tthis\tbook,\tso\twe\thave\tto\tsort\tthe\tdata.\n\nAt\tthe\tsame\ttime,\tthe\tmean\tis\tvery\tsensitive\tto\toutliers\tin\tour\tdata.\tIf\tour\tfriendliest\tuser\nhad\t200\tfriends\t(instead\tof\t100),\tthen\tthe\tmean\twould\trise\tto\t7.82,\twhile\tthe\tmedian\nwould\tstay\tthe\tsame.\tIf\toutliers\tare\tlikely\tto\tbe\tbad\tdata\t(or\totherwise\tunrepresentative\tof\nwhatever\tphenomenon\twe\u2019re\ttrying\tto\tunderstand),\tthen\tthe\tmean\tcan\tsometimes\tgive\tus\ta\nmisleading\tpicture.\tFor\texample,\tthe\tstory\tis\toften\ttold\tthat\tin\tthe\tmid-1980s,\tthe\tmajor\tat\nthe\tUniversity\tof\tNorth\tCarolina\twith\tthe\thighest\taverage\tstarting\tsalary\twas\tgeography,\nmostly\ton\taccount\tof\tNBA\tstar\t(and\toutlier)\tMichael\tJordan.\n\nA\tgeneralization\tof\tthe\tmedian\tis\tthe\tquantile,\twhich\trepresents\tthe\tvalue\tless\tthan\twhich\na\tcertain\tpercentile\tof\tthe\tdata\tlies.\t(The\tmedian\trepresents\tthe\tvalue\tless\tthan\twhich\t50%\nof\tthe\tdata\tlies.)\n\ndef\tquantile(x,\tp):\n\t\t\t\t\"\"\"returns\tthe\tpth-percentile\tvalue\tin\tx\"\"\"\n\t\t\t\tp_index\t=\tint(p\t*\tlen(x))\n\t\t\t\treturn\tsorted(x)[p_index]\n\nquantile(num_friends,\t0.10)\t#\t1\nquantile(num_friends,\t0.25)\t#\t3\nquantile(num_friends,\t0.75)\t#\t9\nquantile(num_friends,\t0.90)\t#\t13\n\nLess\tcommonly\tyou\tmight\twant\tto\tlook\tat\tthe\tmode,\tor\tmost-common\tvalue[s]:\n\ndef\tmode(x):\n\t\t\t\t\"\"\"returns\ta\tlist,\tmight\tbe\tmore\tthan\tone\tmode\"\"\"\n\t\t\t\tcounts\t=\tCounter(x)\n\t\t\t\tmax_count\t=\tmax(counts.values())\n\t\t\t\treturn\t[x_i\tfor\tx_i,\tcount\tin\tcounts.iteritems()\n\t\t\t\t\t\t\t\t\t\t\t\tif\tcount\t==\tmax_count]\n\nmode(num_friends)\t\t\t\t\t\t\t#\t1\tand\t6\n\nBut\tmost\tfrequently\twe\u2019ll\tjust\tuse\tthe\tmean.",
    "100": "Dispersion\n\nDispersion\trefers\tto\tmeasures\tof\thow\tspread\tout\tour\tdata\tis.\tTypically\tthey\u2019re\tstatistics\tfor\nwhich\tvalues\tnear\tzero\tsignify\tnot\tspread\tout\tat\tall\tand\tfor\twhich\tlarge\tvalues\t(whatever\nthat\tmeans)\tsignify\tvery\tspread\tout.\tFor\tinstance,\ta\tvery\tsimple\tmeasure\tis\tthe\trange,\nwhich\tis\tjust\tthe\tdifference\tbetween\tthe\tlargest\tand\tsmallest\telements:\n\n#\t\"range\"\talready\tmeans\tsomething\tin\tPython,\tso\twe'll\tuse\ta\tdifferent\tname\ndef\tdata_range(x):\n\t\t\t\treturn\tmax(x)\t-\tmin(x)\n\ndata_range(num_friends)\t#\t99\n\nThe\trange\tis\tzero\tprecisely\twhen\tthe\tmax\tand\tmin\tare\tequal,\twhich\tcan\tonly\thappen\tif\tthe\nelements\tof\tx\tare\tall\tthe\tsame,\twhich\tmeans\tthe\tdata\tis\tas\tundispersed\tas\tpossible.\nConversely,\tif\tthe\trange\tis\tlarge,\tthen\tthe\tmax\tis\tmuch\tlarger\tthan\tthe\tmin\tand\tthe\tdata\tis\nmore\tspread\tout.\n\nLike\tthe\tmedian,\tthe\trange\tdoesn\u2019t\treally\tdepend\ton\tthe\twhole\tdata\tset.\tA\tdata\tset\twhose\npoints\tare\tall\teither\t0\tor\t100\thas\tthe\tsame\trange\tas\ta\tdata\tset\twhose\tvalues\tare\t0,\t100,\tand\nlots\tof\t50s.\tBut\tit\tseems\tlike\tthe\tfirst\tdata\tset\t\u201cshould\u201d\tbe\tmore\tspread\tout.\n\nA\tmore\tcomplex\tmeasure\tof\tdispersion\tis\tthe\tvariance,\twhich\tis\tcomputed\tas:\n\ndef\tde_mean(x):\n\t\t\t\t\"\"\"translate\tx\tby\tsubtracting\tits\tmean\t(so\tthe\tresult\thas\tmean\t0)\"\"\"\n\t\t\t\tx_bar\t=\tmean(x)\n\t\t\t\treturn\t[x_i\t-\tx_bar\tfor\tx_i\tin\tx]\n\ndef\tvariance(x):\n\t\t\t\t\"\"\"assumes\tx\thas\tat\tleast\ttwo\telements\"\"\"\n\t\t\t\tn\t=\tlen(x)\n\t\t\t\tdeviations\t=\tde_mean(x)\n\t\t\t\treturn\tsum_of_squares(deviations)\t/\t(n\t-\t1)\n\nvariance(num_friends)\t#\t81.54\n\nNOTE\n\nThis\tlooks\tlike\tit\tis\talmost\tthe\taverage\tsquared\tdeviation\tfrom\tthe\tmean,\texcept\tthat\twe\u2019re\tdividing\tby\tn-1\ninstead\tof\tn.\tIn\tfact,\twhen\twe\u2019re\tdealing\twith\ta\tsample\tfrom\ta\tlarger\tpopulation,\tx_bar\tis\tonly\tan\testimate\nof\tthe\tactual\tmean,\twhich\tmeans\tthat\ton\taverage\t(x_i\t-\tx_bar)\t**\t2\tis\tan\tunderestimate\tof\tx_i\u2019s\tsquared\ndeviation\tfrom\tthe\tmean,\twhich\tis\twhy\twe\tdivide\tby\tn-1\tinstead\tof\tn.\tSee\tWikipedia.\n\nNow,\twhatever\tunits\tour\tdata\tis\tin\t(e.g.,\t\u201cfriends\u201d),\tall\tof\tour\tmeasures\tof\tcentral\ttendency\nare\tin\tthat\tsame\tunit.\tThe\trange\twill\tsimilarly\tbe\tin\tthat\tsame\tunit.\tThe\tvariance,\ton\tthe\nother\thand,\thas\tunits\tthat\tare\tthe\tsquare\tof\tthe\toriginal\tunits\t(e.g.,\t\u201cfriends\tsquared\u201d).\tAs\tit\ncan\tbe\thard\tto\tmake\tsense\tof\tthese,\twe\toften\tlook\tinstead\tat\tthe\tstandard\tdeviation:\n\ndef\tstandard_deviation(x):\n\t\t\t\treturn\tmath.sqrt(variance(x))\n\nstandard_deviation(num_friends)\t#\t9.03\n\nBoth\tthe\trange\tand\tthe\tstandard\tdeviation\thave\tthe\tsame\toutlier\tproblem\tthat\twe\tsaw",
    "101": "earlier\tfor\tthe\tmean.\tUsing\tthe\tsame\texample,\tif\tour\tfriendliest\tuser\thad\tinstead\t200\nfriends,\tthe\tstandard\tdeviation\twould\tbe\t14.89,\tmore\tthan\t60%\thigher!\n\nA\tmore\trobust\talternative\tcomputes\tthe\tdifference\tbetween\tthe\t75th\tpercentile\tvalue\tand\nthe\t25th\tpercentile\tvalue:\n\ndef\tinterquartile_range(x):\n\t\t\t\treturn\tquantile(x,\t0.75)\t-\tquantile(x,\t0.25)\n\ninterquartile_range(num_friends)\t#\t6\n\nwhich\tis\tquite\tplainly\tunaffected\tby\ta\tsmall\tnumber\tof\toutliers.",
    "102": "Correlation\n\nDataSciencester\u2019s\tVP\tof\tGrowth\thas\ta\ttheory\tthat\tthe\tamount\tof\ttime\tpeople\tspend\ton\tthe\nsite\tis\trelated\tto\tthe\tnumber\tof\tfriends\tthey\thave\ton\tthe\tsite\t(she\u2019s\tnot\ta\tVP\tfor\tnothing),\nand\tshe\u2019s\tasked\tyou\tto\tverify\tthis.\n\nAfter\tdigging\tthrough\ttraffic\tlogs,\tyou\u2019ve\tcome\tup\twith\ta\tlist\tdaily_minutes\tthat\tshows\nhow\tmany\tminutes\tper\tday\teach\tuser\tspends\ton\tDataSciencester,\tand\tyou\u2019ve\tordered\tit\tso\nthat\tits\telements\tcorrespond\tto\tthe\telements\tof\tour\tprevious\tnum_friends\tlist.\tWe\u2019d\tlike\tto\ninvestigate\tthe\trelationship\tbetween\tthese\ttwo\tmetrics.\n\nWe\u2019ll\tfirst\tlook\tat\tcovariance,\tthe\tpaired\tanalogue\tof\tvariance.\tWhereas\tvariance\nmeasures\thow\ta\tsingle\tvariable\tdeviates\tfrom\tits\tmean,\tcovariance\tmeasures\thow\ttwo\nvariables\tvary\tin\ttandem\tfrom\ttheir\tmeans:\n\ndef\tcovariance(x,\ty):\n\t\t\t\tn\t=\tlen(x)\n\t\t\t\treturn\tdot(de_mean(x),\tde_mean(y))\t/\t(n\t-\t1)\n\ncovariance(num_friends,\tdaily_minutes)\t#\t22.43\n\nRecall\tthat\tdot\tsums\tup\tthe\tproducts\tof\tcorresponding\tpairs\tof\telements.\tWhen\ncorresponding\telements\tof\tx\tand\ty\tare\teither\tboth\tabove\ttheir\tmeans\tor\tboth\tbelow\ttheir\nmeans,\ta\tpositive\tnumber\tenters\tthe\tsum.\tWhen\tone\tis\tabove\tits\tmean\tand\tthe\tother\tbelow,\na\tnegative\tnumber\tenters\tthe\tsum.\tAccordingly,\ta\t\u201clarge\u201d\tpositive\tcovariance\tmeans\tthat\tx\ntends\tto\tbe\tlarge\twhen\ty\tis\tlarge\tand\tsmall\twhen\ty\tis\tsmall.\tA\t\u201clarge\u201d\tnegative\tcovariance\nmeans\tthe\topposite\t\u2014\tthat\tx\ttends\tto\tbe\tsmall\twhen\ty\tis\tlarge\tand\tvice\tversa.\tA\ncovariance\tclose\tto\tzero\tmeans\tthat\tno\tsuch\trelationship\texists.\n\nNonetheless,\tthis\tnumber\tcan\tbe\thard\tto\tinterpret,\tfor\ta\tcouple\tof\treasons:\n\nIts\tunits\tare\tthe\tproduct\tof\tthe\tinputs\u2019\tunits\t(e.g.,\tfriend-minutes-per-day),\twhich\tcan\tbe\nhard\tto\tmake\tsense\tof.\t(What\u2019s\ta\t\u201cfriend-minute-per-day\u201d?)\n\nIf\teach\tuser\thad\ttwice\tas\tmany\tfriends\t(but\tthe\tsame\tnumber\tof\tminutes),\tthe\ncovariance\twould\tbe\ttwice\tas\tlarge.\tBut\tin\ta\tsense\tthe\tvariables\twould\tbe\tjust\tas\ninterrelated.\tSaid\tdifferently,\tit\u2019s\thard\tto\tsay\twhat\tcounts\tas\ta\t\u201clarge\u201d\tcovariance.\n\nFor\tthis\treason,\tit\u2019s\tmore\tcommon\tto\tlook\tat\tthe\tcorrelation,\twhich\tdivides\tout\tthe\nstandard\tdeviations\tof\tboth\tvariables:\n\ndef\tcorrelation(x,\ty):\n\t\t\t\tstdev_x\t=\tstandard_deviation(x)\n\t\t\t\tstdev_y\t=\tstandard_deviation(y)\n\t\t\t\tif\tstdev_x\t>\t0\tand\tstdev_y\t>\t0:\n\t\t\t\t\t\t\t\treturn\tcovariance(x,\ty)\t/\tstdev_x\t/\tstdev_y\n\t\t\t\telse:\n\t\t\t\t\t\t\t\treturn\t0\t\t\t\t#\tif\tno\tvariation,\tcorrelation\tis\tzero\n\ncorrelation(num_friends,\tdaily_minutes)\t#\t0.25",
    "103": "The\tcorrelation\tis\tunitless\tand\talways\tlies\tbetween\t-1\t(perfect\tanti-correlation)\tand\t1\n(perfect\tcorrelation).\tA\tnumber\tlike\t0.25\trepresents\ta\trelatively\tweak\tpositive\tcorrelation.\n\nHowever,\tone\tthing\twe\tneglected\tto\tdo\twas\texamine\tour\tdata.\tCheck\tout\tFigure\t5-2.\n\nFigure\t5-2.\tCorrelation\twith\tan\toutlier\n\nThe\tperson\twith\t100\tfriends\t(who\tspends\tonly\tone\tminute\tper\tday\ton\tthe\tsite)\tis\ta\thuge\noutlier,\tand\tcorrelation\tcan\tbe\tvery\tsensitive\tto\toutliers.\tWhat\thappens\tif\twe\tignore\thim?\n\noutlier\t=\tnum_friends.index(100)\t\t\t\t#\tindex\tof\toutlier\n\nnum_friends_good\t=\t[x\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\ti,\tx\tin\tenumerate(num_friends)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\ti\t!=\toutlier]\n\ndaily_minutes_good\t=\t[x\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\ti,\tx\tin\tenumerate(daily_minutes)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\ti\t!=\toutlier]\n\ncorrelation(num_friends_good,\tdaily_minutes_good)\t#\t0.57\n\nWithout\tthe\toutlier,\tthere\tis\ta\tmuch\tstronger\tcorrelation\t(Figure\t5-3).",
    "104": "Figure\t5-3.\tCorrelation\tafter\tremoving\tthe\toutlier\n\nYou\tinvestigate\tfurther\tand\tdiscover\tthat\tthe\toutlier\twas\tactually\tan\tinternal\ttest\taccount\nthat\tno\tone\tever\tbothered\tto\tremove.\tSo\tyou\tfeel\tpretty\tjustified\tin\texcluding\tit.",
    "105": "Simpson\u2019s\tParadox\n\nOne\tnot\tuncommon\tsurprise\twhen\tanalyzing\tdata\tis\tSimpson\u2019s\tParadox,\tin\twhich\ncorrelations\tcan\tbe\tmisleading\twhen\tconfounding\tvariables\tare\tignored.\n\nFor\texample,\timagine\tthat\tyou\tcan\tidentify\tall\tof\tyour\tmembers\tas\teither\tEast\tCoast\tdata\nscientists\tor\tWest\tCoast\tdata\tscientists.\tYou\tdecide\tto\texamine\twhich\tcoast\u2019s\tdata\nscientists\tare\tfriendlier:\n\ncoast\n\n#\tof\tmembers avg.\t#\tof\tfriends\n\nWest\tCoast 101\n\nEast\tCoast 103\n\n8.2\n\n6.5\n\nIt\tcertainly\tlooks\tlike\tthe\tWest\tCoast\tdata\tscientists\tare\tfriendlier\tthan\tthe\tEast\tCoast\tdata\nscientists.\tYour\tcoworkers\tadvance\tall\tsorts\tof\ttheories\tas\tto\twhy\tthis\tmight\tbe:\tmaybe\tit\u2019s\nthe\tsun,\tor\tthe\tcoffee,\tor\tthe\torganic\tproduce,\tor\tthe\tlaid-back\tPacific\tvibe?\n\nWhen\tplaying\twith\tthe\tdata\tyou\tdiscover\tsomething\tvery\tstrange.\tIf\tyou\tonly\tlook\tat\npeople\twith\tPhDs,\tthe\tEast\tCoast\tdata\tscientists\thave\tmore\tfriends\ton\taverage.\tAnd\tif\tyou\nonly\tlook\tat\tpeople\twithout\tPhDs,\tthe\tEast\tCoast\tdata\tscientists\talso\thave\tmore\tfriends\ton\naverage!\n\ncoast\n\ndegree #\tof\tmembers avg.\t#\tof\tfriends\n\nWest\tCoast PhD\n\nEast\tCoast PhD\n\n35\n\n70\n\nWest\tCoast no\tPhD 66\n\nEast\tCoast no\tPhD 33\n\n3.1\n\n3.2\n\n10.9\n\n13.4\n\nOnce\tyou\taccount\tfor\tthe\tusers\u2019\tdegrees,\tthe\tcorrelation\tgoes\tin\tthe\topposite\tdirection!\nBucketing\tthe\tdata\tas\tEast\tCoast/West\tCoast\tdisguised\tthe\tfact\tthat\tthe\tEast\tCoast\tdata\nscientists\tskew\tmuch\tmore\theavily\ttoward\tPhD\ttypes.\n\nThis\tphenomenon\tcrops\tup\tin\tthe\treal\tworld\twith\tsome\tregularity.\tThe\tkey\tissue\tis\tthat\ncorrelation\tis\tmeasuring\tthe\trelationship\tbetween\tyour\ttwo\tvariables\tall\telse\tbeing\tequal.\nIf\tyour\tdata\tclasses\tare\tassigned\tat\trandom,\tas\tthey\tmight\tbe\tin\ta\twell-designed\nexperiment,\t\u201call\telse\tbeing\tequal\u201d\tmight\tnot\tbe\ta\tterrible\tassumption.\tBut\twhen\tthere\tis\ta\ndeeper\tpattern\tto\tclass\tassignments,\t\u201call\telse\tbeing\tequal\u201d\tcan\tbe\tan\tawful\tassumption.\n\nThe\tonly\treal\tway\tto\tavoid\tthis\tis\tby\tknowing\tyour\tdata\tand\tby\tdoing\twhat\tyou\tcan\tto\nmake\tsure\tyou\u2019ve\tchecked\tfor\tpossible\tconfounding\tfactors.\tObviously,\tthis\tis\tnot\talways\npossible.\tIf\tyou\tdidn\u2019t\thave\tthe\teducational\tattainment\tof\tthese\t200\tdata\tscientists,\tyou",
    "106": "might\tsimply\tconclude\tthat\tthere\twas\tsomething\tinherently\tmore\tsociable\tabout\tthe\tWest\nCoast.",
    "107": "Some\tOther\tCorrelational\tCaveats\n\nA\tcorrelation\tof\tzero\tindicates\tthat\tthere\tis\tno\tlinear\trelationship\tbetween\tthe\ttwo\nvariables.\tHowever,\tthere\tmay\tbe\tother\tsorts\tof\trelationships.\tFor\texample,\tif:\n\nx\t=\t[-2,\t-1,\t0,\t1,\t2]\ny\t=\t[\t2,\t\t1,\t0,\t1,\t2]\n\nthen\tx\tand\ty\thave\tzero\tcorrelation.\tBut\tthey\tcertainly\thave\ta\trelationship\t\u2014\teach\telement\nof\ty\tequals\tthe\tabsolute\tvalue\tof\tthe\tcorresponding\telement\tof\tx.\tWhat\tthey\tdon\u2019t\thave\tis\ta\nrelationship\tin\twhich\tknowing\thow\tx_i\tcompares\tto\tmean(x)\tgives\tus\tinformation\tabout\nhow\ty_i\tcompares\tto\tmean(y).\tThat\tis\tthe\tsort\tof\trelationship\tthat\tcorrelation\tlooks\tfor.\n\nIn\taddition,\tcorrelation\ttells\tyou\tnothing\tabout\thow\tlarge\tthe\trelationship\tis.\tThe\nvariables:\n\nx\t=\t[-2,\t1,\t0,\t1,\t2]\ny\t=\t[99.98,\t99.99,\t100,\t100.01,\t100.02]\n\nare\tperfectly\tcorrelated,\tbut\t(depending\ton\twhat\tyou\u2019re\tmeasuring)\tit\u2019s\tquite\tpossible\tthat\nthis\trelationship\tisn\u2019t\tall\tthat\tinteresting.",
    "108": "Correlation\tand\tCausation\n\nYou\thave\tprobably\theard\tat\tsome\tpoint\tthat\t\u201ccorrelation\tis\tnot\tcausation,\u201d\tmost\tlikely\tby\nsomeone\tlooking\tat\tdata\tthat\tposed\ta\tchallenge\tto\tparts\tof\this\tworldview\tthat\the\twas\nreluctant\tto\tquestion.\tNonetheless,\tthis\tis\tan\timportant\tpoint\t\u2014\tif\tx\tand\ty\tare\tstrongly\ncorrelated,\tthat\tmight\tmean\tthat\tx\tcauses\ty,\tthat\ty\tcauses\tx,\tthat\teach\tcauses\tthe\tother,\tthat\nsome\tthird\tfactor\tcauses\tboth,\tor\tit\tmight\tmean\tnothing.\n\nConsider\tthe\trelationship\tbetween\tnum_friends\tand\tdaily_minutes.\tIt\u2019s\tpossible\tthat\nhaving\tmore\tfriends\ton\tthe\tsite\tcauses\tDataSciencester\tusers\tto\tspend\tmore\ttime\ton\tthe\nsite.\tThis\tmight\tbe\tthe\tcase\tif\teach\tfriend\tposts\ta\tcertain\tamount\tof\tcontent\teach\tday,\nwhich\tmeans\tthat\tthe\tmore\tfriends\tyou\thave,\tthe\tmore\ttime\tit\ttakes\tto\tstay\tcurrent\twith\ntheir\tupdates.\n\nHowever,\tit\u2019s\talso\tpossible\tthat\tthe\tmore\ttime\tyou\tspend\targuing\tin\tthe\tDataSciencester\nforums,\tthe\tmore\tyou\tencounter\tand\tbefriend\tlike-minded\tpeople.\tThat\tis,\tspending\tmore\ntime\ton\tthe\tsite\tcauses\tusers\tto\thave\tmore\tfriends.\n\nA\tthird\tpossibility\tis\tthat\tthe\tusers\twho\tare\tmost\tpassionate\tabout\tdata\tscience\tspend\tmore\ntime\ton\tthe\tsite\t(because\tthey\tfind\tit\tmore\tinteresting)\tand\tmore\tactively\tcollect\tdata\nscience\tfriends\t(because\tthey\tdon\u2019t\twant\tto\tassociate\twith\tanyone\telse).\n\nOne\tway\tto\tfeel\tmore\tconfident\tabout\tcausality\tis\tby\tconducting\trandomized\ttrials.\tIf\tyou\ncan\trandomly\tsplit\tyour\tusers\tinto\ttwo\tgroups\twith\tsimilar\tdemographics\tand\tgive\tone\tof\nthe\tgroups\ta\tslightly\tdifferent\texperience,\tthen\tyou\tcan\toften\tfeel\tpretty\tgood\tthat\tthe\ndifferent\texperiences\tare\tcausing\tthe\tdifferent\toutcomes.\n\nFor\tinstance,\tif\tyou\tdon\u2019t\tmind\tbeing\tangrily\taccused\tof\texperimenting\ton\tyour\tusers,\tyou\ncould\trandomly\tchoose\ta\tsubset\tof\tyour\tusers\tand\tshow\tthem\tcontent\tfrom\tonly\ta\tfraction\nof\ttheir\tfriends.\tIf\tthis\tsubset\tsubsequently\tspent\tless\ttime\ton\tthe\tsite,\tthis\twould\tgive\tyou\nsome\tconfidence\tthat\thaving\tmore\tfriends\tcauses\tmore\ttime\ton\tthe\tsite.",
    "109": "For\tFurther\tExploration\n\nSciPy,\tpandas,\tand\tStatsModels\tall\tcome\twith\ta\twide\tvariety\tof\tstatistical\tfunctions.\n\nStatistics\tis\timportant.\t(Or\tmaybe\tstatistics\tare\timportant?)\tIf\tyou\twant\tto\tbe\ta\tgood\ndata\tscientist\tit\twould\tbe\ta\tgood\tidea\tto\tread\ta\tstatistics\ttextbook.\tMany\tare\tfreely\navailable\tonline.\tA\tcouple\tthat\tI\tlike\tare:\n\nOpenIntro\tStatistics\n\nOpenStax\tIntroductory\tStatistics",
    "110": "",
    "111": "Chapter\t6.\tProbability\n\nThe\tlaws\tof\tprobability,\tso\ttrue\tin\tgeneral,\tso\tfallacious\tin\tparticular.\n\nEdward\tGibbon\n\nIt\tis\thard\tto\tdo\tdata\tscience\twithout\tsome\tsort\tof\tunderstanding\tof\tprobability\tand\tits\nmathematics.\tAs\twith\tour\ttreatment\tof\tstatistics\tin\tChapter\t5,\twe\u2019ll\twave\tour\thands\ta\tlot\nand\telide\tmany\tof\tthe\ttechnicalities.\n\nFor\tour\tpurposes\tyou\tshould\tthink\tof\tprobability\tas\ta\tway\tof\tquantifying\tthe\tuncertainty\nassociated\twith\tevents\tchosen\tfrom\ta\tsome\tuniverse\tof\tevents.\tRather\tthan\tgetting\ntechnical\tabout\twhat\tthese\tterms\tmean,\tthink\tof\trolling\ta\tdie.\tThe\tuniverse\tconsists\tof\tall\npossible\toutcomes.\tAnd\tany\tsubset\tof\tthese\toutcomes\tis\tan\tevent;\tfor\texample,\t\u201cthe\tdie\nrolls\ta\tone\u201d\tor\t\u201cthe\tdie\trolls\tan\teven\tnumber.\u201d\n\nNotationally,\twe\twrite\t\n\n\tto\tmean\t\u201cthe\tprobability\tof\tthe\tevent\tE.\u201d\n\nWe\u2019ll\tuse\tprobability\ttheory\tto\tbuild\tmodels.\tWe\u2019ll\tuse\tprobability\ttheory\tto\tevaluate\nmodels.\tWe\u2019ll\tuse\tprobability\ttheory\tall\tover\tthe\tplace.\n\nOne\tcould,\twere\tone\tso\tinclined,\tget\treally\tdeep\tinto\tthe\tphilosophy\tof\twhat\tprobability\ntheory\tmeans.\t(This\tis\tbest\tdone\tover\tbeers.)\tWe\twon\u2019t\tbe\tdoing\tthat.",
    "112": "Dependence\tand\tIndependence\n\nRoughly\tspeaking,\twe\tsay\tthat\ttwo\tevents\tE\tand\tF\tare\tdependent\tif\tknowing\tsomething\nabout\twhether\tE\thappens\tgives\tus\tinformation\tabout\twhether\tF\thappens\t(and\tvice\tversa).\nOtherwise\tthey\tare\tindependent.\n\nFor\tinstance,\tif\twe\tflip\ta\tfair\tcoin\ttwice,\tknowing\twhether\tthe\tfirst\tflip\tis\tHeads\tgives\tus\nno\tinformation\tabout\twhether\tthe\tsecond\tflip\tis\tHeads.\tThese\tevents\tare\tindependent.\tOn\nthe\tother\thand,\tknowing\twhether\tthe\tfirst\tflip\tis\tHeads\tcertainly\tgives\tus\tinformation\nabout\twhether\tboth\tflips\tare\tTails.\t(If\tthe\tfirst\tflip\tis\tHeads,\tthen\tdefinitely\tit\u2019s\tnot\tthe\tcase\nthat\tboth\tflips\tare\tTails.)\tThese\ttwo\tevents\tare\tdependent.\n\nMathematically,\twe\tsay\tthat\ttwo\tevents\tE\tand\tF\tare\tindependent\tif\tthe\tprobability\tthat\nthey\tboth\thappen\tis\tthe\tproduct\tof\tthe\tprobabilities\tthat\teach\tone\thappens:\n\nIn\tthe\texample\tabove,\tthe\tprobability\tof\t\u201cfirst\tflip\tHeads\u201d\tis\t1/2,\tand\tthe\tprobability\tof\n\u201cboth\tflips\tTails\u201d\tis\t1/4,\tbut\tthe\tprobability\tof\t\u201cfirst\tflip\tHeads\tand\tboth\tflips\tTails\u201d\tis\t0.",
    "113": "Conditional\tProbability\n\nWhen\ttwo\tevents\tE\tand\tF\tare\tindependent,\tthen\tby\tdefinition\twe\thave:\n\nIf\tthey\tare\tnot\tnecessarily\tindependent\t(and\tif\tthe\tprobability\tof\tF\tis\tnot\tzero),\tthen\twe\ndefine\tthe\tprobability\tof\tE\t\u201cconditional\ton\tF\u201d\tas:\n\nYou\tshould\tthink\tof\tthis\tas\tthe\tprobability\tthat\tE\thappens,\tgiven\tthat\twe\tknow\tthat\tF\nhappens.\n\nWe\toften\trewrite\tthis\tas:\n\nWhen\tE\tand\tF\tare\tindependent,\tyou\tcan\tcheck\tthat\tthis\tgives:\n\nwhich\tis\tthe\tmathematical\tway\tof\texpressing\tthat\tknowing\tF\toccurred\tgives\tus\tno\nadditional\tinformation\tabout\twhether\tE\toccurred.\n\nOne\tcommon\ttricky\texample\tinvolves\ta\tfamily\twith\ttwo\t(unknown)\tchildren.\n\nIf\twe\tassume\tthat:\n\n1.\t Each\tchild\tis\tequally\tlikely\tto\tbe\ta\tboy\tor\ta\tgirl\n\n2.\t The\tgender\tof\tthe\tsecond\tchild\tis\tindependent\tof\tthe\tgender\tof\tthe\tfirst\tchild\n\nthen\tthe\tevent\t\u201cno\tgirls\u201d\thas\tprobability\t1/4,\tthe\tevent\t\u201cone\tgirl,\tone\tboy\u201d\thas\tprobability\n1/2,\tand\tthe\tevent\t\u201ctwo\tgirls\u201d\thas\tprobability\t1/4.\n\nNow\twe\tcan\task\twhat\tis\tthe\tprobability\tof\tthe\tevent\t\u201cboth\tchildren\tare\tgirls\u201d\t(B)\nconditional\ton\tthe\tevent\t\u201cthe\tolder\tchild\tis\ta\tgirl\u201d\t(G)?\tUsing\tthe\tdefinition\tof\tconditional\nprobability:\n\nsince\tthe\tevent\tB\tand\tG\t(\u201cboth\tchildren\tare\tgirls\tand\tthe\tolder\tchild\tis\ta\tgirl\u201d)\tis\tjust\tthe\nevent\tB.\t(Once\tyou\tknow\tthat\tboth\tchildren\tare\tgirls,\tit\u2019s\tnecessarily\ttrue\tthat\tthe\tolder\nchild\tis\ta\tgirl.)\n\nMost\tlikely\tthis\tresult\taccords\twith\tyour\tintuition.",
    "114": "We\tcould\talso\task\tabout\tthe\tprobability\tof\tthe\tevent\t\u201cboth\tchildren\tare\tgirls\u201d\tconditional\non\tthe\tevent\t\u201cat\tleast\tone\tof\tthe\tchildren\tis\ta\tgirl\u201d\t(L).\tSurprisingly,\tthe\tanswer\tis\tdifferent\nfrom\tbefore!\n\nAs\tbefore,\tthe\tevent\tB\tand\tL\t(\u201cboth\tchildren\tare\tgirls\tand\tat\tleast\tone\tof\tthe\tchildren\tis\ta\ngirl\u201d)\tis\tjust\tthe\tevent\tB.\tThis\tmeans\twe\thave:\n\nHow\tcan\tthis\tbe\tthe\tcase?\tWell,\tif\tall\tyou\tknow\tis\tthat\tat\tleast\tone\tof\tthe\tchildren\tis\ta\tgirl,\nthen\tit\tis\ttwice\tas\tlikely\tthat\tthe\tfamily\thas\tone\tboy\tand\tone\tgirl\tthan\tthat\tit\thas\tboth\tgirls.\n\nWe\tcan\tcheck\tthis\tby\t\u201cgenerating\u201d\ta\tlot\tof\tfamilies:\n\ndef\trandom_kid():\n\t\t\t\treturn\trandom.choice([\"boy\",\t\"girl\"])\n\nboth_girls\t=\t0\nolder_girl\t=\t0\neither_girl\t=\t0\n\nrandom.seed(0)\nfor\t_\tin\trange(10000):\n\t\t\t\tyounger\t=\trandom_kid()\n\t\t\t\tolder\t=\trandom_kid()\n\t\t\t\tif\tolder\t==\t\"girl\":\n\t\t\t\t\t\t\t\tolder_girl\t+=\t1\n\t\t\t\tif\tolder\t==\t\"girl\"\tand\tyounger\t==\t\"girl\":\n\t\t\t\t\t\t\t\tboth_girls\t+=\t1\n\t\t\t\tif\tolder\t==\t\"girl\"\tor\tyounger\t==\t\"girl\":\n\t\t\t\t\t\t\t\teither_girl\t+=\t1\n\nprint\t\"P(both\t|\tolder):\",\tboth_girls\t/\tolder_girl\t\t\t\t\t\t#\t0.514\t~\t1/2\nprint\t\"P(both\t|\teither):\t\",\tboth_girls\t/\teither_girl\t\t\t#\t0.342\t~\t1/3",
    "115": "Bayes\u2019s\tTheorem\n\nOne\tof\tthe\tdata\tscientist\u2019s\tbest\tfriends\tis\tBayes\u2019s\tTheorem,\twhich\tis\ta\tway\tof\t\u201creversing\u201d\nconditional\tprobabilities.\tLet\u2019s\tsay\twe\tneed\tto\tknow\tthe\tprobability\tof\tsome\tevent\tE\nconditional\ton\tsome\tother\tevent\tF\toccurring.\tBut\twe\tonly\thave\tinformation\tabout\tthe\nprobability\tof\tF\tconditional\ton\tE\toccurring.\tUsing\tthe\tdefinition\tof\tconditional\tprobability\ntwice\ttells\tus\tthat:\n\nThe\tevent\tF\tcan\tbe\tsplit\tinto\tthe\ttwo\tmutually\texclusive\tevents\t\u201cF\tand\tE\u201d\tand\t\u201cF\tand\tnot\nE.\u201d\tIf\twe\twrite\t\n\n\tfor\t\u201cnot\tE\u201d\t(i.e.,\t\u201cE\tdoesn\u2019t\thappen\u201d),\tthen:\n\nso\tthat:\n\nwhich\tis\thow\tBayes\u2019s\tTheorem\tis\toften\tstated.\n\nThis\ttheorem\toften\tgets\tused\tto\tdemonstrate\twhy\tdata\tscientists\tare\tsmarter\tthan\tdoctors.\nImagine\ta\tcertain\tdisease\tthat\taffects\t1\tin\tevery\t10,000\tpeople.\tAnd\timagine\tthat\tthere\tis\ta\ntest\tfor\tthis\tdisease\tthat\tgives\tthe\tcorrect\tresult\t(\u201cdiseased\u201d\tif\tyou\thave\tthe\tdisease,\n\u201cnondiseased\u201d\tif\tyou\tdon\u2019t)\t99%\tof\tthe\ttime.\n\nWhat\tdoes\ta\tpositive\ttest\tmean?\tLet\u2019s\tuse\tT\tfor\tthe\tevent\t\u201cyour\ttest\tis\tpositive\u201d\tand\tD\tfor\nthe\tevent\t\u201cyou\thave\tthe\tdisease.\u201d\tThen\tBayes\u2019s\tTheorem\tsays\tthat\tthe\tprobability\tthat\tyou\nhave\tthe\tdisease,\tconditional\ton\ttesting\tpositive,\tis:\n\nHere\twe\tknow\tthat\t\n\n,\tthe\tprobability\tthat\tsomeone\twith\tthe\tdisease\ttests\n\npositive,\tis\t0.99.\t\n\n,\tthe\tprobability\tthat\tany\tgiven\tperson\thas\tthe\tdisease,\tis\t1/10,000\n\n=\t0.0001.\t\n\n,\tthe\tprobability\tthat\tsomeone\twithout\tthe\tdisease\ttests\tpositive,\n\nis\t0.01.\tAnd\t\n0.9999.\tIf\tyou\tsubstitute\tthese\tnumbers\tinto\tBayes\u2019s\tTheorem\tyou\tfind\n\n,\tthe\tprobability\tthat\tany\tgiven\tperson\tdoesn\u2019t\thave\tthe\tdisease,\tis\n\nThat\tis,\tless\tthan\t1%\tof\tthe\tpeople\twho\ttest\tpositive\tactually\thave\tthe\tdisease.\n\nThis\tassumes\tthat\tpeople\ttake\tthe\ttest\tmore\tor\tless\tat\trandom.\tIf\tonly\tpeople\twith\tcertain\tsymptoms\ttake\nthe\ttest\twe\twould\tinstead\thave\tto\tcondition\ton\tthe\tevent\t\u201cpositive\ttest\tand\tsymptoms\u201d\tand\tthe\tnumber\nwould\tlikely\tbe\ta\tlot\thigher.\n\nNOTE",
    "116": "While\tthis\tis\ta\tsimple\tcalculation\tfor\ta\tdata\tscientist,\tmost\tdoctors\twill\tguess\tthat\t\n\n\tis\tapproximately\t2.\n\nA\tmore\tintuitive\tway\tto\tsee\tthis\tis\tto\timagine\ta\tpopulation\tof\t1\tmillion\tpeople.\tYou\u2019d\nexpect\t100\tof\tthem\tto\thave\tthe\tdisease,\tand\t99\tof\tthose\t100\tto\ttest\tpositive.\tOn\tthe\tother\nhand,\tyou\u2019d\texpect\t999,900\tof\tthem\tnot\tto\thave\tthe\tdisease,\tand\t9,999\tof\tthose\tto\ttest\npositive.\tWhich\tmeans\tthat\tyou\u2019d\texpect\tonly\t99\tout\tof\t(99\t+\t9999)\tpositive\ttesters\tto\nactually\thave\tthe\tdisease.",
    "117": "Random\tVariables\n\nA\trandom\tvariable\tis\ta\tvariable\twhose\tpossible\tvalues\thave\tan\tassociated\tprobability\ndistribution.\tA\tvery\tsimple\trandom\tvariable\tequals\t1\tif\ta\tcoin\tflip\tturns\tup\theads\tand\t0\tif\nthe\tflip\tturns\tup\ttails.\tA\tmore\tcomplicated\tone\tmight\tmeasure\tthe\tnumber\tof\theads\nobserved\twhen\tflipping\ta\tcoin\t10\ttimes\tor\ta\tvalue\tpicked\tfrom\trange(10)\twhere\teach\nnumber\tis\tequally\tlikely.\n\nThe\tassociated\tdistribution\tgives\tthe\tprobabilities\tthat\tthe\tvariable\trealizes\teach\tof\tits\npossible\tvalues.\tThe\tcoin\tflip\tvariable\tequals\t0\twith\tprobability\t0.5\tand\t1\twith\tprobability\n0.5.\tThe\trange(10)\tvariable\thas\ta\tdistribution\tthat\tassigns\tprobability\t0.1\tto\teach\tof\tthe\nnumbers\tfrom\t0\tto\t9.\n\nWe\twill\tsometimes\ttalk\tabout\tthe\texpected\tvalue\tof\ta\trandom\tvariable,\twhich\tis\tthe\naverage\tof\tits\tvalues\tweighted\tby\ttheir\tprobabilities.\tThe\tcoin\tflip\tvariable\thas\tan\nexpected\tvalue\tof\t1/2\t(=\t0\t*\t1/2\t+\t1\t*\t1/2),\tand\tthe\trange(10)\tvariable\thas\tan\texpected\nvalue\tof\t4.5.\n\nRandom\tvariables\tcan\tbe\tconditioned\ton\tevents\tjust\tas\tother\tevents\tcan.\tGoing\tback\tto\tthe\ntwo-child\texample\tfrom\t\u201cConditional\tProbability\u201d,\tif\tX\tis\tthe\trandom\tvariable\nrepresenting\tthe\tnumber\tof\tgirls,\tX\tequals\t0\twith\tprobability\t1/4,\t1\twith\tprobability\t1/2,\nand\t2\twith\tprobability\t1/4.\n\nWe\tcan\tdefine\ta\tnew\trandom\tvariable\tY\tthat\tgives\tthe\tnumber\tof\tgirls\tconditional\ton\tat\nleast\tone\tof\tthe\tchildren\tbeing\ta\tgirl.\tThen\tY\tequals\t1\twith\tprobability\t2/3\tand\t2\twith\nprobability\t1/3.\tAnd\ta\tvariable\tZ\tthat\u2019s\tthe\tnumber\tof\tgirls\tconditional\ton\tthe\tolder\tchild\nbeing\ta\tgirl\tequals\t1\twith\tprobability\t1/2\tand\t2\twith\tprobability\t1/2.\n\nFor\tthe\tmost\tpart,\twe\twill\tbe\tusing\trandom\tvariables\timplicitly\tin\twhat\twe\tdo\twithout\ncalling\tspecial\tattention\tto\tthem.\tBut\tif\tyou\tlook\tdeeply\tyou\u2019ll\tsee\tthem.",
    "118": "Continuous\tDistributions\n\nA\tcoin\tflip\tcorresponds\tto\ta\tdiscrete\tdistribution\t\u2014\tone\tthat\tassociates\tpositive\nprobability\twith\tdiscrete\toutcomes.\tOften\twe\u2019ll\twant\tto\tmodel\tdistributions\tacross\ta\ncontinuum\tof\toutcomes.\t(For\tour\tpurposes,\tthese\toutcomes\twill\talways\tbe\treal\tnumbers,\nalthough\tthat\u2019s\tnot\talways\tthe\tcase\tin\treal\tlife.)\tFor\texample,\tthe\tuniform\tdistribution\tputs\nequal\tweight\ton\tall\tthe\tnumbers\tbetween\t0\tand\t1.\n\nBecause\tthere\tare\tinfinitely\tmany\tnumbers\tbetween\t0\tand\t1,\tthis\tmeans\tthat\tthe\tweight\tit\nassigns\tto\tindividual\tpoints\tmust\tnecessarily\tbe\tzero.\tFor\tthis\treason,\twe\trepresent\ta\ncontinuous\tdistribution\twith\ta\tprobability\tdensity\tfunction\t(pdf)\tsuch\tthat\tthe\tprobability\nof\tseeing\ta\tvalue\tin\ta\tcertain\tinterval\tequals\tthe\tintegral\tof\tthe\tdensity\tfunction\tover\tthe\ninterval.\n\nIf\tyour\tintegral\tcalculus\tis\trusty,\ta\tsimpler\tway\tof\tunderstanding\tthis\tis\tthat\tif\ta\tdistribution\thas\tdensity\n\nfunction\t\n\n,\tthen\tthe\tprobability\tof\tseeing\ta\tvalue\tbetween\t\n\n\tand\t\n\n\tis\tapproximately\t\n\nNOTE\n\n\tif\t\n\n\tis\tsmall.\n\nThe\tdensity\tfunction\tfor\tthe\tuniform\tdistribution\tis\tjust:\n\ndef\tuniform_pdf(x):\n\t\t\t\treturn\t1\tif\tx\t>=\t0\tand\tx\t<\t1\telse\t0\n\nThe\tprobability\tthat\ta\trandom\tvariable\tfollowing\tthat\tdistribution\tis\tbetween\t0.2\tand\t0.3\tis\n1/10,\tas\tyou\u2019d\texpect.\tPython\u2019s\trandom.random()\tis\ta\t[pseudo]random\tvariable\twith\ta\nuniform\tdensity.\n\nWe\twill\toften\tbe\tmore\tinterested\tin\tthe\tcumulative\tdistribution\tfunction\t(cdf),\twhich\tgives\nthe\tprobability\tthat\ta\trandom\tvariable\tis\tless\tthan\tor\tequal\tto\ta\tcertain\tvalue.\tIt\u2019s\tnot\thard\nto\tcreate\tthe\tcumulative\tdistribution\tfunction\tfor\tthe\tuniform\tdistribution\t(Figure\t6-1):\n\ndef\tuniform_cdf(x):\n\t\t\t\t\"returns\tthe\tprobability\tthat\ta\tuniform\trandom\tvariable\tis\t<=\tx\"\n\t\t\t\tif\tx\t<\t0:\t\t\treturn\t0\t\t\t\t#\tuniform\trandom\tis\tnever\tless\tthan\t0\n\t\t\t\telif\tx\t<\t1:\treturn\tx\t\t\t\t#\te.g.\tP(X\t<=\t0.4)\t=\t0.4\n\t\t\t\telse:\t\t\t\t\t\t\treturn\t1\t\t\t\t#\tuniform\trandom\tis\talways\tless\tthan\t1",
    "119": "Figure\t6-1.\tThe\tuniform\tcdf",
    "120": "The\tNormal\tDistribution\n\nThe\tnormal\tdistribution\tis\tthe\tking\tof\tdistributions.\tIt\tis\tthe\tclassic\tbell\tcurve\u2013shaped\ndistribution\tand\tis\tcompletely\tdetermined\tby\ttwo\tparameters:\tits\tmean\t\n\t(mu)\tand\tits\nstandard\tdeviation\t\n\t(sigma).\tThe\tmean\tindicates\twhere\tthe\tbell\tis\tcentered,\tand\tthe\nstandard\tdeviation\thow\t\u201cwide\u201d\tit\tis.\n\nIt\thas\tthe\tdistribution\tfunction:\n\nwhich\twe\tcan\timplement\tas:\n\ndef\tnormal_pdf(x,\tmu=0,\tsigma=1):\n\t\t\t\tsqrt_two_pi\t=\tmath.sqrt(2\t*\tmath.pi)\n\t\t\t\treturn\t(math.exp(-(x-mu)\t**\t2\t/\t2\t/\tsigma\t**\t2)\t/\t(sqrt_two_pi\t*\tsigma))\n\nIn\tFigure\t6-2,\twe\tplot\tsome\tof\tthese\tpdfs\tto\tsee\twhat\tthey\tlook\tlike:\n\nxs\t=\t[x\t/\t10.0\tfor\tx\tin\trange(-50,\t50)]\nplt.plot(xs,[normal_pdf(x,sigma=1)\tfor\tx\tin\txs],'-',label='mu=0,sigma=1')\nplt.plot(xs,[normal_pdf(x,sigma=2)\tfor\tx\tin\txs],'--',label='mu=0,sigma=2')\nplt.plot(xs,[normal_pdf(x,sigma=0.5)\tfor\tx\tin\txs],':',label='mu=0,sigma=0.5')\nplt.plot(xs,[normal_pdf(x,mu=-1)\t\t\tfor\tx\tin\txs],'-.',label='mu=-1,sigma=1')\nplt.legend()\nplt.title(\"Various\tNormal\tpdfs\")\nplt.show()",
    "121": "Figure\t6-2.\tVarious\tnormal\tpdfs\n\nWhen\t\nnormal\trandom\tvariable,\tthen\tit\tturns\tout\tthat:\n\n\tand\t\n\n,\tit\u2019s\tcalled\tthe\tstandard\tnormal\tdistribution.\tIf\tZ\tis\ta\tstandard\n\nis\talso\tnormal\tbut\twith\tmean\t\nrandom\tvariable\twith\tmean\t\n\n\tand\tstandard\tdeviation\t\n\n.\tConversely,\tif\tX\tis\ta\tnormal\n\n\tand\tstandard\tdeviation\t\n\n,\n\nis\ta\tstandard\tnormal\tvariable.\n\nThe\tcumulative\tdistribution\tfunction\tfor\tthe\tnormal\tdistribution\tcannot\tbe\twritten\tin\tan\n\u201celementary\u201d\tmanner,\tbut\twe\tcan\twrite\tit\tusing\tPython\u2019s\tmath.erf:\n\ndef\tnormal_cdf(x,\tmu=0,sigma=1):\n\t\t\t\treturn\t(1\t+\tmath.erf((x\t-\tmu)\t/\tmath.sqrt(2)\t/\tsigma))\t/\t2\n\nAgain,\tin\tFigure\t6-3,\twe\tplot\ta\tfew:\n\nxs\t=\t[x\t/\t10.0\tfor\tx\tin\trange(-50,\t50)]",
    "122": "plt.plot(xs,[normal_cdf(x,sigma=1)\tfor\tx\tin\txs],'-',label='mu=0,sigma=1')\nplt.plot(xs,[normal_cdf(x,sigma=2)\tfor\tx\tin\txs],'--',label='mu=0,sigma=2')\nplt.plot(xs,[normal_cdf(x,sigma=0.5)\tfor\tx\tin\txs],':',label='mu=0,sigma=0.5')\nplt.plot(xs,[normal_cdf(x,mu=-1)\tfor\tx\tin\txs],'-.',label='mu=-1,sigma=1')\nplt.legend(loc=4)\t#\tbottom\tright\nplt.title(\"Various\tNormal\tcdfs\")\nplt.show()\n\nFigure\t6-3.\tVarious\tnormal\tcdfs\n\nSometimes\twe\u2019ll\tneed\tto\tinvert\tnormal_cdf\tto\tfind\tthe\tvalue\tcorresponding\tto\ta\tspecified\nprobability.\tThere\u2019s\tno\tsimple\tway\tto\tcompute\tits\tinverse,\tbut\tnormal_cdf\tis\tcontinuous\nand\tstrictly\tincreasing,\tso\twe\tcan\tuse\ta\tbinary\tsearch:\n\ndef\tinverse_normal_cdf(p,\tmu=0,\tsigma=1,\ttolerance=0.00001):\n\t\t\t\t\"\"\"find\tapproximate\tinverse\tusing\tbinary\tsearch\"\"\"\n\n\t\t\t\t#\tif\tnot\tstandard,\tcompute\tstandard\tand\trescale\n\t\t\t\tif\tmu\t!=\t0\tor\tsigma\t!=\t1:\n\t\t\t\t\t\t\t\treturn\tmu\t+\tsigma\t*\tinverse_normal_cdf(p,\ttolerance=tolerance)\n\n\t\t\t\tlow_z,\tlow_p\t=\t-10.0,\t0\t\t\t\t\t\t\t\t\t\t\t\t#\tnormal_cdf(-10)\tis\t(very\tclose\tto)\t0\n\t\t\t\thi_z,\t\thi_p\t\t=\t\t10.0,\t1\t\t\t\t\t\t\t\t\t\t\t\t#\tnormal_cdf(10)\t\tis\t(very\tclose\tto)\t1\n\t\t\t\twhile\thi_z\t-\tlow_z\t>\ttolerance:\n\t\t\t\t\t\t\t\tmid_z\t=\t(low_z\t+\thi_z)\t/\t2\t\t\t\t\t#\tconsider\tthe\tmidpoint\n\t\t\t\t\t\t\t\tmid_p\t=\tnormal_cdf(mid_z)\t\t\t\t\t\t#\tand\tthe\tcdf's\tvalue\tthere\n\t\t\t\t\t\t\t\tif\tmid_p\t<\tp:\n\t\t\t\t\t\t\t\t\t\t\t\t#\tmidpoint\tis\tstill\ttoo\tlow,\tsearch\tabove\tit\n\t\t\t\t\t\t\t\t\t\t\t\tlow_z,\tlow_p\t=\tmid_z,\tmid_p\n\t\t\t\t\t\t\t\telif\tmid_p\t>\tp:\n\t\t\t\t\t\t\t\t\t\t\t\t#\tmidpoint\tis\tstill\ttoo\thigh,\tsearch\tbelow\tit\n\t\t\t\t\t\t\t\t\t\t\t\thi_z,\thi_p\t=\tmid_z,\tmid_p\n\t\t\t\t\t\t\t\telse:",
    "123": "break\n\n\t\t\t\treturn\tmid_z\n\nThe\tfunction\trepeatedly\tbisects\tintervals\tuntil\tit\tnarrows\tin\ton\ta\tZ\tthat\u2019s\tclose\tenough\tto\nthe\tdesired\tprobability.",
    "124": "The\tCentral\tLimit\tTheorem\n\nOne\treason\tthe\tnormal\tdistribution\tis\tso\tuseful\tis\tthe\tcentral\tlimit\ttheorem,\twhich\tsays\t(in\nessence)\tthat\ta\trandom\tvariable\tdefined\tas\tthe\taverage\tof\ta\tlarge\tnumber\tof\tindependent\nand\tidentically\tdistributed\trandom\tvariables\tis\titself\tapproximately\tnormally\tdistributed.\n\nIn\tparticular,\tif\t\nand\tif\tn\tis\tlarge,\tthen:\n\n\tare\trandom\tvariables\twith\tmean\t\n\n\tand\tstandard\tdeviation\t\n\n,\n\nis\tapproximately\tnormally\tdistributed\twith\tmean\t\nEquivalently\t(but\toften\tmore\tusefully),\n\n\tand\tstandard\tdeviation\t\n\n.\n\nis\tapproximately\tnormally\tdistributed\twith\tmean\t0\tand\tstandard\tdeviation\t1.\n\nAn\teasy\tway\tto\tillustrate\tthis\tis\tby\tlooking\tat\tbinomial\trandom\tvariables,\twhich\thave\ttwo\nparameters\tn\tand\tp.\tA\tBinomial(n,p)\trandom\tvariable\tis\tsimply\tthe\tsum\tof\tn\tindependent\nBernoulli(p)\trandom\tvariables,\teach\tof\twhich\tequals\t1\twith\tprobability\tp\tand\t0\twith\n\nprobability\t\n\n:\n\ndef\tbernoulli_trial(p):\n\t\t\t\treturn\t1\tif\trandom.random()\t<\tp\telse\t0\n\ndef\tbinomial(n,\tp):\n\t\t\t\treturn\tsum(bernoulli_trial(p)\tfor\t_\tin\trange(n))\n\nThe\tmean\tof\ta\tBernoulli(p)\tvariable\tis\tp,\tand\tits\tstandard\tdeviation\tis\t\ncentral\tlimit\ttheorem\tsays\tthat\tas\tn\tgets\tlarge,\ta\tBinomial(n,p)\tvariable\tis\tapproximately\ta\nnormal\trandom\tvariable\twith\tmean\t\n\n\tand\tstandard\tdeviation\t\n\n.\tThe\n\n.\tIf\twe\tplot\tboth,\tyou\tcan\teasily\tsee\tthe\tresemblance:\n\ndef\tmake_hist(p,\tn,\tnum_points):\n\n\t\t\t\tdata\t=\t[binomial(n,\tp)\tfor\t_\tin\trange(num_points)]\n\n\t\t\t\t#\tuse\ta\tbar\tchart\tto\tshow\tthe\tactual\tbinomial\tsamples\n\t\t\t\thistogram\t=\tCounter(data)\n\t\t\t\tplt.bar([x\t-\t0.4\tfor\tx\tin\thistogram.keys()],\n\t\t\t\t\t\t\t\t\t\t\t\t[v\t/\tnum_points\tfor\tv\tin\thistogram.values()],\n\t\t\t\t\t\t\t\t\t\t\t\t0.8,\n\t\t\t\t\t\t\t\t\t\t\t\tcolor='0.75')\n\n\t\t\t\tmu\t=\tp\t*\tn\n\t\t\t\tsigma\t=\tmath.sqrt(n\t*\tp\t*\t(1\t-\tp))",
    "125": "#\tuse\ta\tline\tchart\tto\tshow\tthe\tnormal\tapproximation\n\t\t\t\txs\t=\trange(min(data),\tmax(data)\t+\t1)\n\t\t\t\tys\t=\t[normal_cdf(i\t+\t0.5,\tmu,\tsigma)\t-\tnormal_cdf(i\t-\t0.5,\tmu,\tsigma)\n\t\t\t\t\t\t\t\t\t\tfor\ti\tin\txs]\n\t\t\t\tplt.plot(xs,ys)\n\t\t\t\tplt.title(\"Binomial\tDistribution\tvs.\tNormal\tApproximation\")\n\t\t\t\tplt.show()\n\nFor\texample,\twhen\tyou\tcall\tmake_hist(0.75,\t100,\t10000),\tyou\tget\tthe\tgraph\tin\nFigure\t6-4.\n\nFigure\t6-4.\tThe\toutput\tfrom\tmake_hist\n\nThe\tmoral\tof\tthis\tapproximation\tis\tthat\tif\tyou\twant\tto\tknow\tthe\tprobability\tthat\t(say)\ta\nfair\tcoin\tturns\tup\tmore\tthan\t60\theads\tin\t100\tflips,\tyou\tcan\testimate\tit\tas\tthe\tprobability\nthat\ta\tNormal(50,5)\tis\tgreater\tthan\t60,\twhich\tis\teasier\tthan\tcomputing\tthe\nBinomial(100,0.5)\tcdf.\t(Although\tin\tmost\tapplications\tyou\u2019d\tprobably\tbe\tusing\tstatistical\nsoftware\tthat\twould\tgladly\tcompute\twhatever\tprobabilities\tyou\twant.)",
    "126": "For\tFurther\tExploration\n\nscipy.stats\tcontains\tpdf\tand\tcdf\tfunctions\tfor\tmost\tof\tthe\tpopular\tprobability\ndistributions.\n\nRemember\thow,\tat\tthe\tend\tof\tChapter\t5,\tI\tsaid\tthat\tit\twould\tbe\ta\tgood\tidea\tto\tstudy\ta\nstatistics\ttextbook?\tIt\twould\talso\tbe\ta\tgood\tidea\tto\tstudy\ta\tprobability\ttextbook.\tThe\nbest\tone\tI\tknow\tthat\u2019s\tavailable\tonline\tis\tIntroduction\tto\tProbability.",
    "127": "",
    "128": "Chapter\t7.\tHypothesis\tand\tInference\n\nIt\tis\tthe\tmark\tof\ta\ttruly\tintelligent\tperson\tto\tbe\tmoved\tby\tstatistics.\n\nGeorge\tBernard\tShaw\n\nWhat\twill\twe\tdo\twith\tall\tthis\tstatistics\tand\tprobability\ttheory?\tThe\tscience\tpart\tof\tdata\nscience\tfrequently\tinvolves\tforming\tand\ttesting\thypotheses\tabout\tour\tdata\tand\tthe\nprocesses\tthat\tgenerate\tit.",
    "129": "Statistical\tHypothesis\tTesting\n\nOften,\tas\tdata\tscientists,\twe\u2019ll\twant\tto\ttest\twhether\ta\tcertain\thypothesis\tis\tlikely\tto\tbe\ttrue.\nFor\tour\tpurposes,\thypotheses\tare\tassertions\tlike\t\u201cthis\tcoin\tis\tfair\u201d\tor\t\u201cdata\tscientists\tprefer\nPython\tto\tR\u201d\tor\t\u201cpeople\tare\tmore\tlikely\tto\tnavigate\taway\tfrom\tthe\tpage\twithout\tever\nreading\tthe\tcontent\tif\twe\tpop\tup\tan\tirritating\tinterstitial\tadvertisement\twith\ta\ttiny,\thard-to-\nfind\tclose\tbutton\u201d\tthat\tcan\tbe\ttranslated\tinto\tstatistics\tabout\tdata.\tUnder\tvarious\nassumptions,\tthose\tstatistics\tcan\tbe\tthought\tof\tas\tobservations\tof\trandom\tvariables\tfrom\nknown\tdistributions,\twhich\tallows\tus\tto\tmake\tstatements\tabout\thow\tlikely\tthose\nassumptions\tare\tto\thold.\n\nIn\tthe\tclassical\tsetup,\twe\thave\ta\tnull\thypothesis\t\nand\tsome\talternative\thypothesis\t\ndecide\twhether\twe\tcan\treject\t\nan\texample.\n\n\tthat\trepresents\tsome\tdefault\tposition,\n\tthat\twe\u2019d\tlike\tto\tcompare\tit\twith.\tWe\tuse\tstatistics\tto\n\tas\tfalse\tor\tnot.\tThis\twill\tprobably\tmake\tmore\tsense\twith",
    "130": "Example:\tFlipping\ta\tCoin\n\nImagine\twe\thave\ta\tcoin\tand\twe\twant\tto\ttest\twhether\tit\u2019s\tfair.\tWe\u2019ll\tmake\tthe\tassumption\nthat\tthe\tcoin\thas\tsome\tprobability\tp\tof\tlanding\theads,\tand\tso\tour\tnull\thypothesis\tis\tthat\tthe\n\ncoin\tis\tfair\t\u2014\tthat\tis,\tthat\t\n\n.\tWe\u2019ll\ttest\tthis\tagainst\tthe\talternative\thypothesis\t\n\n.\n\nIn\tparticular,\tour\ttest\twill\tinvolve\tflipping\tthe\tcoin\tsome\tnumber\tn\ttimes\tand\tcounting\tthe\nnumber\tof\theads\tX.\tEach\tcoin\tflip\tis\ta\tBernoulli\ttrial,\twhich\tmeans\tthat\tX\tis\ta\nBinomial(n,p)\trandom\tvariable,\twhich\t(as\twe\tsaw\tin\tChapter\t6)\twe\tcan\tapproximate\tusing\nthe\tnormal\tdistribution:\n\ndef\tnormal_approximation_to_binomial(n,\tp):\n\t\t\t\t\"\"\"finds\tmu\tand\tsigma\tcorresponding\tto\ta\tBinomial(n,\tp)\"\"\"\n\t\t\t\tmu\t=\tp\t*\tn\n\t\t\t\tsigma\t=\tmath.sqrt(p\t*\t(1\t-\tp)\t*\tn)\n\t\t\t\treturn\tmu,\tsigma\n\nWhenever\ta\trandom\tvariable\tfollows\ta\tnormal\tdistribution,\twe\tcan\tuse\tnormal_cdf\tto\nfigure\tout\tthe\tprobability\tthat\tits\trealized\tvalue\tlies\twithin\t(or\toutside)\ta\tparticular\ninterval:\n\n#\tthe\tnormal\tcdf\t_is_\tthe\tprobability\tthe\tvariable\tis\tbelow\ta\tthreshold\nnormal_probability_below\t=\tnormal_cdf\n\n#\tit's\tabove\tthe\tthreshold\tif\tit's\tnot\tbelow\tthe\tthreshold\ndef\tnormal_probability_above(lo,\tmu=0,\tsigma=1):\n\t\t\t\treturn\t1\t-\tnormal_cdf(lo,\tmu,\tsigma)\n\n#\tit's\tbetween\tif\tit's\tless\tthan\thi,\tbut\tnot\tless\tthan\tlo\ndef\tnormal_probability_between(lo,\thi,\tmu=0,\tsigma=1):\n\t\t\t\treturn\tnormal_cdf(hi,\tmu,\tsigma)\t-\tnormal_cdf(lo,\tmu,\tsigma)\n\n#\tit's\toutside\tif\tit's\tnot\tbetween\ndef\tnormal_probability_outside(lo,\thi,\tmu=0,\tsigma=1):\n\t\t\t\treturn\t1\t-\tnormal_probability_between(lo,\thi,\tmu,\tsigma)\n\nWe\tcan\talso\tdo\tthe\treverse\t\u2014\tfind\teither\tthe\tnontail\tregion\tor\tthe\t(symmetric)\tinterval\naround\tthe\tmean\tthat\taccounts\tfor\ta\tcertain\tlevel\tof\tlikelihood.\tFor\texample,\tif\twe\twant\tto\nfind\tan\tinterval\tcentered\tat\tthe\tmean\tand\tcontaining\t60%\tprobability,\tthen\twe\tfind\tthe\ncutoffs\twhere\tthe\tupper\tand\tlower\ttails\teach\tcontain\t20%\tof\tthe\tprobability\t(leaving\t60%):\n\ndef\tnormal_upper_bound(probability,\tmu=0,\tsigma=1):\n\t\t\t\t\"\"\"returns\tthe\tz\tfor\twhich\tP(Z\t<=\tz)\t=\tprobability\"\"\"\n\t\t\t\treturn\tinverse_normal_cdf(probability,\tmu,\tsigma)\n\ndef\tnormal_lower_bound(probability,\tmu=0,\tsigma=1):\n\t\t\t\t\"\"\"returns\tthe\tz\tfor\twhich\tP(Z\t>=\tz)\t=\tprobability\"\"\"\n\t\t\t\treturn\tinverse_normal_cdf(1\t-\tprobability,\tmu,\tsigma)\n\ndef\tnormal_two_sided_bounds(probability,\tmu=0,\tsigma=1):\n\t\t\t\t\"\"\"returns\tthe\tsymmetric\t(about\tthe\tmean)\tbounds\n\t\t\t\tthat\tcontain\tthe\tspecified\tprobability\"\"\"\n\t\t\t\ttail_probability\t=\t(1\t-\tprobability)\t/\t2\n\n\t\t\t\t#\tupper\tbound\tshould\thave\ttail_probability\tabove\tit\n\t\t\t\tupper_bound\t=\tnormal_lower_bound(tail_probability,\tmu,\tsigma)",
    "131": "#\tlower\tbound\tshould\thave\ttail_probability\tbelow\tit\n\t\t\t\tlower_bound\t=\tnormal_upper_bound(tail_probability,\tmu,\tsigma)\n\n\t\t\t\treturn\tlower_bound,\tupper_bound\n\nIn\tparticular,\tlet\u2019s\tsay\tthat\twe\tchoose\tto\tflip\tthe\tcoin\t\nhypothesis\tof\tfairness\tis\ttrue,\tX\tshould\tbe\tdistributed\tapproximately\tnormally\twith\tmean\n50\tand\tstandard\tdeviation\t15.8:\n\n\ttimes.\tIf\tour\n\nmu_0,\tsigma_0\t=\tnormal_approximation_to_binomial(1000,\t0.5)\n\nWe\tneed\tto\tmake\ta\tdecision\tabout\tsignificance\t\u2014\thow\twilling\twe\tare\tto\tmake\ta\ttype\t1\n\teven\tthough\tit\u2019s\ttrue.\tFor\treasons\tlost\tto\nerror\t(\u201cfalse\tpositive\u201d),\tin\twhich\twe\treject\t\nthe\tannals\tof\thistory,\tthis\twillingness\tis\toften\tset\tat\t5%\tor\t1%.\tLet\u2019s\tchoose\t5%.\n\nConsider\tthe\ttest\tthat\trejects\t\n\n\tif\tX\tfalls\toutside\tthe\tbounds\tgiven\tby:\n\nnormal_two_sided_bounds(0.95,\tmu_0,\tsigma_0)\t\t\t#\t(469,\t531)\n\nAssuming\tp\treally\tequals\t0.5\t(i.e.,\t\n\tis\ttrue),\tthere\tis\tjust\ta\t5%\tchance\twe\tobserve\tan\tX\nthat\tlies\toutside\tthis\tinterval,\twhich\tis\tthe\texact\tsignificance\twe\twanted.\tSaid\tdifferently,\n\tis\ttrue,\tthen,\tapproximately\t19\ttimes\tout\tof\t20,\tthis\ttest\twill\tgive\tthe\tcorrect\tresult.\nif\t\n\nWe\tare\talso\toften\tinterested\tin\tthe\tpower\tof\ta\ttest,\twhich\tis\tthe\tprobability\tof\tnot\tmaking\ta\n\teven\tthough\tit\u2019s\tfalse.\tIn\torder\tto\tmeasure\tthis,\ntype\t2\terror,\tin\twhich\twe\tfail\tto\treject\t\nwe\thave\tto\tspecify\twhat\texactly\t\n\tbeing\tfalse\tmeans.\t(Knowing\tmerely\tthat\tp\tis\tnot\t0.5\ndoesn\u2019t\tgive\tyou\ta\tton\tof\tinformation\tabout\tthe\tdistribution\tof\tX.)\tIn\tparticular,\tlet\u2019s\tcheck\nwhat\thappens\tif\tp\tis\treally\t0.55,\tso\tthat\tthe\tcoin\tis\tslightly\tbiased\ttoward\theads.\n\nIn\tthat\tcase,\twe\tcan\tcalculate\tthe\tpower\tof\tthe\ttest\twith:\n\n#\t95%\tbounds\tbased\ton\tassumption\tp\tis\t0.5\nlo,\thi\t=\tnormal_two_sided_bounds(0.95,\tmu_0,\tsigma_0)\n\n#\tactual\tmu\tand\tsigma\tbased\ton\tp\t=\t0.55\nmu_1,\tsigma_1\t=\tnormal_approximation_to_binomial(1000,\t0.55)\n\n#\ta\ttype\t2\terror\tmeans\twe\tfail\tto\treject\tthe\tnull\thypothesis\n#\twhich\twill\thappen\twhen\tX\tis\tstill\tin\tour\toriginal\tinterval\ntype_2_probability\t=\tnormal_probability_between(lo,\thi,\tmu_1,\tsigma_1)\npower\t=\t1\t-\ttype_2_probability\t\t\t\t\t\t#\t0.887\n\nImagine\tinstead\tthat\tour\tnull\thypothesis\twas\tthat\tthe\tcoin\tis\tnot\tbiased\ttoward\theads,\tor\n\n.\tIn\tthat\tcase\twe\twant\ta\tone-sided\ttest\tthat\trejects\tthe\tnull\thypothesis\twhen\n\nthat\t\nX\tis\tmuch\tlarger\tthan\t50\tbut\tnot\twhen\tX\tis\tsmaller\tthan\t50.\tSo\ta\t5%-significance\ttest\ninvolves\tusing\tnormal_probability_below\tto\tfind\tthe\tcutoff\tbelow\twhich\t95%\tof\tthe\nprobability\tlies:\n\nhi\t=\tnormal_upper_bound(0.95,\tmu_0,\tsigma_0)\n#\tis\t526\t(<\t531,\tsince\twe\tneed\tmore\tprobability\tin\tthe\tupper\ttail)\n\ntype_2_probability\t=\tnormal_probability_below(hi,\tmu_1,\tsigma_1)",
    "132": "power\t=\t1\t-\ttype_2_probability\t\t\t\t\t\t#\t0.936\n\nThis\tis\ta\tmore\tpowerful\ttest,\tsince\tit\tno\tlonger\trejects\t\nvery\tunlikely\tto\thappen\tif\t\n531\t(which\tis\tsomewhat\tlikely\tto\thappen\tif\t\n\n\tis\ttrue)\tand\tinstead\trejects\t\n\n\tis\ttrue).\t===\tp-values\n\n\twhen\tX\tis\tbelow\t469\t(which\tis\n\twhen\tX\tis\tbetween\t526\tand\n\nAn\talternative\tway\tof\tthinking\tabout\tthe\tpreceding\ttest\tinvolves\tp-values.\tInstead\tof\nchoosing\tbounds\tbased\ton\tsome\tprobability\tcutoff,\twe\tcompute\tthe\tprobability\t\u2014\n\tis\ttrue\t\u2014\tthat\twe\twould\tsee\ta\tvalue\tat\tleast\tas\textreme\tas\tthe\tone\twe\nassuming\t\nactually\tobserved.\n\nFor\tour\ttwo-sided\ttest\tof\twhether\tthe\tcoin\tis\tfair,\twe\tcompute:\n\ndef\ttwo_sided_p_value(x,\tmu=0,\tsigma=1):\n\t\t\t\tif\tx\t>=\tmu:\n\t\t\t\t\t\t\t\t#\tif\tx\tis\tgreater\tthan\tthe\tmean,\tthe\ttail\tis\twhat's\tgreater\tthan\tx\n\t\t\t\t\t\t\t\treturn\t2\t*\tnormal_probability_above(x,\tmu,\tsigma)\n\t\t\t\telse:\n\t\t\t\t\t\t\t\t#\tif\tx\tis\tless\tthan\tthe\tmean,\tthe\ttail\tis\twhat's\tless\tthan\tx\n\t\t\t\t\t\t\t\treturn\t2\t*\tnormal_probability_below(x,\tmu,\tsigma)\n\nIf\twe\twere\tto\tsee\t530\theads,\twe\twould\tcompute:\n\ntwo_sided_p_value(529.5,\tmu_0,\tsigma_0)\t\t\t#\t0.062\n\nNOTE\n\nWhy\tdid\twe\tuse\t529.5\tinstead\tof\t530?\tThis\tis\twhat\u2019s\tcalled\ta\tcontinuity\tcorrection.\tIt\treflects\tthe\tfact\tthat\nnormal_probability_between(529.5,\t530.5,\tmu_0,\tsigma_0)\tis\ta\tbetter\testimate\tof\tthe\tprobability\tof\nseeing\t530\theads\tthan\tnormal_probability_between(530,\t531,\tmu_0,\tsigma_0)\tis.\n\nCorrespondingly,\tnormal_probability_above(529.5,\tmu_0,\tsigma_0)\tis\ta\tbetter\testimate\tof\tthe\nprobability\tof\tseeing\tat\tleast\t530\theads.\tYou\tmay\thave\tnoticed\tthat\twe\talso\tused\tthis\tin\tthe\tcode\tthat\nproduced\tFigure\t6-4.\n\nOne\tway\tto\tconvince\tyourself\tthat\tthis\tis\ta\tsensible\testimate\tis\twith\ta\tsimulation:\n\nextreme_value_count\t=\t0\nfor\t_\tin\trange(100000):\n\t\t\t\tnum_heads\t=\tsum(1\tif\trandom.random()\t<\t0.5\telse\t0\t\t\t\t#\tcount\t#\tof\theads\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\t_\tin\trange(1000))\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tin\t1000\tflips\n\t\t\t\tif\tnum_heads\t>=\t530\tor\tnum_heads\t<=\t470:\t\t\t\t\t\t\t\t\t\t\t\t\t#\tand\tcount\thow\toften\n\t\t\t\t\t\t\t\textreme_value_count\t+=\t1\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tthe\t#\tis\t'extreme'\n\nprint\textreme_value_count\t/\t100000\t\t\t#\t0.062\n\nSince\tthe\tp-value\tis\tgreater\tthan\tour\t5%\tsignificance,\twe\tdon\u2019t\treject\tthe\tnull.\tIf\twe\ninstead\tsaw\t532\theads,\tthe\tp-value\twould\tbe:\n\ntwo_sided_p_value(531.5,\tmu_0,\tsigma_0)\t\t\t#\t0.0463\n\nwhich\tis\tsmaller\tthan\tthe\t5%\tsignificance,\twhich\tmeans\twe\twould\treject\tthe\tnull.\tIt\u2019s\tthe\nexact\tsame\ttest\tas\tbefore.\tIt\u2019s\tjust\ta\tdifferent\tway\tof\tapproaching\tthe\tstatistics.",
    "133": "Similarly,\twe\twould\thave:\n\nupper_p_value\t=\tnormal_probability_above\nlower_p_value\t=\tnormal_probability_below\n\nFor\tour\tone-sided\ttest,\tif\twe\tsaw\t525\theads\twe\twould\tcompute:\n\nupper_p_value(524.5,\tmu_0,\tsigma_0)\t#\t0.061\n\nwhich\tmeans\twe\twouldn\u2019t\treject\tthe\tnull.\tIf\twe\tsaw\t527\theads,\tthe\tcomputation\twould\tbe:\n\nupper_p_value(526.5,\tmu_0,\tsigma_0)\t#\t0.047\n\nand\twe\twould\treject\tthe\tnull.\n\nWARNING\n\nMake\tsure\tyour\tdata\tis\troughly\tnormally\tdistributed\tbefore\tusing\tnormal_probability_above\tto\tcompute\np-values.\tThe\tannals\tof\tbad\tdata\tscience\tare\tfilled\twith\texamples\tof\tpeople\topining\tthat\tthe\tchance\tof\tsome\nobserved\tevent\toccurring\tat\trandom\tis\tone\tin\ta\tmillion,\twhen\twhat\tthey\treally\tmean\tis\t\u201cthe\tchance,\nassuming\tthe\tdata\tis\tdistributed\tnormally,\u201d\twhich\tis\tpretty\tmeaningless\tif\tthe\tdata\tisn\u2019t.\n\nThere\tare\tvarious\tstatistical\ttests\tfor\tnormality,\tbut\teven\tplotting\tthe\tdata\tis\ta\tgood\tstart.",
    "134": "Confidence\tIntervals\n\nWe\u2019ve\tbeen\ttesting\thypotheses\tabout\tthe\tvalue\tof\tthe\theads\tprobability\tp,\twhich\tis\ta\nparameter\tof\tthe\tunknown\t\u201cheads\u201d\tdistribution.\tWhen\tthis\tis\tthe\tcase,\ta\tthird\tapproach\tis\nto\tconstruct\ta\tconfidence\tinterval\taround\tthe\tobserved\tvalue\tof\tthe\tparameter.\n\nFor\texample,\twe\tcan\testimate\tthe\tprobability\tof\tthe\tunfair\tcoin\tby\tlooking\tat\tthe\taverage\nvalue\tof\tthe\tBernoulli\tvariables\tcorresponding\tto\teach\tflip\t\u2014\t1\tif\theads,\t0\tif\ttails.\tIf\twe\nobserve\t525\theads\tout\tof\t1,000\tflips,\tthen\twe\testimate\tp\tequals\t0.525.\n\nHow\tconfident\tcan\twe\tbe\tabout\tthis\testimate?\tWell,\tif\twe\tknew\tthe\texact\tvalue\tof\tp,\tthe\ncentral\tlimit\ttheorem\t(recall\t\u201cThe\tCentral\tLimit\tTheorem\u201d)\ttells\tus\tthat\tthe\taverage\tof\nthose\tBernoulli\tvariables\tshould\tbe\tapproximately\tnormal,\twith\tmean\tp\tand\tstandard\ndeviation:\n\nmath.sqrt(p\t*\t(1\t-\tp)\t/\t1000)\n\nHere\twe\tdon\u2019t\tknow\tp,\tso\tinstead\twe\tuse\tour\testimate:\n\np_hat\t=\t525\t/\t1000\nmu\t=\tp_hat\nsigma\t=\tmath.sqrt(p_hat\t*\t(1\t-\tp_hat)\t/\t1000)\t\t\t#\t0.0158\n\nThis\tis\tnot\tentirely\tjustified,\tbut\tpeople\tseem\tto\tdo\tit\tanyway.\tUsing\tthe\tnormal\napproximation,\twe\tconclude\tthat\twe\tare\t\u201c95%\tconfident\u201d\tthat\tthe\tfollowing\tinterval\ncontains\tthe\ttrue\tparameter\tp:\n\nnormal_two_sided_bounds(0.95,\tmu,\tsigma)\t\t\t\t\t\t\t\t#\t[0.4940,\t0.5560]\n\nThis\tis\ta\tstatement\tabout\tthe\tinterval,\tnot\tabout\tp.\tYou\tshould\tunderstand\tit\tas\tthe\tassertion\tthat\tif\tyou\twere\nto\trepeat\tthe\texperiment\tmany\ttimes,\t95%\tof\tthe\ttime\tthe\t\u201ctrue\u201d\tparameter\t(which\tis\tthe\tsame\tevery\ttime)\nwould\tlie\twithin\tthe\tobserved\tconfidence\tinterval\t(which\tmight\tbe\tdifferent\tevery\ttime).\n\nNOTE\n\nIn\tparticular,\twe\tdo\tnot\tconclude\tthat\tthe\tcoin\tis\tunfair,\tsince\t0.5\tfalls\twithin\tour\nconfidence\tinterval.\n\nIf\tinstead\twe\u2019d\tseen\t540\theads,\tthen\twe\u2019d\thave:\n\np_hat\t=\t540\t/\t1000\nmu\t=\tp_hat\nsigma\t=\tmath.sqrt(p_hat\t*\t(1\t-\tp_hat)\t/\t1000)\t#\t0.0158\nnormal_two_sided_bounds(0.95,\tmu,\tsigma)\t#\t[0.5091,\t0.5709]\n\nHere,\t\u201cfair\tcoin\u201d\tdoesn\u2019t\tlie\tin\tthe\tconfidence\tinterval.\t(The\t\u201cfair\tcoin\u201d\thypothesis\tdoesn\u2019t\npass\ta\ttest\tthat\tyou\u2019d\texpect\tit\tto\tpass\t95%\tof\tthe\ttime\tif\tit\twere\ttrue.)",
    "135": "P-hacking\n\nA\tprocedure\tthat\terroneously\trejects\tthe\tnull\thypothesis\tonly\t5%\tof\tthe\ttime\twill\t\u2014\tby\ndefinition\t\u2014\t5%\tof\tthe\ttime\terroneously\treject\tthe\tnull\thypothesis:\n\ndef\trun_experiment():\n\t\t\t\t\"\"\"flip\ta\tfair\tcoin\t1000\ttimes,\tTrue\t=\theads,\tFalse\t=\ttails\"\"\"\n\t\t\t\treturn\t[random.random()\t<\t0.5\tfor\t_\tin\trange(1000)]\n\ndef\treject_fairness(experiment):\n\t\t\t\t\"\"\"using\tthe\t5%\tsignificance\tlevels\"\"\"\n\t\t\t\tnum_heads\t=\tlen([flip\tfor\tflip\tin\texperiment\tif\tflip])\n\t\t\t\treturn\tnum_heads\t<\t469\tor\tnum_heads\t>\t531\n\nrandom.seed(0)\nexperiments\t=\t[run_experiment()\tfor\t_\tin\trange(1000)]\nnum_rejections\t=\tlen([experiment\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\texperiment\tin\texperiments\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\treject_fairness(experiment)])\n\nprint\tnum_rejections\t\t\t#\t46\n\nWhat\tthis\tmeans\tis\tthat\tif\tyou\u2019re\tsetting\tout\tto\tfind\t\u201csignificant\u201d\tresults,\tyou\tusually\tcan.\nTest\tenough\thypotheses\tagainst\tyour\tdata\tset,\tand\tone\tof\tthem\twill\talmost\tcertainly\tappear\nsignificant.\tRemove\tthe\tright\toutliers,\tand\tyou\tcan\tprobably\tget\tyour\tp\tvalue\tbelow\t0.05.\n(We\tdid\tsomething\tvaguely\tsimilar\tin\t\u201cCorrelation\u201d;\tdid\tyou\tnotice?)\n\nThis\tis\tsometimes\tcalled\tP-hacking\tand\tis\tin\tsome\tways\ta\tconsequence\tof\tthe\t\u201cinference\nfrom\tp-values\tframework.\u201d\tA\tgood\tarticle\tcriticizing\tthis\tapproach\tis\t\u201cThe\tEarth\tIs\nRound.\u201d\n\nIf\tyou\twant\tto\tdo\tgood\tscience,\tyou\tshould\tdetermine\tyour\thypotheses\tbefore\tlooking\tat\nthe\tdata,\tyou\tshould\tclean\tyour\tdata\twithout\tthe\thypotheses\tin\tmind,\tand\tyou\tshould\tkeep\nin\tmind\tthat\tp-values\tare\tnot\tsubstitutes\tfor\tcommon\tsense.\t(An\talternative\tapproach\tis\n\u201cBayesian\tInference\u201d.)",
    "136": "Example:\tRunning\tan\tA/B\tTest\n\nOne\tof\tyour\tprimary\tresponsibilities\tat\tDataSciencester\tis\texperience\toptimization,\twhich\nis\ta\teuphemism\tfor\ttrying\tto\tget\tpeople\tto\tclick\ton\tadvertisements.\tOne\tof\tyour\tadvertisers\nhas\tdeveloped\ta\tnew\tenergy\tdrink\ttargeted\tat\tdata\tscientists,\tand\tthe\tVP\tof\nAdvertisements\twants\tyour\thelp\tchoosing\tbetween\tadvertisement\tA\t(\u201ctastes\tgreat!\u201d)\tand\nadvertisement\tB\t(\u201cless\tbias!\u201d).\n\nBeing\ta\tscientist,\tyou\tdecide\tto\trun\tan\texperiment\tby\trandomly\tshowing\tsite\tvisitors\tone\nof\tthe\ttwo\tadvertisements\tand\ttracking\thow\tmany\tpeople\tclick\ton\teach\tone.\n\nIf\t990\tout\tof\t1,000\tA-viewers\tclick\ttheir\tad\twhile\tonly\t10\tout\tof\t1,000\tB-viewers\tclick\ntheir\tad,\tyou\tcan\tbe\tpretty\tconfident\tthat\tA\tis\tthe\tbetter\tad.\tBut\twhat\tif\tthe\tdifferences\tare\nnot\tso\tstark?\tHere\u2019s\twhere\tyou\u2019d\tuse\tstatistical\tinference.\n\nLet\u2019s\tsay\tthat\t\nview\tas\ta\tBernoulli\ttrial\twhere\t\n\n\tpeople\tsee\tad\tA,\tand\tthat\t\n\n\tof\tthem\tclick\tit.\tWe\tcan\tthink\tof\teach\tad\n\tis\tthe\tprobability\tthat\tsomeone\tclicks\tad\tA.\tThen\t(if\t\n\n\tis\tlarge,\twhich\tit\tis\there)\twe\tknow\tthat\t\n\n\tis\tapproximately\ta\tnormal\trandom\n\nvariable\twith\tmean\t\n\n\tand\tstandard\tdeviation\t\n\n.\n\nSimilarly,\t\n\ndeviation\t\n\n\tis\tapproximately\ta\tnormal\trandom\tvariable\twith\tmean\t\n\n\tand\tstandard\n\n:\n\ndef\testimated_parameters(N,\tn):\n\t\t\t\tp\t=\tn\t/\tN\n\t\t\t\tsigma\t=\tmath.sqrt(p\t*\t(1\t-\tp)\t/\tN)\n\t\t\t\treturn\tp,\tsigma\n\nIf\twe\tassume\tthose\ttwo\tnormals\tare\tindependent\t(which\tseems\treasonable,\tsince\tthe\nindividual\tBernoulli\ttrials\tought\tto\tbe),\tthen\ttheir\tdifference\tshould\talso\tbe\tnormal\twith\n\nmean\t\n\n\tand\tstandard\tdeviation\t\n\n.\n\nNOTE\n\nThis\tis\tsort\tof\tcheating.\tThe\tmath\tonly\tworks\tout\texactly\tlike\tthis\tif\tyou\tknow\tthe\tstandard\tdeviations.\tHere\nwe\u2019re\testimating\tthem\tfrom\tthe\tdata,\twhich\tmeans\tthat\twe\treally\tshould\tbe\tusing\ta\tt-distribution.\tBut\tfor\nlarge\tenough\tdata\tsets,\tit\u2019s\tclose\tenough\tthat\tit\tdoesn\u2019t\tmake\tmuch\tof\ta\tdifference.\n\nThis\tmeans\twe\tcan\ttest\tthe\tnull\thypothesis\tthat\t\n\tis\tzero),\tusing\tthe\tstatistic:\n\n\tand\t\n\n\tare\tthe\tsame\t(that\tis,\tthat\t\n\ndef\ta_b_test_statistic(N_A,\tn_A,\tN_B,\tn_B):\n\t\t\t\tp_A,\tsigma_A\t=\testimated_parameters(N_A,\tn_A)\n\t\t\t\tp_B,\tsigma_B\t=\testimated_parameters(N_B,\tn_B)\n\t\t\t\treturn\t(p_B\t-\tp_A)\t/\tmath.sqrt(sigma_A\t**\t2\t+\tsigma_B\t**\t2)\n\nwhich\tshould\tapproximately\tbe\ta\tstandard\tnormal.\n\nFor\texample,\tif\t\u201ctastes\tgreat\u201d\tgets\t200\tclicks\tout\tof\t1,000\tviews\tand\t\u201cless\tbias\u201d\tgets\t180\nclicks\tout\tof\t1,000\tviews,\tthe\tstatistic\tequals:",
    "137": "z\t=\ta_b_test_statistic(1000,\t200,\t1000,\t180)\t\t\t\t#\t-1.14\n\nThe\tprobability\tof\tseeing\tsuch\ta\tlarge\tdifference\tif\tthe\tmeans\twere\tactually\tequal\twould\nbe:\n\ntwo_sided_p_value(z)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t0.254\n\nwhich\tis\tlarge\tenough\tthat\tyou\tcan\u2019t\tconclude\tthere\u2019s\tmuch\tof\ta\tdifference.\tOn\tthe\tother\nhand,\tif\t\u201cless\tbias\u201d\tonly\tgot\t150\tclicks,\twe\u2019d\thave:\n\nz\t=\ta_b_test_statistic(1000,\t200,\t1000,\t150)\t\t\t\t#\t-2.94\ntwo_sided_p_value(z)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t0.003\n\nwhich\tmeans\tthere\u2019s\tonly\ta\t0.003\tprobability\tyou\u2019d\tsee\tsuch\ta\tlarge\tdifference\tif\tthe\tads\nwere\tequally\teffective.",
    "138": "Bayesian\tInference\n\nThe\tprocedures\twe\u2019ve\tlooked\tat\thave\tinvolved\tmaking\tprobability\tstatements\tabout\tour\ntests:\t\u201cthere\u2019s\tonly\ta\t3%\tchance\tyou\u2019d\tobserve\tsuch\tan\textreme\tstatistic\tif\tour\tnull\nhypothesis\twere\ttrue.\u201d\n\nAn\talternative\tapproach\tto\tinference\tinvolves\ttreating\tthe\tunknown\tparameters\tthemselves\nas\trandom\tvariables.\tThe\tanalyst\t(that\u2019s\tyou)\tstarts\twith\ta\tprior\tdistribution\tfor\tthe\nparameters\tand\tthen\tuses\tthe\tobserved\tdata\tand\tBayes\u2019s\tTheorem\tto\tget\tan\tupdated\nposterior\tdistribution\tfor\tthe\tparameters.\tRather\tthan\tmaking\tprobability\tjudgments\tabout\nthe\ttests,\tyou\tmake\tprobability\tjudgments\tabout\tthe\tparameters\tthemselves.\n\nFor\texample,\twhen\tthe\tunknown\tparameter\tis\ta\tprobability\t(as\tin\tour\tcoin-flipping\nexample),\twe\toften\tuse\ta\tprior\tfrom\tthe\tBeta\tdistribution,\twhich\tputs\tall\tits\tprobability\nbetween\t0\tand\t1:\n\ndef\tB(alpha,\tbeta):\n\t\t\t\t\"\"\"a\tnormalizing\tconstant\tso\tthat\tthe\ttotal\tprobability\tis\t1\"\"\"\n\t\t\t\treturn\tmath.gamma(alpha)\t*\tmath.gamma(beta)\t/\tmath.gamma(alpha\t+\tbeta)\n\ndef\tbeta_pdf(x,\talpha,\tbeta):\n\t\t\t\tif\tx\t<\t0\tor\tx\t>\t1:\t\t\t\t\t\t\t\t\t\t#\tno\tweight\toutside\tof\t[0,\t1]\n\t\t\t\t\t\t\t\treturn\t0\n\t\t\t\treturn\tx\t**\t(alpha\t-\t1)\t*\t(1\t-\tx)\t**\t(beta\t-\t1)\t/\tB(alpha,\tbeta)\n\nGenerally\tspeaking,\tthis\tdistribution\tcenters\tits\tweight\tat:\n\nalpha\t/\t(alpha\t+\tbeta)\n\nand\tthe\tlarger\talpha\tand\tbeta\tare,\tthe\t\u201ctighter\u201d\tthe\tdistribution\tis.\n\nFor\texample,\tif\talpha\tand\tbeta\tare\tboth\t1,\tit\u2019s\tjust\tthe\tuniform\tdistribution\t(centered\tat\n0.5,\tvery\tdispersed).\tIf\talpha\tis\tmuch\tlarger\tthan\tbeta,\tmost\tof\tthe\tweight\tis\tnear\t1.\tAnd\nif\talpha\tis\tmuch\tsmaller\tthan\tbeta,\tmost\tof\tthe\tweight\tis\tnear\tzero.\tFigure\t7-1\tshows\nseveral\tdifferent\tBeta\tdistributions.\n\nSo\tlet\u2019s\tsay\twe\tassume\ta\tprior\tdistribution\ton\tp.\tMaybe\twe\tdon\u2019t\twant\tto\ttake\ta\tstand\ton\nwhether\tthe\tcoin\tis\tfair,\tand\twe\tchoose\talpha\tand\tbeta\tto\tboth\tequal\t1.\tOr\tmaybe\twe\thave\na\tstrong\tbelief\tthat\tit\tlands\theads\t55%\tof\tthe\ttime,\tand\twe\tchoose\talpha\tequals\t55,\tbeta\nequals\t45.\n\nThen\twe\tflip\tour\tcoin\ta\tbunch\tof\ttimes\tand\tsee\th\theads\tand\tt\ttails.\tBayes\u2019s\tTheorem\t(and\nsome\tmathematics\tthat\u2019s\ttoo\ttedious\tfor\tus\tto\tgo\tthrough\there)\ttells\tus\tthat\tthe\tposterior\ndistribution\tfor\tp\tis\tagain\ta\tBeta\tdistribution\tbut\twith\tparameters\talpha\t+\th\tand\tbeta\t+\nt.",
    "139": "NOTE\n\nIt\tis\tno\tcoincidence\tthat\tthe\tposterior\tdistribution\twas\tagain\ta\tBeta\tdistribution.\tThe\tnumber\tof\theads\tis\ngiven\tby\ta\tBinomial\tdistribution,\tand\tthe\tBeta\tis\tthe\tconjugate\tprior\tto\tthe\tBinomial\tdistribution.\tThis\nmeans\tthat\twhenever\tyou\tupdate\ta\tBeta\tprior\tusing\tobservations\tfrom\tthe\tcorresponding\tbinomial,\tyou\twill\nget\tback\ta\tBeta\tposterior.\n\nLet\u2019s\tsay\tyou\tflip\tthe\tcoin\t10\ttimes\tand\tsee\tonly\t3\theads.\n\nFigure\t7-1.\tExample\tBeta\tdistributions\n\nIf\tyou\tstarted\twith\tthe\tuniform\tprior\t(in\tsome\tsense\trefusing\tto\ttake\ta\tstand\tabout\tthe\ncoin\u2019s\tfairness),\tyour\tposterior\tdistribution\twould\tbe\ta\tBeta(4,\t8),\tcentered\taround\t0.33.\nSince\tyou\tconsidered\tall\tprobabilities\tequally\tlikely,\tyour\tbest\tguess\tis\tsomething\tpretty\nclose\tto\tthe\tobserved\tprobability.\n\nIf\tyou\tstarted\twith\ta\tBeta(20,\t20)\t(expressing\tthe\tbelief\tthat\tthe\tcoin\twas\troughly\tfair),\nyour\tposterior\tdistribution\twould\tbe\ta\tBeta(23,\t27),\tcentered\taround\t0.46,\tindicating\ta\nrevised\tbelief\tthat\tmaybe\tthe\tcoin\tis\tslightly\tbiased\ttoward\ttails.\n\nAnd\tif\tyou\tstarted\twith\ta\tBeta(30,\t10)\t(expressing\ta\tbelief\tthat\tthe\tcoin\twas\tbiased\tto\tflip\n75%\theads),\tyour\tposterior\tdistribution\twould\tbe\ta\tBeta(33,\t17),\tcentered\taround\t0.66.\tIn\nthat\tcase\tyou\u2019d\tstill\tbelieve\tin\ta\theads\tbias,\tbut\tless\tstrongly\tthan\tyou\tdid\tinitially.\tThese\nthree\tdifferent\tposteriors\tare\tplotted\tin\tFigure\t7-2.",
    "140": "Figure\t7-2.\tPosteriors\tarising\tfrom\tdifferent\tpriors\n\nIf\tyou\tflipped\tthe\tcoin\tmore\tand\tmore\ttimes,\tthe\tprior\twould\tmatter\tless\tand\tless\tuntil\neventually\tyou\u2019d\thave\t(nearly)\tthe\tsame\tposterior\tdistribution\tno\tmatter\twhich\tprior\tyou\nstarted\twith.\n\nFor\texample,\tno\tmatter\thow\tbiased\tyou\tinitially\tthought\tthe\tcoin\twas,\tit\twould\tbe\thard\tto\nmaintain\tthat\tbelief\tafter\tseeing\t1,000\theads\tout\tof\t2,000\tflips\t(unless\tyou\tare\ta\tlunatic\nwho\tpicks\tsomething\tlike\ta\tBeta(1000000,1)\tprior).\n\nWhat\u2019s\tinteresting\tis\tthat\tthis\tallows\tus\tto\tmake\tprobability\tstatements\tabout\thypotheses:\n\u201cBased\ton\tthe\tprior\tand\tthe\tobserved\tdata,\tthere\tis\tonly\ta\t5%\tlikelihood\tthe\tcoin\u2019s\theads\nprobability\tis\tbetween\t49%\tand\t51%.\u201d\tThis\tis\tphilosophically\tvery\tdifferent\tfrom\ta\nstatement\tlike\t\u201cif\tthe\tcoin\twere\tfair\twe\twould\texpect\tto\tobserve\tdata\tso\textreme\tonly\t5%\nof\tthe\ttime.\u201d\n\nUsing\tBayesian\tinference\tto\ttest\thypotheses\tis\tconsidered\tsomewhat\tcontroversial\t\u2014\tin\npart\tbecause\tits\tmathematics\tcan\tget\tsomewhat\tcomplicated,\tand\tin\tpart\tbecause\tof\tthe\nsubjective\tnature\tof\tchoosing\ta\tprior.\tWe\twon\u2019t\tuse\tit\tany\tfurther\tin\tthis\tbook,\tbut\tit\u2019s\ngood\tto\tknow\tabout.",
    "141": "For\tFurther\tExploration\n\nWe\u2019ve\tbarely\tscratched\tthe\tsurface\tof\twhat\tyou\tshould\tknow\tabout\tstatistical\tinference.\nThe\tbooks\trecommended\tat\tthe\tend\tof\tChapter\t5\tgo\tinto\ta\tlot\tmore\tdetail.\n\nCoursera\toffers\ta\tData\tAnalysis\tand\tStatistical\tInference\tcourse\tthat\tcovers\tmany\tof\nthese\ttopics.",
    "142": "",
    "143": "Chapter\t8.\tGradient\tDescent\n\nThose\twho\tboast\tof\ttheir\tdescent,\tbrag\ton\twhat\tthey\towe\tto\tothers.\n\nSeneca\n\nFrequently\twhen\tdoing\tdata\tscience,\twe\u2019ll\tbe\ttrying\tto\tthe\tfind\tthe\tbest\tmodel\tfor\ta\tcertain\nsituation.\tAnd\tusually\t\u201cbest\u201d\twill\tmean\tsomething\tlike\t\u201cminimizes\tthe\terror\tof\tthe\tmodel\u201d\nor\t\u201cmaximizes\tthe\tlikelihood\tof\tthe\tdata.\u201d\tIn\tother\twords,\tit\twill\trepresent\tthe\tsolution\tto\nsome\tsort\tof\toptimization\tproblem.\n\nThis\tmeans\twe\u2019ll\tneed\tto\tsolve\ta\tnumber\tof\toptimization\tproblems.\tAnd\tin\tparticular,\nwe\u2019ll\tneed\tto\tsolve\tthem\tfrom\tscratch.\tOur\tapproach\twill\tbe\ta\ttechnique\tcalled\tgradient\ndescent,\twhich\tlends\titself\tpretty\twell\tto\ta\tfrom-scratch\ttreatment.\tYou\tmight\tnot\tfind\tit\nsuper\texciting\tin\tand\tof\titself,\tbut\tit\twill\tenable\tus\tto\tdo\texciting\tthings\tthroughout\tthe\nbook,\tso\tbear\twith\tme.",
    "144": "The\tIdea\tBehind\tGradient\tDescent\n\nSuppose\twe\thave\tsome\tfunction\tf\tthat\ttakes\tas\tinput\ta\tvector\tof\treal\tnumbers\tand\toutputs\na\tsingle\treal\tnumber.\tOne\tsimple\tsuch\tfunction\tis:\n\ndef\tsum_of_squares(v):\n\t\t\t\t\"\"\"computes\tthe\tsum\tof\tsquared\telements\tin\tv\"\"\"\n\t\t\t\treturn\tsum(v_i\t**\t2\tfor\tv_i\tin\tv)\n\nWe\u2019ll\tfrequently\tneed\tto\tmaximize\t(or\tminimize)\tsuch\tfunctions.\tThat\tis,\twe\tneed\tto\tfind\nthe\tinput\tv\tthat\tproduces\tthe\tlargest\t(or\tsmallest)\tpossible\tvalue.\n\nFor\tfunctions\tlike\tours,\tthe\tgradient\t(if\tyou\tremember\tyour\tcalculus,\tthis\tis\tthe\tvector\tof\npartial\tderivatives)\tgives\tthe\tinput\tdirection\tin\twhich\tthe\tfunction\tmost\tquickly\tincreases.\n(If\tyou\tdon\u2019t\tremember\tyour\tcalculus,\ttake\tmy\tword\tfor\tit\tor\tlook\tit\tup\ton\tthe\tInternet.)\n\nAccordingly,\tone\tapproach\tto\tmaximizing\ta\tfunction\tis\tto\tpick\ta\trandom\tstarting\tpoint,\ncompute\tthe\tgradient,\ttake\ta\tsmall\tstep\tin\tthe\tdirection\tof\tthe\tgradient\t(i.e.,\tthe\tdirection\nthat\tcauses\tthe\tfunction\tto\tincrease\tthe\tmost),\tand\trepeat\twith\tthe\tnew\tstarting\tpoint.\nSimilarly,\tyou\tcan\ttry\tto\tminimize\ta\tfunction\tby\ttaking\tsmall\tsteps\tin\tthe\topposite\ndirection,\tas\tshown\tin\tFigure\t8-1.\n\nFigure\t8-1.\tFinding\ta\tminimum\tusing\tgradient\tdescent\n\nNOTE\n\nIf\ta\tfunction\thas\ta\tunique\tglobal\tminimum,\tthis\tprocedure\tis\tlikely\tto\tfind\tit.\tIf\ta\tfunction\thas\tmultiple\n(local)\tminima,\tthis\tprocedure\tmight\t\u201cfind\u201d\tthe\twrong\tone\tof\tthem,\tin\twhich\tcase\tyou\tmight\tre-run\tthe\nprocedure\tfrom\ta\tvariety\tof\tstarting\tpoints.\tIf\ta\tfunction\thas\tno\tminimum,\tthen\tit\u2019s\tpossible\tthe\tprocedure\nmight\tgo\ton\tforever.",
    "145": "Estimating\tthe\tGradient\n\nIf\tf\tis\ta\tfunction\tof\tone\tvariable,\tits\tderivative\tat\ta\tpoint\tx\tmeasures\thow\tf(x)\tchanges\nwhen\twe\tmake\ta\tvery\tsmall\tchange\tto\tx.\tIt\tis\tdefined\tas\tthe\tlimit\tof\tthe\tdifference\nquotients:\n\ndef\tdifference_quotient(f,\tx,\th):\n\t\t\t\treturn\t(f(x\t+\th)\t-\tf(x))\t/\th\n\nas\th\tapproaches\tzero.\n\n(Many\ta\twould-be\tcalculus\tstudent\thas\tbeen\tstymied\tby\tthe\tmathematical\tdefinition\tof\nlimit.\tHere\twe\u2019ll\tcheat\tand\tsimply\tsay\tthat\tit\tmeans\twhat\tyou\tthink\tit\tmeans.)\n\nFigure\t8-2.\tApproximating\ta\tderivative\twith\ta\tdifference\tquotient\n\nThe\tderivative\tis\tthe\tslope\tof\tthe\ttangent\tline\tat\t\n\n,\twhile\tthe\tdifference\tquotient\n\nis\tthe\tslope\tof\tthe\tnot-quite-tangent\tline\tthat\truns\tthrough\t\ngets\tsmaller\tand\tsmaller,\tthe\tnot-quite-tangent\tline\tgets\tcloser\tand\tcloser\tto\tthe\ttangent\nline\t(Figure\t8-2).\n\n.\tAs\th\n\nFor\tmany\tfunctions\tit\u2019s\teasy\tto\texactly\tcalculate\tderivatives.\tFor\texample,\tthe\tsquare\nfunction:",
    "146": "def\tsquare(x):\n\t\t\t\treturn\tx\t*\tx\n\nhas\tthe\tderivative:\n\ndef\tderivative(x):\n\t\t\t\treturn\t2\t*\tx\n\nwhich\tyou\tcan\tcheck\t\u2014\tif\tyou\tare\tso\tinclined\t\u2014\tby\texplicitly\tcomputing\tthe\tdifference\nquotient\tand\ttaking\tthe\tlimit.\n\nWhat\tif\tyou\tcouldn\u2019t\t(or\tdidn\u2019t\twant\tto)\tfind\tthe\tgradient?\tAlthough\twe\tcan\u2019t\ttake\tlimits\nin\tPython,\twe\tcan\testimate\tderivatives\tby\tevaluating\tthe\tdifference\tquotient\tfor\ta\tvery\nsmall\te.\tFigure\t8-3\tshows\tthe\tresults\tof\tone\tsuch\testimation:\n\nderivative_estimate\t=\tpartial(difference_quotient,\tsquare,\th=0.00001)\n\n#\tplot\tto\tshow\tthey're\tbasically\tthe\tsame\nimport\tmatplotlib.pyplot\tas\tplt\nx\t=\trange(-10,10)\nplt.title(\"Actual\tDerivatives\tvs.\tEstimates\")\nplt.plot(x,\tmap(derivative,\tx),\t'rx',\tlabel='Actual')\t\t\t\t\t\t\t\t\t\t\t\t\t#\tred\t\tx\nplt.plot(x,\tmap(derivative_estimate,\tx),\t'b+',\tlabel='Estimate')\t\t#\tblue\t+\nplt.legend(loc=9)\nplt.show()\n\nFigure\t8-3.\tGoodness\tof\tdifference\tquotient\tapproximation",
    "147": "When\tf\tis\ta\tfunction\tof\tmany\tvariables,\tit\thas\tmultiple\tpartial\tderivatives,\teach\tindicating\nhow\tf\tchanges\twhen\twe\tmake\tsmall\tchanges\tin\tjust\tone\tof\tthe\tinput\tvariables.\n\nWe\tcalculate\tits\tith\tpartial\tderivative\tby\ttreating\tit\tas\ta\tfunction\tof\tjust\tits\tith\tvariable,\nholding\tthe\tother\tvariables\tfixed:\n\ndef\tpartial_difference_quotient(f,\tv,\ti,\th):\n\t\t\t\t\"\"\"compute\tthe\tith\tpartial\tdifference\tquotient\tof\tf\tat\tv\"\"\"\n\t\t\t\tw\t=\t[v_j\t+\t(h\tif\tj\t==\ti\telse\t0)\t\t\t\t#\tadd\th\tto\tjust\tthe\tith\telement\tof\tv\n\t\t\t\t\t\t\t\t\tfor\tj,\tv_j\tin\tenumerate(v)]\n\n\t\t\t\treturn\t(f(w)\t-\tf(v))\t/\th\n\nafter\twhich\twe\tcan\testimate\tthe\tgradient\tthe\tsame\tway:\n\ndef\testimate_gradient(f,\tv,\th=0.00001):\n\t\t\t\treturn\t[partial_difference_quotient(f,\tv,\ti,\th)\n\t\t\t\t\t\t\t\t\t\t\t\tfor\ti,\t_\tin\tenumerate(v)]\n\nA\tmajor\tdrawback\tto\tthis\t\u201cestimate\tusing\tdifference\tquotients\u201d\tapproach\tis\tthat\tit\u2019s\tcomputationally\nexpensive.\tIf\tv\thas\tlength\tn,\testimate_gradient\thas\tto\tevaluate\tf\ton\t2n\tdifferent\tinputs.\tIf\tyou\u2019re\nrepeatedly\testimating\tgradients,\tyou\u2019re\tdoing\ta\twhole\tlot\tof\textra\twork.\n\nNOTE",
    "148": "Using\tthe\tGradient\n\nIt\u2019s\teasy\tto\tsee\tthat\tthe\tsum_of_squares\tfunction\tis\tsmallest\twhen\tits\tinput\tv\tis\ta\tvector\tof\nzeroes.\tBut\timagine\twe\tdidn\u2019t\tknow\tthat.\tLet\u2019s\tuse\tgradients\tto\tfind\tthe\tminimum\tamong\nall\tthree-dimensional\tvectors.\tWe\u2019ll\tjust\tpick\ta\trandom\tstarting\tpoint\tand\tthen\ttake\ttiny\nsteps\tin\tthe\topposite\tdirection\tof\tthe\tgradient\tuntil\twe\treach\ta\tpoint\twhere\tthe\tgradient\tis\nvery\tsmall:\n\ndef\tstep(v,\tdirection,\tstep_size):\n\t\t\t\t\"\"\"move\tstep_size\tin\tthe\tdirection\tfrom\tv\"\"\"\n\t\t\t\treturn\t[v_i\t+\tstep_size\t*\tdirection_i\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tv_i,\tdirection_i\tin\tzip(v,\tdirection)]\n\ndef\tsum_of_squares_gradient(v):\n\t\t\t\treturn\t[2\t*\tv_i\tfor\tv_i\tin\tv]\n\n#\tpick\ta\trandom\tstarting\tpoint\nv\t=\t[random.randint(-10,10)\tfor\ti\tin\trange(3)]\n\ntolerance\t=\t0.0000001\n\nwhile\tTrue:\n\t\t\t\tgradient\t=\tsum_of_squares_gradient(v)\t\t\t#\tcompute\tthe\tgradient\tat\tv\n\t\t\t\tnext_v\t=\tstep(v,\tgradient,\t-0.01)\t\t\t\t\t\t\t#\ttake\ta\tnegative\tgradient\tstep\n\t\t\t\tif\tdistance(next_v,\tv)\t<\ttolerance:\t\t\t\t\t#\tstop\tif\twe're\tconverging\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\tv\t=\tnext_v\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tcontinue\tif\twe're\tnot\n\nIf\tyou\trun\tthis,\tyou\u2019ll\tfind\tthat\tit\talways\tends\tup\twith\ta\tv\tthat\u2019s\tvery\tclose\tto\t[0,0,0].\tThe\nsmaller\tyou\tmake\tthe\ttolerance,\tthe\tcloser\tit\twill\tget.",
    "149": "Choosing\tthe\tRight\tStep\tSize\n\nAlthough\tthe\trationale\tfor\tmoving\tagainst\tthe\tgradient\tis\tclear,\thow\tfar\tto\tmove\tis\tnot.\nIndeed,\tchoosing\tthe\tright\tstep\tsize\tis\tmore\tof\tan\tart\tthan\ta\tscience.\tPopular\toptions\ninclude:\n\nUsing\ta\tfixed\tstep\tsize\n\nGradually\tshrinking\tthe\tstep\tsize\tover\ttime\n\nAt\teach\tstep,\tchoosing\tthe\tstep\tsize\tthat\tminimizes\tthe\tvalue\tof\tthe\tobjective\tfunction\n\nThe\tlast\tsounds\toptimal\tbut\tis,\tin\tpractice,\ta\tcostly\tcomputation.\tWe\tcan\tapproximate\tit\tby\ntrying\ta\tvariety\tof\tstep\tsizes\tand\tchoosing\tthe\tone\tthat\tresults\tin\tthe\tsmallest\tvalue\tof\tthe\nobjective\tfunction:\n\nstep_sizes\t=\t[100,\t10,\t1,\t0.1,\t0.01,\t0.001,\t0.0001,\t0.00001]\n\nIt\tis\tpossible\tthat\tcertain\tstep\tsizes\twill\tresult\tin\tinvalid\tinputs\tfor\tour\tfunction.\tSo\twe\u2019ll\nneed\tto\tcreate\ta\t\u201csafe\tapply\u201d\tfunction\tthat\treturns\tinfinity\t(which\tshould\tnever\tbe\tthe\nminimum\tof\tanything)\tfor\tinvalid\tinputs:\n\ndef\tsafe(f):\n\t\t\t\t\"\"\"return\ta\tnew\tfunction\tthat's\tthe\tsame\tas\tf,\n\t\t\t\texcept\tthat\tit\toutputs\tinfinity\twhenever\tf\tproduces\tan\terror\"\"\"\n\t\t\t\tdef\tsafe_f(*args,\t**kwargs):\n\t\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\t\t\t\t\treturn\tf(*args,\t**kwargs)\n\t\t\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\t\t\t\t\t\treturn\tfloat('inf')\t\t\t\t\t\t\t\t\t#\tthis\tmeans\t\"infinity\"\tin\tPython\n\t\t\t\treturn\tsafe_f",
    "150": "Putting\tIt\tAll\tTogether\n\nIn\tthe\tgeneral\tcase,\twe\thave\tsome\ttarget_fn\tthat\twe\twant\tto\tminimize,\tand\twe\talso\thave\nits\tgradient_fn.\tFor\texample,\tthe\ttarget_fn\tcould\trepresent\tthe\terrors\tin\ta\tmodel\tas\ta\nfunction\tof\tits\tparameters,\tand\twe\tmight\twant\tto\tfind\tthe\tparameters\tthat\tmake\tthe\terrors\nas\tsmall\tas\tpossible.\n\nFurthermore,\tlet\u2019s\tsay\twe\thave\t(somehow)\tchosen\ta\tstarting\tvalue\tfor\tthe\tparameters\ntheta_0.\tThen\twe\tcan\timplement\tgradient\tdescent\tas:\n\ndef\tminimize_batch(target_fn,\tgradient_fn,\ttheta_0,\ttolerance=0.000001):\n\t\t\t\t\"\"\"use\tgradient\tdescent\tto\tfind\ttheta\tthat\tminimizes\ttarget\tfunction\"\"\"\n\n\t\t\t\tstep_sizes\t=\t[100,\t10,\t1,\t0.1,\t0.01,\t0.001,\t0.0001,\t0.00001]\n\n\t\t\t\ttheta\t=\ttheta_0\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tset\ttheta\tto\tinitial\tvalue\n\t\t\t\ttarget_fn\t=\tsafe(target_fn)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tsafe\tversion\tof\ttarget_fn\n\t\t\t\tvalue\t=\ttarget_fn(theta)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tvalue\twe're\tminimizing\n\n\t\t\t\twhile\tTrue:\n\t\t\t\t\t\t\t\tgradient\t=\tgradient_fn(theta)\n\t\t\t\t\t\t\t\tnext_thetas\t=\t[step(theta,\tgradient,\t-step_size)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tstep_size\tin\tstep_sizes]\n\n\t\t\t\t\t\t\t\t#\tchoose\tthe\tone\tthat\tminimizes\tthe\terror\tfunction\n\t\t\t\t\t\t\t\tnext_theta\t=\tmin(next_thetas,\tkey=target_fn)\n\t\t\t\t\t\t\t\tnext_value\t=\ttarget_fn(next_theta)\n\n\t\t\t\t\t\t\t\t#\tstop\tif\twe're\t\"converging\"\n\t\t\t\t\t\t\t\tif\tabs(value\t-\tnext_value)\t<\ttolerance:\n\t\t\t\t\t\t\t\t\t\t\t\treturn\ttheta\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t\t\ttheta,\tvalue\t=\tnext_theta,\tnext_value\n\nWe\tcalled\tit\tminimize_batch\tbecause,\tfor\teach\tgradient\tstep,\tit\tlooks\tat\tthe\tentire\tdata\tset\n(because\ttarget_fn\treturns\tthe\terror\ton\tthe\twhole\tdata\tset).\tIn\tthe\tnext\tsection,\twe\u2019ll\tsee\nan\talternative\tapproach\tthat\tonly\tlooks\tat\tone\tdata\tpoint\tat\ta\ttime.\n\nSometimes\twe\u2019ll\tinstead\twant\tto\tmaximize\ta\tfunction,\twhich\twe\tcan\tdo\tby\tminimizing\tits\nnegative\t(which\thas\ta\tcorresponding\tnegative\tgradient):\n\ndef\tnegate(f):\n\t\t\t\t\"\"\"return\ta\tfunction\tthat\tfor\tany\tinput\tx\treturns\t-f(x)\"\"\"\n\t\t\t\treturn\tlambda\t*args,\t**kwargs:\t-f(*args,\t**kwargs)\n\ndef\tnegate_all(f):\n\t\t\t\t\"\"\"the\tsame\twhen\tf\treturns\ta\tlist\tof\tnumbers\"\"\"\n\t\t\t\treturn\tlambda\t*args,\t**kwargs:\t[-y\tfor\ty\tin\tf(*args,\t**kwargs)]\n\ndef\tmaximize_batch(target_fn,\tgradient_fn,\ttheta_0,\ttolerance=0.000001):\n\t\t\t\treturn\tminimize_batch(negate(target_fn),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnegate_all(gradient_fn),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttheta_0,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttolerance)",
    "151": "Stochastic\tGradient\tDescent\n\nAs\twe\tmentioned\tbefore,\toften\twe\u2019ll\tbe\tusing\tgradient\tdescent\tto\tchoose\tthe\tparameters\tof\na\tmodel\tin\ta\tway\tthat\tminimizes\tsome\tnotion\tof\terror.\tUsing\tthe\tprevious\tbatch\tapproach,\neach\tgradient\tstep\trequires\tus\tto\tmake\ta\tprediction\tand\tcompute\tthe\tgradient\tfor\tthe\twhole\ndata\tset,\twhich\tmakes\teach\tstep\ttake\ta\tlong\ttime.\n\nNow,\tusually\tthese\terror\tfunctions\tare\tadditive,\twhich\tmeans\tthat\tthe\tpredictive\terror\ton\nthe\twhole\tdata\tset\tis\tsimply\tthe\tsum\tof\tthe\tpredictive\terrors\tfor\teach\tdata\tpoint.\n\nWhen\tthis\tis\tthe\tcase,\twe\tcan\tinstead\tapply\ta\ttechnique\tcalled\tstochastic\tgradient\tdescent,\nwhich\tcomputes\tthe\tgradient\t(and\ttakes\ta\tstep)\tfor\tonly\tone\tpoint\tat\ta\ttime.\tIt\tcycles\tover\nour\tdata\trepeatedly\tuntil\tit\treaches\ta\tstopping\tpoint.\n\nDuring\teach\tcycle,\twe\u2019ll\twant\tto\titerate\tthrough\tour\tdata\tin\ta\trandom\torder:\n\ndef\tin_random_order(data):\n\t\t\t\t\"\"\"generator\tthat\treturns\tthe\telements\tof\tdata\tin\trandom\torder\"\"\"\n\t\t\t\tindexes\t=\t[i\tfor\ti,\t_\tin\tenumerate(data)]\t\t#\tcreate\ta\tlist\tof\tindexes\n\t\t\t\trandom.shuffle(indexes)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tshuffle\tthem\n\t\t\t\tfor\ti\tin\tindexes:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\treturn\tthe\tdata\tin\tthat\torder\n\t\t\t\t\t\t\t\tyield\tdata[i]\n\nAnd\twe\u2019ll\twant\tto\ttake\ta\tgradient\tstep\tfor\teach\tdata\tpoint.\tThis\tapproach\tleaves\tthe\npossibility\tthat\twe\tmight\tcircle\taround\tnear\ta\tminimum\tforever,\tso\twhenever\twe\tstop\ngetting\timprovements\twe\u2019ll\tdecrease\tthe\tstep\tsize\tand\teventually\tquit:\n\ndef\tminimize_stochastic(target_fn,\tgradient_fn,\tx,\ty,\ttheta_0,\talpha_0=0.01):\n\n\t\t\t\tdata\t=\tzip(x,\ty)\n\t\t\t\ttheta\t=\ttheta_0\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tinitial\tguess\n\t\t\t\talpha\t=\talpha_0\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tinitial\tstep\tsize\n\t\t\t\tmin_theta,\tmin_value\t=\tNone,\tfloat(\"inf\")\t\t\t#\tthe\tminimum\tso\tfar\n\t\t\t\titerations_with_no_improvement\t=\t0\n\n\t\t\t\t#\tif\twe\tever\tgo\t100\titerations\twith\tno\timprovement,\tstop\n\t\t\t\twhile\titerations_with_no_improvement\t<\t100:\n\t\t\t\t\t\t\t\tvalue\t=\tsum(\ttarget_fn(x_i,\ty_i,\ttheta)\tfor\tx_i,\ty_i\tin\tdata\t)\n\n\t\t\t\t\t\t\t\tif\tvalue\t<\tmin_value:\n\t\t\t\t\t\t\t\t\t\t\t\t#\tif\twe've\tfound\ta\tnew\tminimum,\tremember\tit\n\t\t\t\t\t\t\t\t\t\t\t\t#\tand\tgo\tback\tto\tthe\toriginal\tstep\tsize\n\t\t\t\t\t\t\t\t\t\t\t\tmin_theta,\tmin_value\t=\ttheta,\tvalue\n\t\t\t\t\t\t\t\t\t\t\t\titerations_with_no_improvement\t=\t0\n\t\t\t\t\t\t\t\t\t\t\t\talpha\t=\talpha_0\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t\t\t#\totherwise\twe're\tnot\timproving,\tso\ttry\tshrinking\tthe\tstep\tsize\n\t\t\t\t\t\t\t\t\t\t\t\titerations_with_no_improvement\t+=\t1\n\t\t\t\t\t\t\t\t\t\t\t\talpha\t*=\t0.9\n\n\t\t\t\t\t\t\t\t#\tand\ttake\ta\tgradient\tstep\tfor\teach\tof\tthe\tdata\tpoints\n\t\t\t\t\t\t\t\tfor\tx_i,\ty_i\tin\tin_random_order(data):\n\t\t\t\t\t\t\t\t\t\t\t\tgradient_i\t=\tgradient_fn(x_i,\ty_i,\ttheta)\n\t\t\t\t\t\t\t\t\t\t\t\ttheta\t=\tvector_subtract(theta,\tscalar_multiply(alpha,\tgradient_i))\n\n\t\t\t\treturn\tmin_theta\n\nThe\tstochastic\tversion\twill\ttypically\tbe\ta\tlot\tfaster\tthan\tthe\tbatch\tversion.\tOf\tcourse,\twe\u2019ll\nwant\ta\tversion\tthat\tmaximizes\tas\twell:",
    "152": "def\tmaximize_stochastic(target_fn,\tgradient_fn,\tx,\ty,\ttheta_0,\talpha_0=0.01):\n\t\t\t\treturn\tminimize_stochastic(negate(target_fn),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnegate_all(gradient_fn),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tx,\ty,\ttheta_0,\talpha_0)",
    "153": "For\tFurther\tExploration\n\nKeep\treading!\tWe\u2019ll\tbe\tusing\tgradient\tdescent\tto\tsolve\tproblems\tthroughout\tthe\trest\tof\nthe\tbook.\n\nAt\tthis\tpoint,\tyou\u2019re\tundoubtedly\tsick\tof\tme\trecommending\tthat\tyou\tread\ttextbooks.\tIf\nit\u2019s\tany\tconsolation,\tActive\tCalculus\tseems\tnicer\tthan\tthe\tcalculus\ttextbooks\tI\tlearned\nfrom.\n\nscikit-learn\thas\ta\tStochastic\tGradient\tDescent\tmodule\tthat\tis\tnot\tas\tgeneral\tas\tours\tin\nsome\tways\tand\tmore\tgeneral\tin\tother\tways.\tReally,\tthough,\tin\tmost\treal-world\nsituations\tyou\u2019ll\tbe\tusing\tlibraries\tin\twhich\tthe\toptimization\tis\talready\ttaken\tcare\tof\nbehind\tthe\tscenes,\tand\tyou\twon\u2019t\thave\tto\tworry\tabout\tit\tyourself\t(other\tthan\twhen\tit\ndoesn\u2019t\twork\tcorrectly,\twhich\tone\tday,\tinevitably,\tit\twon\u2019t).",
    "154": "",
    "155": "Chapter\t9.\tGetting\tData\n\nTo\twrite\tit,\tit\ttook\tthree\tmonths;\tto\tconceive\tit,\tthree\tminutes;\tto\tcollect\tthe\tdata\tin\tit,\nall\tmy\tlife.\n\nF.\tScott\tFitzgerald\n\nIn\torder\tto\tbe\ta\tdata\tscientist\tyou\tneed\tdata.\tIn\tfact,\tas\ta\tdata\tscientist\tyou\twill\tspend\tan\nembarrassingly\tlarge\tfraction\tof\tyour\ttime\tacquiring,\tcleaning,\tand\ttransforming\tdata.\tIn\ta\npinch,\tyou\tcan\talways\ttype\tthe\tdata\tin\tyourself\t(or\tif\tyou\thave\tminions,\tmake\tthem\tdo\tit),\nbut\tusually\tthis\tis\tnot\ta\tgood\tuse\tof\tyour\ttime.\tIn\tthis\tchapter,\twe\u2019ll\tlook\tat\tdifferent\tways\nof\tgetting\tdata\tinto\tPython\tand\tinto\tthe\tright\tformats.",
    "156": "stdin\tand\tstdout\n\nIf\tyou\trun\tyour\tPython\tscripts\tat\tthe\tcommand\tline,\tyou\tcan\tpipe\tdata\tthrough\tthem\tusing\nsys.stdin\tand\tsys.stdout.\tFor\texample,\there\tis\ta\tscript\tthat\treads\tin\tlines\tof\ttext\tand\nspits\tback\tout\tthe\tones\tthat\tmatch\ta\tregular\texpression:\n\n#\tegrep.py\nimport\tsys,\tre\n\n#\tsys.argv\tis\tthe\tlist\tof\tcommand-line\targuments\n#\tsys.argv[0]\tis\tthe\tname\tof\tthe\tprogram\titself\n#\tsys.argv[1]\twill\tbe\tthe\tregex\tspecified\tat\tthe\tcommand\tline\nregex\t=\tsys.argv[1]\n\n#\tfor\tevery\tline\tpassed\tinto\tthe\tscript\nfor\tline\tin\tsys.stdin:\n\t\t\t\t#\tif\tit\tmatches\tthe\tregex,\twrite\tit\tto\tstdout\n\t\t\t\tif\tre.search(regex,\tline):\n\t\t\t\t\t\t\t\tsys.stdout.write(line)\n\nAnd\there\u2019s\tone\tthat\tcounts\tthe\tlines\tit\treceives\tand\tthen\twrites\tout\tthe\tcount:\n\n#\tline_count.py\nimport\tsys\n\ncount\t=\t0\nfor\tline\tin\tsys.stdin:\n\t\t\t\tcount\t+=\t1\n\n#\tprint\tgoes\tto\tsys.stdout\nprint\tcount\n\nYou\tcould\tthen\tuse\tthese\tto\tcount\thow\tmany\tlines\tof\ta\tfile\tcontain\tnumbers.\tIn\tWindows,\nyou\u2019d\tuse:\n\ntype\tSomeFile.txt\t|\tpython\tegrep.py\t\"[0-9]\"\t|\tpython\tline_count.py\n\nwhereas\tin\ta\tUnix\tsystem\tyou\u2019d\tuse:\n\ncat\tSomeFile.txt\t|\tpython\tegrep.py\t\"[0-9]\"\t|\tpython\tline_count.py\n\nThe\t|\tis\tthe\tpipe\tcharacter,\twhich\tmeans\t\u201cuse\tthe\toutput\tof\tthe\tleft\tcommand\tas\tthe\tinput\nof\tthe\tright\tcommand.\u201d\tYou\tcan\tbuild\tpretty\telaborate\tdata-processing\tpipelines\tthis\tway.\n\nIf\tyou\tare\tusing\tWindows,\tyou\tcan\tprobably\tleave\tout\tthe\tpython\tpart\tof\tthis\tcommand:\n\nNOTE\n\ntype\tSomeFile.txt\t|\tegrep.py\t\"[0-9]\"\t|\tline_count.py\n\nIf\tyou\tare\ton\ta\tUnix\tsystem,\tdoing\tso\tmight\trequire\ta\tlittle\tmore\twork.\n\nSimilarly,\there\u2019s\ta\tscript\tthat\tcounts\tthe\twords\tin\tits\tinput\tand\twrites\tout\tthe\tmost\tcommon\nones:\n\n#\tmost_common_words.py",
    "157": "import\tsys\nfrom\tcollections\timport\tCounter\n\n#\tpass\tin\tnumber\tof\twords\tas\tfirst\targument\ntry:\n\t\t\t\tnum_words\t=\tint(sys.argv[1])\nexcept:\n\t\t\t\tprint\t\"usage:\tmost_common_words.py\tnum_words\"\n\t\t\t\tsys.exit(1)\t\t\t#\tnon-zero\texit\tcode\tindicates\terror\n\ncounter\t=\tCounter(word.lower()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tlowercase\twords\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tline\tin\tsys.stdin\t\t\t\t\t\t\t\t\t\t\t\t\t#\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tword\tin\tline.strip().split()\t\t#\tsplit\ton\tspaces\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tword)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tskip\tempty\t'words'\n\nfor\tword,\tcount\tin\tcounter.most_common(num_words):\n\t\t\t\tsys.stdout.write(str(count))\n\t\t\t\tsys.stdout.write(\"\\t\")\n\t\t\t\tsys.stdout.write(word)\n\t\t\t\tsys.stdout.write(\"\\n\")\n\nafter\twhich\tyou\tcould\tdo\tsomething\tlike:\n\nC:\\DataScience>type\tthe_bible.txt\t|\tpython\tmost_common_words.py\t10\n64193\t\t\tthe\n51380\t\t\tand\n34753\t\t\tof\n13643\t\t\tto\n12799\t\t\tthat\n12560\t\t\tin\n10263\t\t\the\n9840\t\t\t\tshall\n8987\t\t\t\tunto\n8836\t\t\t\tfor\n\nIf\tyou\tare\ta\tseasoned\tUnix\tprogrammer,\tyou\tare\tprobably\tfamiliar\twith\ta\twide\tvariety\tof\tcommand-line\ntools\t(for\texample,\tegrep)\tthat\tare\tbuilt\tinto\tyour\toperating\tsystem\tand\tthat\tare\tprobably\tpreferable\tto\nbuilding\tyour\town\tfrom\tscratch.\tStill,\tit\u2019s\tgood\tto\tknow\tyou\tcan\tif\tyou\tneed\tto.\n\nNOTE",
    "158": "Reading\tFiles\n\nYou\tcan\talso\texplicitly\tread\tfrom\tand\twrite\tto\tfiles\tdirectly\tin\tyour\tcode.\tPython\tmakes\nworking\twith\tfiles\tpretty\tsimple.",
    "159": "The\tBasics\tof\tText\tFiles\n\nThe\tfirst\tstep\tto\tworking\twith\ta\ttext\tfile\tis\tto\tobtain\ta\tfile\tobject\tusing\topen:\n\n#\t'r'\tmeans\tread-only\nfile_for_reading\t=\topen('reading_file.txt',\t'r')\n\n#\t'w'\tis\twrite\u2014will\tdestroy\tthe\tfile\tif\tit\talready\texists!\nfile_for_writing\t=\topen('writing_file.txt',\t'w')\n\n#\t'a'\tis\tappend\u2014for\tadding\tto\tthe\tend\tof\tthe\tfile\nfile_for_appending\t=\topen('appending_file.txt',\t'a')\n\n#\tdon't\tforget\tto\tclose\tyour\tfiles\twhen\tyou're\tdone\nfile_for_writing.close()\n\nBecause\tit\tis\teasy\tto\tforget\tto\tclose\tyour\tfiles,\tyou\tshould\talways\tuse\tthem\tin\ta\twith\nblock,\tat\tthe\tend\tof\twhich\tthey\twill\tbe\tclosed\tautomatically:\n\nwith\topen(filename,'r')\tas\tf:\n\t\t\t\tdata\t=\tfunction_that_gets_data_from(f)\n\n#\tat\tthis\tpoint\tf\thas\talready\tbeen\tclosed,\tso\tdon't\ttry\tto\tuse\tit\nprocess(data)\n\nIf\tyou\tneed\tto\tread\ta\twhole\ttext\tfile,\tyou\tcan\tjust\titerate\tover\tthe\tlines\tof\tthe\tfile\tusing\nfor:\n\nstarts_with_hash\t=\t0\n\nwith\topen('input.txt','r')\tas\tf:\n\t\t\t\tfor\tline\tin\tfile:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tlook\tat\teach\tline\tin\tthe\tfile\n\t\t\t\t\t\t\t\tif\tre.match(\"^#\",line):\t\t\t\t\t#\tuse\ta\tregex\tto\tsee\tif\tit\tstarts\twith\t'#'\n\t\t\t\t\t\t\t\t\t\t\t\tstarts_with_hash\t+=\t1\t\t\t#\tif\tit\tdoes,\tadd\t1\tto\tthe\tcount\n\nEvery\tline\tyou\tget\tthis\tway\tends\tin\ta\tnewline\tcharacter,\tso\tyou\u2019ll\toften\twant\tto\tstrip()\tit\nbefore\tdoing\tanything\twith\tit.\n\nFor\texample,\timagine\tyou\thave\ta\tfile\tfull\tof\temail\taddresses,\tone\tper\tline,\tand\tthat\tyou\nneed\tto\tgenerate\ta\thistogram\tof\tthe\tdomains.\tThe\trules\tfor\tcorrectly\textracting\tdomains\nare\tsomewhat\tsubtle\t(e.g.,\tthe\tPublic\tSuffix\tList),\tbut\ta\tgood\tfirst\tapproximation\tis\tto\tjust\ntake\tthe\tparts\tof\tthe\temail\taddresses\tthat\tcome\tafter\tthe\t@.\t(Which\tgives\tthe\twrong\tanswer\nfor\temail\taddresses\tlike\tjoel@mail.datasciencester.com.)\n\ndef\tget_domain(email_address):\n\t\t\t\t\"\"\"split\ton\t'@'\tand\treturn\tthe\tlast\tpiece\"\"\"\n\t\t\t\treturn\temail_address.lower().split(\"@\")[-1]\n\nwith\topen('email_addresses.txt',\t'r')\tas\tf:\n\t\t\t\tdomain_counts\t=\tCounter(get_domain(line.strip())\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tline\tin\tf\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\t\"@\"\tin\tline)",
    "160": "Delimited\tFiles\n\nThe\thypothetical\temail\taddresses\tfile\twe\tjust\tprocessed\thad\tone\taddress\tper\tline.\tMore\nfrequently\tyou\u2019ll\twork\twith\tfiles\twith\tlots\tof\tdata\ton\teach\tline.\tThese\tfiles\tare\tvery\toften\neither\tcomma-separated\tor\ttab-separated.\tEach\tline\thas\tseveral\tfields,\twith\ta\tcomma\t(or\ta\ntab)\tindicating\twhere\tone\tfield\tends\tand\tthe\tnext\tfield\tstarts.\n\nThis\tstarts\tto\tget\tcomplicated\twhen\tyou\thave\tfields\twith\tcommas\tand\ttabs\tand\tnewlines\tin\nthem\t(which\tyou\tinevitably\tdo).\tFor\tthis\treason,\tit\u2019s\tpretty\tmuch\talways\ta\tmistake\tto\ttry\tto\nparse\tthem\tyourself.\tInstead,\tyou\tshould\tuse\tPython\u2019s\tcsv\tmodule\t(or\tthe\tpandas\tlibrary).\nFor\ttechnical\treasons\tthat\tyou\tshould\tfeel\tfree\tto\tblame\ton\tMicrosoft,\tyou\tshould\talways\nwork\twith\tcsv\tfiles\tin\tbinary\tmode\tby\tincluding\ta\tb\tafter\tthe\tr\tor\tw\t(see\tStack\tOverflow).\n\nIf\tyour\tfile\thas\tno\theaders\t(which\tmeans\tyou\tprobably\twant\teach\trow\tas\ta\tlist,\tand\nwhich\tplaces\tthe\tburden\ton\tyou\tto\tknow\twhat\u2019s\tin\teach\tcolumn),\tyou\tcan\tuse\tcsv.reader\nto\titerate\tover\tthe\trows,\teach\tof\twhich\twill\tbe\tan\tappropriately\tsplit\tlist.\n\nFor\texample,\tif\twe\thad\ta\ttab-delimited\tfile\tof\tstock\tprices:\n\n6/20/2014\t\t\tAAPL\t\t\t\t90.91\n6/20/2014\t\t\tMSFT\t\t\t\t41.68\n6/20/2014\t\t\tFB\t\t64.5\n6/19/2014\t\t\tAAPL\t\t\t\t91.86\n6/19/2014\t\t\tMSFT\t\t\t\t41.51\n6/19/2014\t\t\tFB\t\t64.34\n\nwe\tcould\tprocess\tthem\twith:\n\nimport\tcsv\n\nwith\topen('tab_delimited_stock_prices.txt',\t'rb')\tas\tf:\n\t\t\t\treader\t=\tcsv.reader(f,\tdelimiter='\\t')\n\t\t\t\tfor\trow\tin\treader:\n\t\t\t\t\t\t\t\tdate\t=\trow[0]\n\t\t\t\t\t\t\t\tsymbol\t=\trow[1]\n\t\t\t\t\t\t\t\tclosing_price\t=\tfloat(row[2])\n\t\t\t\t\t\t\t\tprocess(date,\tsymbol,\tclosing_price)\n\nIf\tyour\tfile\thas\theaders:\n\ndate:symbol:closing_price\n6/20/2014:AAPL:90.91\n6/20/2014:MSFT:41.68\n6/20/2014:FB:64.5\n\nyou\tcan\teither\tskip\tthe\theader\trow\t(with\tan\tinitial\tcall\tto\treader.next())\tor\tget\teach\trow\nas\ta\tdict\t(with\tthe\theaders\tas\tkeys)\tby\tusing\tcsv.DictReader:\n\nwith\topen('colon_delimited_stock_prices.txt',\t'rb')\tas\tf:\n\t\t\t\treader\t=\tcsv.DictReader(f,\tdelimiter=':')\n\t\t\t\tfor\trow\tin\treader:\n\t\t\t\t\t\t\t\tdate\t=\trow[\"date\"]\n\t\t\t\t\t\t\t\tsymbol\t=\trow[\"symbol\"]\n\t\t\t\t\t\t\t\tclosing_price\t=\tfloat(row[\"closing_price\"])\n\t\t\t\t\t\t\t\tprocess(date,\tsymbol,\tclosing_price)",
    "161": "Even\tif\tyour\tfile\tdoesn\u2019t\thave\theaders\tyou\tcan\tstill\tuse\tDictReader\tby\tpassing\tit\tthe\tkeys\nas\ta\tfieldnames\tparameter.\n\nYou\tcan\tsimilarly\twrite\tout\tdelimited\tdata\tusing\tcsv.writer:\n\ntoday_prices\t=\t{\t'AAPL'\t:\t90.91,\t'MSFT'\t:\t41.68,\t'FB'\t:\t64.5\t}\n\nwith\topen('comma_delimited_stock_prices.txt','wb')\tas\tf:\n\t\t\t\twriter\t=\tcsv.writer(f,\tdelimiter=',')\n\t\t\t\tfor\tstock,\tprice\tin\ttoday_prices.items():\n\t\t\t\t\t\t\t\twriter.writerow([stock,\tprice])\n\ncsv.writer\twill\tdo\tthe\tright\tthing\tif\tyour\tfields\tthemselves\thave\tcommas\tin\tthem.\tYour\nown\thand-rolled\twriter\tprobably\twon\u2019t.\tFor\texample,\tif\tyou\tattempt:\n\nresults\t=\t[[\"test1\",\t\"success\",\t\"Monday\"],\n\t\t\t\t\t\t\t\t\t\t\t[\"test2\",\t\"success,\tkind\tof\",\t\"Tuesday\"],\n\t\t\t\t\t\t\t\t\t\t\t[\"test3\",\t\"failure,\tkind\tof\",\t\"Wednesday\"],\n\t\t\t\t\t\t\t\t\t\t\t[\"test4\",\t\"failure,\tutter\",\t\"Thursday\"]]\n\n#\tdon't\tdo\tthis!\nwith\topen('bad_csv.txt',\t'wb')\tas\tf:\n\t\t\t\tfor\trow\tin\tresults:\n\t\t\t\t\t\t\t\tf.write(\",\".join(map(str,\trow)))\t#\tmight\thave\ttoo\tmany\tcommas\tin\tit!\n\t\t\t\t\t\t\t\tf.write(\"\\n\")\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\trow\tmight\thave\tnewlines\tas\twell!\n\nYou\twill\tend\tup\twith\ta\tcsv\tfile\tthat\tlooks\tlike:\n\ntest1,success,Monday\ntest2,success,\tkind\tof,Tuesday\ntest3,failure,\tkind\tof,Wednesday\ntest4,failure,\tutter,Thursday\n\nand\tthat\tno\tone\twill\tever\tbe\table\tto\tmake\tsense\tof.",
    "162": "Scraping\tthe\tWeb\n\nAnother\tway\tto\tget\tdata\tis\tby\tscraping\tit\tfrom\tweb\tpages.\tFetching\tweb\tpages,\tit\tturns\nout,\tis\tpretty\teasy;\tgetting\tmeaningful\tstructured\tinformation\tout\tof\tthem\tless\tso.",
    "163": "HTML\tand\tthe\tParsing\tThereof\n\nPages\ton\tthe\tWeb\tare\twritten\tin\tHTML,\tin\twhich\ttext\tis\t(ideally)\tmarked\tup\tinto\telements\nand\ttheir\tattributes:\n\n<html>\n\t\t<head>\n\t\t\t\t<title>A\tweb\tpage</title>\n\t\t</head>\n\t\t<body>\n\t\t\t\t<p\tid=\"author\">Joel\tGrus</p>\n\t\t\t\t<p\tid=\"subject\">Data\tScience</p>\n\t\t</body>\n</html>\n\nIn\ta\tperfect\tworld,\twhere\tall\tweb\tpages\tare\tmarked\tup\tsemantically\tfor\tour\tbenefit,\twe\nwould\tbe\table\tto\textract\tdata\tusing\trules\tlike\t\u201cfind\tthe\t<p>\telement\twhose\tid\tis\tsubject\nand\treturn\tthe\ttext\tit\tcontains.\u201d\tIn\tthe\tactual\tworld,\tHTML\tis\tnot\tgenerally\twell-formed,\nlet\talone\tannotated.\tThis\tmeans\twe\u2019ll\tneed\thelp\tmaking\tsense\tof\tit.\n\nTo\tget\tdata\tout\tof\tHTML,\twe\twill\tuse\tthe\tBeautifulSoup\tlibrary,\twhich\tbuilds\ta\ttree\tout\tof\nthe\tvarious\telements\ton\ta\tweb\tpage\tand\tprovides\ta\tsimple\tinterface\tfor\taccessing\tthem.\tAs\nI\twrite\tthis,\tthe\tlatest\tversion\tis\tBeautiful\tSoup\t4.3.2\t(pip\tinstall\tbeautifulsoup4),\nwhich\tis\twhat\twe\u2019ll\tbe\tusing.\tWe\u2019ll\talso\tbe\tusing\tthe\trequests\tlibrary\t(pip\tinstall\nrequests),\twhich\tis\ta\tmuch\tnicer\tway\tof\tmaking\tHTTP\trequests\tthan\tanything\tthat\u2019s\tbuilt\ninto\tPython.\n\nPython\u2019s\tbuilt-in\tHTML\tparser\tis\tnot\tthat\tlenient,\twhich\tmeans\tthat\tit\tdoesn\u2019t\talways\tcope\nwell\twith\tHTML\tthat\u2019s\tnot\tperfectly\tformed.\tFor\tthat\treason,\twe\u2019ll\tuse\ta\tdifferent\tparser,\nwhich\twe\tneed\tto\tinstall:\n\npip\tinstall\thtml5lib\n\nTo\tuse\tBeautiful\tSoup,\twe\u2019ll\tneed\tto\tpass\tsome\tHTML\tinto\tthe\tBeautifulSoup()\nfunction.\tIn\tour\texamples,\tthis\twill\tbe\tthe\tresult\tof\ta\tcall\tto\trequests.get:\n\nfrom\tbs4\timport\tBeautifulSoup\nimport\trequests\nhtml\t=\trequests.get(\"http://www.example.com\").text\nsoup\t=\tBeautifulSoup(html,\t'html5lib')\n\nafter\twhich\twe\tcan\tget\tpretty\tfar\tusing\ta\tfew\tsimple\tmethods.\n\nWe\u2019ll\ttypically\twork\twith\tTag\tobjects,\twhich\tcorrespond\tto\tthe\ttags\trepresenting\tthe\nstructure\tof\tan\tHTML\tpage.\n\nFor\texample,\tto\tfind\tthe\tfirst\t<p>\ttag\t(and\tits\tcontents)\tyou\tcan\tuse:\n\nfirst_paragraph\t=\tsoup.find('p')\t\t\t\t\t\t\t\t#\tor\tjust\tsoup.p\n\nYou\tcan\tget\tthe\ttext\tcontents\tof\ta\tTag\tusing\tits\ttext\tproperty:",
    "164": "first_paragraph_text\t=\tsoup.p.text\nfirst_paragraph_words\t=\tsoup.p.text.split()\n\nAnd\tyou\tcan\textract\ta\ttag\u2019s\tattributes\tby\ttreating\tit\tlike\ta\tdict:\n\nfirst_paragraph_id\t=\tsoup.p['id']\t\t\t\t\t\t\t#\traises\tKeyError\tif\tno\t'id'\nfirst_paragraph_id2\t=\tsoup.p.get('id')\t\t#\treturns\tNone\tif\tno\t'id'\n\nYou\tcan\tget\tmultiple\ttags\tat\tonce:\n\nall_paragraphs\t=\tsoup.find_all('p')\t\t#\tor\tjust\tsoup('p')\nparagraphs_with_ids\t=\t[p\tfor\tp\tin\tsoup('p')\tif\tp.get('id')]\n\nFrequently\tyou\u2019ll\twant\tto\tfind\ttags\twith\ta\tspecific\tclass:\n\nimportant_paragraphs\t=\tsoup('p',\t{'class'\t:\t'important'})\nimportant_paragraphs2\t=\tsoup('p',\t'important')\nimportant_paragraphs3\t=\t[p\tfor\tp\tin\tsoup('p')\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\t'important'\tin\tp.get('class',\t[])]\n\nAnd\tyou\tcan\tcombine\tthese\tto\timplement\tmore\telaborate\tlogic.\tFor\texample,\tif\tyou\twant\nto\tfind\tevery\t<span>\telement\tthat\tis\tcontained\tinside\ta\t<div>\telement,\tyou\tcould\tdo\tthis:\n\n#\twarning,\twill\treturn\tthe\tsame\tspan\tmultiple\ttimes\n#\tif\tit\tsits\tinside\tmultiple\tdivs\n#\tbe\tmore\tclever\tif\tthat's\tthe\tcase\nspans_inside_divs\t=\t[span\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tdiv\tin\tsoup('div')\t\t\t\t\t#\tfor\teach\t<div>\ton\tthe\tpage\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tspan\tin\tdiv('span')]\t\t\t#\tfind\teach\t<span>\tinside\tit\n\nJust\tthis\thandful\tof\tfeatures\twill\tallow\tus\tto\tdo\tquite\ta\tlot.\tIf\tyou\tend\tup\tneeding\tto\tdo\nmore-complicated\tthings\t(or\tif\tyou\u2019re\tjust\tcurious),\tcheck\tthe\tdocumentation.\n\nOf\tcourse,\twhatever\tdata\tis\timportant\twon\u2019t\ttypically\tbe\tlabeled\tas\tclass=\"important\".\nYou\u2019ll\tneed\tto\tcarefully\tinspect\tthe\tsource\tHTML,\treason\tthrough\tyour\tselection\tlogic,\nand\tworry\tabout\tedge\tcases\tto\tmake\tsure\tyour\tdata\tis\tcorrect.\tLet\u2019s\tlook\tat\tan\texample.",
    "165": "Example:\tO\u2019Reilly\tBooks\tAbout\tData\n\nA\tpotential\tinvestor\tin\tDataSciencester\tthinks\tdata\tis\tjust\ta\tfad.\tTo\tprove\thim\twrong,\tyou\ndecide\tto\texamine\thow\tmany\tdata\tbooks\tO\u2019Reilly\thas\tpublished\tover\ttime.\tAfter\tdigging\nthrough\tits\twebsite,\tyou\tfind\tthat\tit\thas\tmany\tpages\tof\tdata\tbooks\t(and\tvideos),\treachable\nthrough\t30-items-at-a-time\tdirectory\tpages\twith\tURLs\tlike:\n\nhttp://shop.oreilly.com/category/browse-subjects/data.do?\nsortby=publicationDate&page=1\n\nUnless\tyou\twant\tto\tbe\ta\tjerk\t(and\tunless\tyou\twant\tyour\tscraper\tto\tget\tbanned),\twhenever\nyou\twant\tto\tscrape\tdata\tfrom\ta\twebsite\tyou\tshould\tfirst\tcheck\tto\tsee\tif\tit\thas\tsome\tsort\tof\naccess\tpolicy.\tLooking\tat:\n\nhttp://oreilly.com/terms/\n\nthere\tseems\tto\tbe\tnothing\tprohibiting\tthis\tproject.\tIn\torder\tto\tbe\tgood\tcitizens,\twe\tshould\nalso\tcheck\tfor\ta\trobots.txt\tfile\tthat\ttells\twebcrawlers\thow\tto\tbehave.\tThe\timportant\tlines\tin\nhttp://shop.oreilly.com/robots.txt\tare:\n\nCrawl-delay:\t30\nRequest-rate:\t1/30\n\nThe\tfirst\ttells\tus\tthat\twe\tshould\twait\t30\tseconds\tbetween\trequests,\tthe\tsecond\tthat\twe\nshould\trequest\tonly\tone\tpage\tevery\t30\tseconds.\tSo\tbasically\tthey\u2019re\ttwo\tdifferent\tways\tof\nsaying\tthe\tsame\tthing.\t(There\tare\tother\tlines\tthat\tindicate\tdirectories\tnot\tto\tscrape,\tbut\nthey\tdon\u2019t\tinclude\tour\tURL,\tso\twe\u2019re\tOK\tthere.)\n\nNOTE\n\nThere\u2019s\talways\tthe\tpossibility\tthat\tO\u2019Reilly\twill\tat\tsome\tpoint\trevamp\tits\twebsite\tand\tbreak\tall\tthe\tlogic\tin\nthis\tsection.\tI\twill\tdo\twhat\tI\tcan\tto\tprevent\tthat,\tof\tcourse,\tbut\tI\tdon\u2019t\thave\ta\tton\tof\tinfluence\tover\tthere.\nAlthough,\tif\tevery\tone\tof\tyou\twere\tto\tconvince\teveryone\tyou\tknow\tto\tbuy\ta\tcopy\tof\tthis\tbook\u2026\n\nTo\tfigure\tout\thow\tto\textract\tthe\tdata,\tlet\u2019s\tdownload\tone\tof\tthose\tpages\tand\tfeed\tit\tto\nBeautiful\tSoup:\n\n#\tyou\tdon't\thave\tto\tsplit\tthe\turl\tlike\tthis\tunless\tit\tneeds\tto\tfit\tin\ta\tbook\nurl\t=\t\"http://shop.oreilly.com/category/browse-subjects/\"\t+\t\\\n\t\t\t\t\t\t\"data.do?sortby=publicationDate&page=1\"\nsoup\t=\tBeautifulSoup(requests.get(url).text,\t'html5lib')\n\nIf\tyou\tview\tthe\tsource\tof\tthe\tpage\t(in\tyour\tbrowser,\tright-click\tand\tselect\t\u201cView\tsource\u201d\nor\t\u201cView\tpage\tsource\u201d\tor\twhatever\toption\tlooks\tthe\tmost\tlike\tthat),\tyou\u2019ll\tsee\tthat\teach\nbook\t(or\tvideo)\tseems\tto\tbe\tuniquely\tcontained\tin\ta\t<td>\ttable\tcell\telement\twhose\tclass\nis\tthumbtext.\tHere\tis\t(an\tabridged\tversion\tof)\tthe\trelevant\tHTML\tfor\tone\tbook:\n\n<td\tclass=\"thumbtext\">\n\t\t<div\tclass=\"thumbcontainer\">\n\t\t\t\t<div\tclass=\"thumbdiv\">\n\t\t\t\t\t\t<a\thref=\"/product/9781118903407.do\">",
    "166": "<img\tsrc=\"...\"/>\n\t\t\t\t\t\t</a>\n\t\t\t\t</div>\n\t\t</div>\n\t\t<div\tclass=\"widthchange\">\n\t\t\t\t<div\tclass=\"thumbheader\">\n\t\t\t\t\t\t<a\thref=\"/product/9781118903407.do\">Getting\ta\tBig\tData\tJob\tFor\tDummies</a>\n\t\t\t\t</div>\n\t\t\t\t<div\tclass=\"AuthorName\">By\tJason\tWilliamson</div>\n\t\t\t\t<span\tclass=\"directorydate\">\t\t\t\t\t\t\t\tDecember\t2014\t\t\t\t</span>\n\t\t\t\t<div\tstyle=\"clear:both;\">\n\t\t\t\t\t\t<div\tid=\"146350\">\n\t\t\t\t\t\t\t\t<span\tclass=\"pricelabel\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tEbook:\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span\tclass=\"price\">&nbsp;$29.99</span>\n\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t</div>\n</td>\n\nA\tgood\tfirst\tstep\tis\tto\tfind\tall\tof\tthe\ttd\tthumbtext\ttag\telements:\n\ntds\t=\tsoup('td',\t'thumbtext')\nprint\tlen(tds)\n#\t30\n\nNext\twe\u2019d\tlike\tto\tfilter\tout\tthe\tvideos.\t(The\twould-be\tinvestor\tis\tonly\timpressed\tby\nbooks.)\tIf\twe\tinspect\tthe\tHTML\tfurther,\twe\tsee\tthat\teach\ttd\tcontains\tone\tor\tmore\tspan\nelements\twhose\tclass\tis\tpricelabel,\tand\twhose\ttext\tlooks\tlike\tEbook:\tor\tVideo:\tor\nPrint:.\tIt\tappears\tthat\tthe\tvideos\tcontain\tonly\tone\tpricelabel,\twhose\ttext\tstarts\twith\nVideo\t(after\tremoving\tleading\tspaces).\tThis\tmeans\twe\tcan\ttest\tfor\tvideos\twith:\n\ndef\tis_video(td):\n\t\t\t\t\"\"\"it's\ta\tvideo\tif\tit\thas\texactly\tone\tpricelabel,\tand\tif\n\t\t\t\tthe\tstripped\ttext\tinside\tthat\tpricelabel\tstarts\twith\t'Video'\"\"\"\n\t\t\t\tpricelabels\t=\ttd('span',\t'pricelabel')\n\t\t\t\treturn\t(len(pricelabels)\t==\t1\tand\n\t\t\t\t\t\t\t\t\t\t\t\tpricelabels[0].text.strip().startswith(\"Video\"))\n\nprint\tlen([td\tfor\ttd\tin\ttds\tif\tnot\tis_video(td)])\n#\t21\tfor\tme,\tmight\tbe\tdifferent\tfor\tyou\n\nNow\twe\u2019re\tready\tto\tstart\tpulling\tdata\tout\tof\tthe\ttd\telements.\tIt\tlooks\tlike\tthe\tbook\ttitle\tis\nthe\ttext\tinside\tthe\t<a>\ttag\tinside\tthe\t<div\tclass=\"thumbheader\">:\n\ntitle\t=\ttd.find(\"div\",\t\"thumbheader\").a.text\n\nThe\tauthor(s)\tare\tin\tthe\ttext\tof\tthe\tAuthorName\t<div>.\tThey\tare\tprefaced\tby\ta\tBy\t(which\nwe\twant\tto\tget\trid\tof)\tand\tseparated\tby\tcommas\t(which\twe\twant\tto\tsplit\tout,\tafter\twhich\nwe\u2019ll\tneed\tto\tget\trid\tof\tspaces):\n\nauthor_name\t=\ttd.find('div',\t'AuthorName').text\nauthors\t=\t[x.strip()\tfor\tx\tin\tre.sub(\"^By\t\",\t\"\",\tauthor_name).split(\",\")]\n\nThe\tISBN\tseems\tto\tbe\tcontained\tin\tthe\tlink\tthat\u2019s\tin\tthe\tthumbheader\t<div>:",
    "167": "isbn_link\t=\ttd.find(\"div\",\t\"thumbheader\").a.get(\"href\")\n\n#\tre.match\tcaptures\tthe\tpart\tof\tthe\tregex\tin\tparentheses\nisbn\t=\tre.match(\"/product/(.*)\\.do\",\tisbn_link).group(1)\n\nAnd\tthe\tdate\tis\tjust\tthe\tcontents\tof\tthe\t<span\tclass=\"directorydate\">:\n\ndate\t=\ttd.find(\"span\",\t\"directorydate\").text.strip()\n\nLet\u2019s\tput\tthis\tall\ttogether\tinto\ta\tfunction:\n\ndef\tbook_info(td):\n\t\t\t\t\"\"\"given\ta\tBeautifulSoup\t<td>\tTag\trepresenting\ta\tbook,\n\t\t\t\textract\tthe\tbook's\tdetails\tand\treturn\ta\tdict\"\"\"\n\n\t\t\t\ttitle\t=\ttd.find(\"div\",\t\"thumbheader\").a.text\n\t\t\t\tby_author\t=\ttd.find('div',\t'AuthorName').text\n\t\t\t\tauthors\t=\t[x.strip()\tfor\tx\tin\tre.sub(\"^By\t\",\t\"\",\tby_author).split(\",\")]\n\t\t\t\tisbn_link\t=\ttd.find(\"div\",\t\"thumbheader\").a.get(\"href\")\n\t\t\t\tisbn\t=\tre.match(\"/product/(.*)\\.do\",\tisbn_link).groups()[0]\n\t\t\t\tdate\t=\ttd.find(\"span\",\t\"directorydate\").text.strip()\n\n\t\t\t\treturn\t{\n\t\t\t\t\t\t\t\t\"title\"\t:\ttitle,\n\t\t\t\t\t\t\t\t\"authors\"\t:\tauthors,\n\t\t\t\t\t\t\t\t\"isbn\"\t:\tisbn,\n\t\t\t\t\t\t\t\t\"date\"\t:\tdate\n\t\t\t\t}\n\nAnd\tnow\twe\u2019re\tready\tto\tscrape:\n\nfrom\tbs4\timport\tBeautifulSoup\nimport\trequests\nfrom\ttime\timport\tsleep\nbase_url\t=\t\"http://shop.oreilly.com/category/browse-subjects/\"\t+\t\\\n\t\t\t\t\t\t\t\t\t\t\t\"data.do?sortby=publicationDate&page=\"\n\nbooks\t=\t[]\n\nNUM_PAGES\t=\t31\t\t\t\t\t#\tat\tthe\ttime\tof\twriting,\tprobably\tmore\tby\tnow\n\nfor\tpage_num\tin\trange(1,\tNUM_PAGES\t+\t1):\n\t\t\t\tprint\t\"souping\tpage\",\tpage_num,\t\",\",\tlen(books),\t\"\tfound\tso\tfar\"\n\t\t\t\turl\t=\tbase_url\t+\tstr(page_num)\n\t\t\t\tsoup\t=\tBeautifulSoup(requests.get(url).text,\t'html5lib')\n\n\t\t\t\tfor\ttd\tin\tsoup('td',\t'thumbtext'):\n\t\t\t\t\t\t\t\tif\tnot\tis_video(td):\n\t\t\t\t\t\t\t\t\t\t\t\tbooks.append(book_info(td))\n\n\t\t\t\t#\tnow\tbe\ta\tgood\tcitizen\tand\trespect\tthe\trobots.txt!\n\t\t\t\tsleep(30)\n\nNOTE\n\nExtracting\tdata\tfrom\tHTML\tlike\tthis\tis\tmore\tdata\tart\tthan\tdata\tscience.\tThere\tare\tcountless\tother\tfind-the-\nbooks\tand\tfind-the-title\tlogics\tthat\twould\thave\tworked\tjust\tas\twell.\n\nNow\tthat\twe\u2019ve\tcollected\tthe\tdata,\twe\tcan\tplot\tthe\tnumber\tof\tbooks\tpublished\teach\tyear\n(Figure\t9-1):\n\ndef\tget_year(book):\n\t\t\t\t\"\"\"book[\"date\"]\tlooks\tlike\t'November\t2014'\tso\twe\tneed\tto\n\t\t\t\tsplit\ton\tthe\tspace\tand\tthen\ttake\tthe\tsecond\tpiece\"\"\"",
    "168": "return\tint(book[\"date\"].split()[1])\n\n#\t2014\tis\tthe\tlast\tcomplete\tyear\tof\tdata\t(when\tI\tran\tthis)\nyear_counts\t=\tCounter(get_year(book)\tfor\tbook\tin\tbooks\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tget_year(book)\t<=\t2014)\n\nimport\tmatplotlib.pyplot\tas\tplt\nyears\t=\tsorted(year_counts)\nbook_counts\t=\t[year_counts[year]\tfor\tyear\tin\tyears]\nplt.plot(years,\tbook_counts)\nplt.ylabel(\"#\tof\tdata\tbooks\")\nplt.title(\"Data\tis\tBig!\")\nplt.show()\n\nFigure\t9-1.\tNumber\tof\tdata\tbooks\tper\tyear\n\nUnfortunately,\tthe\twould-be\tinvestor\tlooks\tat\tthe\tgraph\tand\tdecides\tthat\t2013\twas\t\u201cpeak\ndata.\u201d",
    "169": "Using\tAPIs\n\nMany\twebsites\tand\tweb\tservices\tprovide\tapplication\tprogramming\tinterfaces\t(APIs),\nwhich\tallow\tyou\tto\texplicitly\trequest\tdata\tin\ta\tstructured\tformat.\tThis\tsaves\tyou\tthe\ntrouble\tof\thaving\tto\tscrape\tthem!",
    "170": "JSON\t(and\tXML)\n\nBecause\tHTTP\tis\ta\tprotocol\tfor\ttransferring\ttext,\tthe\tdata\tyou\trequest\tthrough\ta\tweb\tAPI\nneeds\tto\tbe\tserialized\tinto\ta\tstring\tformat.\tOften\tthis\tserialization\tuses\tJavaScript\tObject\nNotation\t(JSON).\tJavaScript\tobjects\tlook\tquite\tsimilar\tto\tPython\tdicts,\twhich\tmakes\ttheir\nstring\trepresentations\teasy\tto\tinterpret:\n\n{\t\"title\"\t:\t\"Data\tScience\tBook\",\n\t\t\"author\"\t:\t\"Joel\tGrus\",\n\t\t\"publicationYear\"\t:\t2014,\n\t\t\"topics\"\t:\t[\t\"data\",\t\"science\",\t\"data\tscience\"]\t}\n\nWe\tcan\tparse\tJSON\tusing\tPython\u2019s\tjson\tmodule.\tIn\tparticular,\twe\twill\tuse\tits\tloads\nfunction,\twhich\tdeserializes\ta\tstring\trepresenting\ta\tJSON\tobject\tinto\ta\tPython\tobject:\n\nimport\tjson\nserialized\t=\t\"\"\"{\t\"title\"\t:\t\"Data\tScience\tBook\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"author\"\t:\t\"Joel\tGrus\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"publicationYear\"\t:\t2014,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"topics\"\t:\t[\t\"data\",\t\"science\",\t\"data\tscience\"]\t}\"\"\"\n\n#\tparse\tthe\tJSON\tto\tcreate\ta\tPython\tdict\ndeserialized\t=\tjson.loads(serialized)\nif\t\"data\tscience\"\tin\tdeserialized[\"topics\"]:\n\t\t\t\tprint\tdeserialized\n\nSometimes\tan\tAPI\tprovider\thates\tyou\tand\tonly\tprovides\tresponses\tin\tXML:\n\n<Book>\n\t\t<Title>Data\tScience\tBook</Title>\n\t\t<Author>Joel\tGrus</Author>\n\t\t<PublicationYear>2014</PublicationYear>\n\t\t<Topics>\n\t\t\t\t<Topic>data</Topic>\n\t\t\t\t<Topic>science</Topic>\n\t\t\t\t<Topic>data\tscience</Topic>\n\t\t</Topics>\n</Book>\n\nYou\tcan\tuse\tBeautifulSoup\tto\tget\tdata\tfrom\tXML\tsimilarly\tto\thow\twe\tused\tit\tto\tget\tdata\nfrom\tHTML;\tcheck\tits\tdocumentation\tfor\tdetails.",
    "171": "Using\tan\tUnauthenticated\tAPI\n\nMost\tAPIs\tthese\tdays\trequire\tyou\tto\tfirst\tauthenticate\tyourself\tin\torder\tto\tuse\tthem.\tWhile\nwe\tdon\u2019t\tbegrudge\tthem\tthis\tpolicy,\tit\tcreates\ta\tlot\tof\textra\tboilerplate\tthat\tmuddies\tup\tour\nexposition.\tAccordingly,\twe\u2019ll\tfirst\ttake\ta\tlook\tat\tGitHub\u2019s\tAPI,\twith\twhich\tyou\tcan\tdo\nsome\tsimple\tthings\tunauthenticated:\n\nimport\trequests,\tjson\nendpoint\t=\t\"https://api.github.com/users/joelgrus/repos\"\n\nrepos\t=\tjson.loads(requests.get(endpoint).text)\n\nAt\tthis\tpoint\trepos\tis\ta\tlist\tof\tPython\tdicts,\teach\trepresenting\ta\tpublic\trepository\tin\tmy\nGitHub\taccount.\t(Feel\tfree\tto\tsubstitute\tyour\tusername\tand\tget\tyour\tGitHub\trepository\ndata\tinstead.\tYou\tdo\thave\ta\tGitHub\taccount,\tright?)\n\nWe\tcan\tuse\tthis\tto\tfigure\tout\twhich\tmonths\tand\tdays\tof\tthe\tweek\tI\u2019m\tmost\tlikely\tto\tcreate\na\trepository.\tThe\tonly\tissue\tis\tthat\tthe\tdates\tin\tthe\tresponse\tare\t(Unicode)\tstrings:\n\nu'created_at':\tu'2013-07-05T02:02:28Z'\n\nPython\tdoesn\u2019t\tcome\twith\ta\tgreat\tdate\tparser,\tso\twe\u2019ll\tneed\tto\tinstall\tone:\n\npip\tinstall\tpython-dateutil\n\nfrom\twhich\tyou\u2019ll\tprobably\tonly\tever\tneed\tthe\tdateutil.parser.parse\tfunction:\n\nfrom\tdateutil.parser\timport\tparse\n\ndates\t=\t[parse(repo[\"created_at\"])\tfor\trepo\tin\trepos]\nmonth_counts\t=\tCounter(date.month\tfor\tdate\tin\tdates)\nweekday_counts\t=\tCounter(date.weekday()\tfor\tdate\tin\tdates)\n\nSimilarly,\tyou\tcan\tget\tthe\tlanguages\tof\tmy\tlast\tfive\trepositories:\n\nlast_5_repositories\t=\tsorted(repos,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkey=lambda\tr:\tr[\"created_at\"],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treverse=True)[:5]\n\nlast_5_languages\t=\t[repo[\"language\"]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\trepo\tin\tlast_5_repositories]\n\nTypically\twe\twon\u2019t\tbe\tworking\twith\tAPIs\tat\tthis\tlow\t\u201cmake\tthe\trequests\tand\tparse\tthe\nresponses\tourselves\u201d\tlevel.\tOne\tof\tthe\tbenefits\tof\tusing\tPython\tis\tthat\tsomeone\thas\nalready\tbuilt\ta\tlibrary\tfor\tpretty\tmuch\tany\tAPI\tyou\u2019re\tinterested\tin\taccessing.\tWhen\nthey\u2019re\tdone\twell,\tthese\tlibraries\tcan\tsave\tyou\ta\tlot\tof\tthe\ttrouble\tof\tfiguring\tout\tthe\nhairier\tdetails\tof\tAPI\taccess.\t(When\tthey\u2019re\tnot\tdone\twell,\tor\twhen\tit\tturns\tout\tthey\u2019re\nbased\ton\tdefunct\tversions\tof\tthe\tcorresponding\tAPIs,\tthey\tcan\tcause\tyou\tenormous\nheadaches.)\n\nNonetheless,\tyou\u2019ll\toccasionally\thave\tto\troll\tyour\town\tAPI-access\tlibrary\t(or,\tmore\tlikely,",
    "172": "debug\twhy\tsomeone\telse\u2019s\tisn\u2019t\tworking),\tso\tit\u2019s\tgood\tto\tknow\tsome\tof\tthe\tdetails.",
    "173": "Finding\tAPIs\n\nIf\tyou\tneed\tdata\tfrom\ta\tspecific\tsite,\tlook\tfor\ta\tdevelopers\tor\tAPI\tsection\tof\tthe\tsite\tfor\ndetails,\tand\ttry\tsearching\tthe\tWeb\tfor\t\u201cpython\t__\tapi\u201d\tto\tfind\ta\tlibrary.\tThere\tis\ta\tRotten\nTomatoes\tAPI\tfor\tPython.\tThere\tare\tmultiple\tPython\twrappers\tfor\tthe\tKlout\tAPI,\tfor\tthe\nYelp\tAPI,\tfor\tthe\tIMDB\tAPI,\tand\tso\ton.\n\nIf\tyou\u2019re\tlooking\tfor\tlists\tof\tAPIs\tthat\thave\tPython\twrappers,\ttwo\tdirectories\tare\tat\tPython\nAPI\tand\tPython\tfor\tBeginners.\n\nIf\tyou\twant\ta\tdirectory\tof\tweb\tAPIs\tmore\tbroadly\t(without\tPython\twrappers\tnecessarily),\na\tgood\tresource\tis\tProgrammable\tWeb,\twhich\thas\ta\thuge\tdirectory\tof\tcategorized\tAPIs.\n\nAnd\tif\tafter\tall\tthat\tyou\tcan\u2019t\tfind\twhat\tyou\tneed,\tthere\u2019s\talways\tscraping,\tthe\tlast\trefuge\nof\tthe\tdata\tscientist.",
    "174": "Example:\tUsing\tthe\tTwitter\tAPIs\n\nTwitter\tis\ta\tfantastic\tsource\tof\tdata\tto\twork\twith.\tYou\tcan\tuse\tit\tto\tget\treal-time\tnews.\tYou\ncan\tuse\tit\tto\tmeasure\treactions\tto\tcurrent\tevents.\tYou\tcan\tuse\tit\tto\tfind\tlinks\trelated\tto\nspecific\ttopics.\tYou\tcan\tuse\tit\tfor\tpretty\tmuch\tanything\tyou\tcan\timagine,\tjust\tas\tlong\tas\nyou\tcan\tget\taccess\tto\tits\tdata.\tAnd\tyou\tcan\tget\taccess\tto\tits\tdata\tthrough\tits\tAPI.\n\nTo\tinteract\twith\tthe\tTwitter\tAPIs\twe\u2019ll\tbe\tusing\tthe\tTwython\tlibrary\t(pip\tinstall\ntwython).\tThere\tare\tquite\ta\tfew\tPython\tTwitter\tlibraries\tout\tthere,\tbut\tthis\tis\tthe\tone\tthat\nI\u2019ve\thad\tthe\tmost\tsuccess\tworking\twith.\tYou\tare\tencouraged\tto\texplore\tthe\tothers\tas\twell!",
    "175": "Getting\tCredentials\n\nIn\torder\tto\tuse\tTwitter\u2019s\tAPIs,\tyou\tneed\tto\tget\tsome\tcredentials\t(for\twhich\tyou\tneed\ta\nTwitter\taccount,\twhich\tyou\tshould\thave\tanyway\tso\tthat\tyou\tcan\tbe\tpart\tof\tthe\tlively\tand\nfriendly\tTwitter\t#datascience\tcommunity).\tLike\tall\tinstructions\tthat\trelate\tto\twebsites\nthat\tI\tdon\u2019t\tcontrol,\tthese\tmay\tgo\tobsolete\tat\tsome\tpoint\tbut\twill\thopefully\twork\tfor\ta\nwhile.\t(Although\tthey\thave\talready\tchanged\tat\tleast\tonce\twhile\tI\twas\twriting\tthis\tbook,\tso\ngood\tluck!)\n\n1.\t Go\tto\thttps://apps.twitter.com/.\n\n2.\t If\tyou\tare\tnot\tsigned\tin,\tclick\tSign\tin\tand\tenter\tyour\tTwitter\tusername\tand\tpassword.\n\n3.\t Click\tCreate\tNew\tApp.\n\n4.\t Give\tit\ta\tname\t(such\tas\t\u201cData\tScience\u201d)\tand\ta\tdescription,\tand\tput\tany\tURL\tas\tthe\n\nwebsite\t(it\tdoesn\u2019t\tmatter\twhich\tone).\n\n5.\t Agree\tto\tthe\tTerms\tof\tService\tand\tclick\tCreate.\n\n6.\t Take\tnote\tof\tthe\tconsumer\tkey\tand\tconsumer\tsecret.\n\n7.\t Click\t\u201cCreate\tmy\taccess\ttoken.\u201d\n\n8.\t Take\tnote\tof\tthe\taccess\ttoken\tand\taccess\ttoken\tsecret\t(you\tmay\thave\tto\trefresh\tthe\n\npage).\n\nThe\tconsumer\tkey\tand\tconsumer\tsecret\ttell\tTwitter\twhat\tapplication\tis\taccessing\tits\tAPIs,\nwhile\tthe\taccess\ttoken\tand\taccess\ttoken\tsecret\ttell\tTwitter\twho\tis\taccessing\tits\tAPIs.\tIf\nyou\thave\tever\tused\tyour\tTwitter\taccount\tto\tlog\tin\tto\tsome\tother\tsite,\tthe\t\u201cclick\tto\nauthorize\u201d\tpage\twas\tgenerating\tan\taccess\ttoken\tfor\tthat\tsite\tto\tuse\tto\tconvince\tTwitter\tthat\nit\twas\tyou\t(or,\tat\tleast,\tacting\ton\tyour\tbehalf).\tAs\twe\tdon\u2019t\tneed\tthis\t\u201clet\tanyone\tlog\tin\u201d\nfunctionality,\twe\tcan\tget\tby\twith\tthe\tstatically\tgenerated\taccess\ttoken\tand\taccess\ttoken\nsecret.\n\nCaution\n\nThe\tconsumer\tkey/secret\tand\taccess\ttoken\tkey/secret\tshould\tbe\ttreated\tlike\tpasswords.\nYou\tshouldn\u2019t\tshare\tthem,\tyou\tshouldn\u2019t\tpublish\tthem\tin\tyour\tbook,\tand\tyou\tshouldn\u2019t\ncheck\tthem\tinto\tyour\tpublic\tGitHub\trepository.\tOne\tsimple\tsolution\tis\tto\tstore\tthem\tin\ta\ncredentials.json\tfile\tthat\tdoesn\u2019t\tget\tchecked\tin,\tand\tto\thave\tyour\tcode\tuse\tjson.loads\tto\nretrieve\tthem.\n\nUsing\tTwython\n\nFirst\twe\u2019ll\tlook\tat\tthe\tSearch\tAPI,\twhich\trequires\tonly\tthe\tconsumer\tkey\tand\tsecret,\tnot\nthe\taccess\ttoken\tor\tsecret:",
    "176": "from\ttwython\timport\tTwython\n\ntwitter\t=\tTwython(CONSUMER_KEY,\tCONSUMER_SECRET)\n\n#\tsearch\tfor\ttweets\tcontaining\tthe\tphrase\t\"data\tscience\"\nfor\tstatus\tin\ttwitter.search(q='\"data\tscience\"')[\"statuses\"]:\n\t\t\t\tuser\t=\tstatus[\"user\"][\"screen_name\"].encode('utf-8')\n\t\t\t\ttext\t=\tstatus[\"text\"].encode('utf-8')\n\t\t\t\tprint\tuser,\t\":\",\ttext\n\t\t\t\tprint\n\nNOTE\n\nThe\t.encode(\"utf-8\")\tis\tnecessary\tto\tdeal\twith\tthe\tfact\tthat\ttweets\toften\tcontain\tUnicode\tcharacters\tthat\nprint\tcan\u2019t\tdeal\twith.\t(If\tyou\tleave\tit\tout,\tyou\twill\tvery\tlikely\tget\ta\tUnicodeEncodeError.)\n\nIt\tis\talmost\tcertain\tthat\tat\tsome\tpoint\tin\tyour\tdata\tscience\tcareer\tyou\twill\trun\tinto\tsome\tserious\tUnicode\nproblems,\tat\twhich\tpoint\tyou\twill\tneed\tto\trefer\tto\tthe\tPython\tdocumentation\tor\telse\tgrudgingly\tstart\tusing\nPython\t3,\twhich\tplays\tmuch\tmore\tnicely\twith\tUnicode\ttext.\n\nIf\tyou\trun\tthis,\tyou\tshould\tget\tsome\ttweets\tback\tlike:\n\nhaithemnyc:\tData\tscientists\twith\tthe\ttechnical\tsavvy\t&amp;\tanalytical\tchops\tto\nderive\tmeaning\tfrom\tbig\tdata\tare\tin\tdemand.\thttp://t.co/HsF9Q0dShP\n\nRPubsRecent:\tData\tScience\thttp://t.co/6hcHUz2PHM\n\nspleonard1:\tUsing\t#dplyr\tin\t#R\tto\twork\tthrough\ta\tprocrastinated\tassignment\tfor\n@rdpeng\tin\t@coursera\tdata\tscience\tspecialization.\t\tSo\teasy\tand\tAwesome.\n\nThis\tisn\u2019t\tthat\tinteresting,\tlargely\tbecause\tthe\tTwitter\tSearch\tAPI\tjust\tshows\tyou\twhatever\nhandful\tof\trecent\tresults\tit\tfeels\tlike.\tWhen\tyou\u2019re\tdoing\tdata\tscience,\tmore\toften\tyou\nwant\ta\tlot\tof\ttweets.\tThis\tis\twhere\tthe\tStreaming\tAPI\tis\tuseful.\tIt\tallows\tyou\tto\tconnect\tto\n(a\tsample\tof)\tthe\tgreat\tTwitter\tfirehose.\tTo\tuse\tit,\tyou\u2019ll\tneed\tto\tauthenticate\tusing\tyour\naccess\ttokens.\n\nIn\torder\tto\taccess\tthe\tStreaming\tAPI\twith\tTwython,\twe\tneed\tto\tdefine\ta\tclass\tthat\tinherits\nfrom\tTwythonStreamer\tand\tthat\toverrides\tits\ton_success\tmethod\t(and\tpossibly\tits\non_error\tmethod):\n\nfrom\ttwython\timport\tTwythonStreamer\n\n#\tappending\tdata\tto\ta\tglobal\tvariable\tis\tpretty\tpoor\tform\n#\tbut\tit\tmakes\tthe\texample\tmuch\tsimpler\ntweets\t=\t[]\n\nclass\tMyStreamer(TwythonStreamer):\n\t\t\t\t\"\"\"our\town\tsubclass\tof\tTwythonStreamer\tthat\tspecifies\n\t\t\t\thow\tto\tinteract\twith\tthe\tstream\"\"\"\n\n\t\t\t\tdef\ton_success(self,\tdata):\n\t\t\t\t\t\t\t\t\"\"\"what\tdo\twe\tdo\twhen\ttwitter\tsends\tus\tdata?\n\t\t\t\t\t\t\t\there\tdata\twill\tbe\ta\tPython\tdict\trepresenting\ta\ttweet\"\"\"\n\n\t\t\t\t\t\t\t\t#\tonly\twant\tto\tcollect\tEnglish-language\ttweets\n\t\t\t\t\t\t\t\tif\tdata['lang']\t==\t'en':\n\t\t\t\t\t\t\t\t\t\t\t\ttweets.append(data)\n\t\t\t\t\t\t\t\t\t\t\t\tprint\t\"received\ttweet\t#\",\tlen(tweets)\n\n\t\t\t\t\t\t\t\t#\tstop\twhen\twe've\tcollected\tenough\n\t\t\t\t\t\t\t\tif\tlen(tweets)\t>=\t1000:\n\t\t\t\t\t\t\t\t\t\t\t\tself.disconnect()\n\n\t\t\t\tdef\ton_error(self,\tstatus_code,\tdata):",
    "177": "print\tstatus_code,\tdata\n\t\t\t\t\t\t\t\tself.disconnect()\n\nMyStreamer\twill\tconnect\tto\tthe\tTwitter\tstream\tand\twait\tfor\tTwitter\tto\tfeed\tit\tdata.\tEach\ntime\tit\treceives\tsome\tdata\t(here,\ta\tTweet\trepresented\tas\ta\tPython\tobject)\tit\tpasses\tit\tto\tthe\non_success\tmethod,\twhich\tappends\tit\tto\tour\ttweets\tlist\tif\tits\tlanguage\tis\tEnglish,\tand\nthen\tdisconnects\tthe\tstreamer\tafter\tit\u2019s\tcollected\t1,000\ttweets.\n\nAll\tthat\u2019s\tleft\tis\tto\tinitialize\tit\tand\tstart\tit\trunning:\n\nstream\t=\tMyStreamer(CONSUMER_KEY,\tCONSUMER_SECRET,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tACCESS_TOKEN,\tACCESS_TOKEN_SECRET)\n\n#\tstarts\tconsuming\tpublic\tstatuses\tthat\tcontain\tthe\tkeyword\t'data'\nstream.statuses.filter(track='data')\n\n#\tif\tinstead\twe\twanted\tto\tstart\tconsuming\ta\tsample\tof\t*all*\tpublic\tstatuses\n#\tstream.statuses.sample()\n\nThis\twill\trun\tuntil\tit\tcollects\t1,000\ttweets\t(or\tuntil\tit\tencounters\tan\terror)\tand\tstop,\tat\nwhich\tpoint\tyou\tcan\tstart\tanalyzing\tthose\ttweets.\tFor\tinstance,\tyou\tcould\tfind\tthe\tmost\ncommon\thashtags\twith:\n\ntop_hashtags\t=\tCounter(hashtag['text'].lower()\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\ttweet\tin\ttweets\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\thashtag\tin\ttweet[\"entities\"][\"hashtags\"])\n\nprint\ttop_hashtags.most_common(5)\n\nEach\ttweet\tcontains\ta\tlot\tof\tdata.\tYou\tcan\teither\tpoke\taround\tyourself\tor\tdig\tthrough\tthe\nTwitter\tAPI\tdocumentation.\n\nIn\ta\tnon-toy\tproject\tyou\tprobably\twouldn\u2019t\twant\tto\trely\ton\tan\tin-memory\tlist\tfor\tstoring\tthe\ttweets.\nInstead\tyou\u2019d\twant\tto\tsave\tthem\tto\ta\tfile\tor\ta\tdatabase,\tso\tthat\tyou\u2019d\thave\tthem\tpermanently.\n\nNOTE",
    "178": "For\tFurther\tExploration\n\npandas\tis\tthe\tprimary\tlibrary\tthat\tdata\tscience\ttypes\tuse\tfor\tworking\twith\t(and,\tin\nparticular,\timporting)\tdata.\n\nScrapy\tis\ta\tmore\tfull-featured\tlibrary\tfor\tbuilding\tmore\tcomplicated\tweb\tscrapers\tthat\ndo\tthings\tlike\tfollow\tunknown\tlinks.",
    "179": "",
    "180": "Chapter\t10.\tWorking\twith\tData\n\nExperts\toften\tpossess\tmore\tdata\tthan\tjudgment.\n\nColin\tPowell\n\nWorking\twith\tdata\tis\tboth\tan\tart\tand\ta\tscience.\tWe\u2019ve\tmostly\tbeen\ttalking\tabout\tthe\nscience\tpart,\tbut\tin\tthis\tchapter\twe\u2019ll\tlook\tat\tsome\tof\tthe\tart.",
    "181": "Exploring\tYour\tData\n\nAfter\tyou\u2019ve\tidentified\tthe\tquestions\tyou\u2019re\ttrying\tto\tanswer\tand\thave\tgotten\tyour\thands\non\tsome\tdata,\tyou\tmight\tbe\ttempted\tto\tdive\tin\tand\timmediately\tstart\tbuilding\tmodels\tand\ngetting\tanswers.\tBut\tyou\tshould\tresist\tthis\turge.\tYour\tfirst\tstep\tshould\tbe\tto\texplore\tyour\ndata.",
    "182": "Exploring\tOne-Dimensional\tData\n\nThe\tsimplest\tcase\tis\twhen\tyou\thave\ta\tone-dimensional\tdata\tset,\twhich\tis\tjust\ta\tcollection\nof\tnumbers.\tFor\texample,\tthese\tcould\tbe\tthe\tdaily\taverage\tnumber\tof\tminutes\teach\tuser\nspends\ton\tyour\tsite,\tthe\tnumber\tof\ttimes\teach\tof\ta\tcollection\tof\tdata\tscience\ttutorial\nvideos\twas\twatched,\tor\tthe\tnumber\tof\tpages\tof\teach\tof\tthe\tdata\tscience\tbooks\tin\tyour\tdata\nscience\tlibrary.\n\nAn\tobvious\tfirst\tstep\tis\tto\tcompute\ta\tfew\tsummary\tstatistics.\tYou\u2019d\tlike\tto\tknow\thow\nmany\tdata\tpoints\tyou\thave,\tthe\tsmallest,\tthe\tlargest,\tthe\tmean,\tand\tthe\tstandard\tdeviation.\n\nBut\teven\tthese\tdon\u2019t\tnecessarily\tgive\tyou\ta\tgreat\tunderstanding.\tA\tgood\tnext\tstep\tis\tto\ncreate\ta\thistogram,\tin\twhich\tyou\tgroup\tyour\tdata\tinto\tdiscrete\tbuckets\tand\tcount\thow\nmany\tpoints\tfall\tinto\teach\tbucket:\n\ndef\tbucketize(point,\tbucket_size):\n\t\t\t\t\"\"\"floor\tthe\tpoint\tto\tthe\tnext\tlower\tmultiple\tof\tbucket_size\"\"\"\n\t\t\t\treturn\tbucket_size\t*\tmath.floor(point\t/\tbucket_size)\n\ndef\tmake_histogram(points,\tbucket_size):\n\t\t\t\t\"\"\"buckets\tthe\tpoints\tand\tcounts\thow\tmany\tin\teach\tbucket\"\"\"\n\t\t\t\treturn\tCounter(bucketize(point,\tbucket_size)\tfor\tpoint\tin\tpoints)\n\ndef\tplot_histogram(points,\tbucket_size,\ttitle=\"\"):\n\t\t\t\thistogram\t=\tmake_histogram(points,\tbucket_size)\n\t\t\t\tplt.bar(histogram.keys(),\thistogram.values(),\twidth=bucket_size)\n\t\t\t\tplt.title(title)\n\t\t\t\tplt.show()\n\nFor\texample,\tconsider\tthe\ttwo\tfollowing\tsets\tof\tdata:\n\nrandom.seed(0)\n\n#\tuniform\tbetween\t-100\tand\t100\nuniform\t=\t[200\t*\trandom.random()\t-\t100\tfor\t_\tin\trange(10000)]\n\n#\tnormal\tdistribution\twith\tmean\t0,\tstandard\tdeviation\t57\nnormal\t=\t[57\t*\tinverse_normal_cdf(random.random())\n\t\t\t\t\t\t\t\t\t\tfor\t_\tin\trange(10000)]\n\nBoth\thave\tmeans\tclose\tto\t0\tand\tstandard\tdeviations\tclose\tto\t58.\tHowever,\tthey\thave\tvery\ndifferent\tdistributions.\tFigure\t10-1\tshows\tthe\tdistribution\tof\tuniform:\n\nplot_histogram(uniform,\t10,\t\"Uniform\tHistogram\")\n\nwhile\tFigure\t10-2\tshows\tthe\tdistribution\tof\tnormal:\n\nplot_histogram(normal,\t10,\t\"Normal\tHistogram\")\n\nIn\tthis\tcase,\tboth\tdistributions\thad\tpretty\tdifferent\tmax\tand\tmin,\tbut\teven\tknowing\tthat\nwouldn\u2019t\thave\tbeen\tsufficient\tto\tunderstand\thow\tthey\tdiffered.",
    "183": "Figure\t10-1.\tHistogram\tof\tuniform",
    "184": "Two\tDimensions\n\nNow\timagine\tyou\thave\ta\tdata\tset\twith\ttwo\tdimensions.\tMaybe\tin\taddition\tto\tdaily\nminutes\tyou\thave\tyears\tof\tdata\tscience\texperience.\tOf\tcourse\tyou\u2019d\twant\tto\tunderstand\neach\tdimension\tindividually.\tBut\tyou\tprobably\talso\twant\tto\tscatter\tthe\tdata.\n\nFor\texample,\tconsider\tanother\tfake\tdata\tset:\n\ndef\trandom_normal():\n\t\t\t\t\"\"\"returns\ta\trandom\tdraw\tfrom\ta\tstandard\tnormal\tdistribution\"\"\"\n\t\t\t\treturn\tinverse_normal_cdf(random.random())\n\nxs\t=\t[random_normal()\tfor\t_\tin\trange(1000)]\nys1\t=\t[\tx\t+\trandom_normal()\t/\t2\tfor\tx\tin\txs]\nys2\t=\t[-x\t+\trandom_normal()\t/\t2\tfor\tx\tin\txs]\n\nIf\tyou\twere\tto\trun\tplot_histogram\ton\tys1\tand\tys2\tyou\u2019d\tget\tvery\tsimilar\tlooking\tplots\n(indeed,\tboth\tare\tnormally\tdistributed\twith\tthe\tsame\tmean\tand\tstandard\tdeviation).\n\nBut\teach\thas\ta\tvery\tdifferent\tjoint\tdistribution\twith\txs,\tas\tshown\tin\tFigure\t10-3:\n\nFigure\t10-2.\tHistogram\tof\tnormal\n\nplt.scatter(xs,\tys1,\tmarker='.',\tcolor='black',\tlabel='ys1')\nplt.scatter(xs,\tys2,\tmarker='.',\tcolor='gray',\t\tlabel='ys2')\nplt.xlabel('xs')\nplt.ylabel('ys')\nplt.legend(loc=9)",
    "185": "plt.title(\"Very\tDifferent\tJoint\tDistributions\")\nplt.show()\n\nThis\tdifference\twould\talso\tbe\tapparent\tif\tyou\tlooked\tat\tthe\tcorrelations:\n\nFigure\t10-3.\tScattering\ttwo\tdifferent\tys\n\nprint\tcorrelation(xs,\tys1)\t\t\t\t\t\t#\t\t0.9\nprint\tcorrelation(xs,\tys2)\t\t\t\t\t\t#\t-0.9",
    "186": "Many\tDimensions\n\nWith\tmany\tdimensions,\tyou\u2019d\tlike\tto\tknow\thow\tall\tthe\tdimensions\trelate\tto\tone\tanother.\nA\tsimple\tapproach\tis\tto\tlook\tat\tthe\tcorrelation\tmatrix,\tin\twhich\tthe\tentry\tin\trow\ti\tand\ncolumn\tj\tis\tthe\tcorrelation\tbetween\tthe\tith\tdimension\tand\tthe\tjth\tdimension\tof\tthe\tdata:\n\ndef\tcorrelation_matrix(data):\n\t\t\t\t\"\"\"returns\tthe\tnum_columns\tx\tnum_columns\tmatrix\twhose\t(i,\tj)th\tentry\n\t\t\t\tis\tthe\tcorrelation\tbetween\tcolumns\ti\tand\tj\tof\tdata\"\"\"\n\n\t\t\t\t_,\tnum_columns\t=\tshape(data)\n\n\t\t\t\tdef\tmatrix_entry(i,\tj):\n\t\t\t\t\t\t\t\treturn\tcorrelation(get_column(data,\ti),\tget_column(data,\tj))\n\n\t\t\t\treturn\tmake_matrix(num_columns,\tnum_columns,\tmatrix_entry)\n\nA\tmore\tvisual\tapproach\t(if\tyou\tdon\u2019t\thave\ttoo\tmany\tdimensions)\tis\tto\tmake\ta\tscatterplot\nmatrix\t(Figure\t10-4)\tshowing\tall\tthe\tpairwise\tscatterplots.\tTo\tdo\tthat\twe\u2019ll\tuse\nplt.subplots(),\twhich\tallows\tus\tto\tcreate\tsubplots\tof\tour\tchart.\tWe\tgive\tit\tthe\tnumber\tof\nrows\tand\tthe\tnumber\tof\tcolumns,\tand\tit\treturns\ta\tfigure\tobject\t(which\twe\twon\u2019t\tuse)\tand\na\ttwo-dimensional\tarray\tof\taxes\tobjects\t(each\tof\twhich\twe\u2019ll\tplot\tto):\n\nimport\tmatplotlib.pyplot\tas\tplt\n\n_,\tnum_columns\t=\tshape(data)\nfig,\tax\t=\tplt.subplots(num_columns,\tnum_columns)\n\nfor\ti\tin\trange(num_columns):\n\t\t\t\tfor\tj\tin\trange(num_columns):\n\n\t\t\t\t\t\t\t\t#\tscatter\tcolumn_j\ton\tthe\tx-axis\tvs\tcolumn_i\ton\tthe\ty-axis\n\t\t\t\t\t\t\t\tif\ti\t!=\tj:\tax[i][j].scatter(get_column(data,\tj),\tget_column(data,\ti))\n\n\t\t\t\t\t\t\t\t#\tunless\ti\t==\tj,\tin\twhich\tcase\tshow\tthe\tseries\tname\n\t\t\t\t\t\t\t\telse:\tax[i][j].annotate(\"series\t\"\t+\tstr(i),\t(0.5,\t0.5),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\txycoords='axes\tfraction',\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tha=\"center\",\tva=\"center\")\n\n\t\t\t\t\t\t\t\t#\tthen\thide\taxis\tlabels\texcept\tleft\tand\tbottom\tcharts\n\t\t\t\t\t\t\t\tif\ti\t<\tnum_columns\t-\t1:\tax[i][j].xaxis.set_visible(False)\n\t\t\t\t\t\t\t\tif\tj\t>\t0:\tax[i][j].yaxis.set_visible(False)\n\n#\tfix\tthe\tbottom\tright\tand\ttop\tleft\taxis\tlabels,\twhich\tare\twrong\tbecause\n#\ttheir\tcharts\tonly\thave\ttext\tin\tthem\nax[-1][-1].set_xlim(ax[0][-1].get_xlim())\nax[0][0].set_ylim(ax[0][1].get_ylim())\n\nplt.show()",
    "187": "Figure\t10-4.\tScatterplot\tmatrix\n\nLooking\tat\tthe\tscatterplots,\tyou\tcan\tsee\tthat\tseries\t1\tis\tvery\tnegatively\tcorrelated\twith\nseries\t0,\tseries\t2\tis\tpositively\tcorrelated\twith\tseries\t1,\tand\tseries\t3\tonly\ttakes\ton\tthe\tvalues\n0\tand\t6,\twith\t0\tcorresponding\tto\tsmall\tvalues\tof\tseries\t2\tand\t6\tcorresponding\tto\tlarge\nvalues.\n\nThis\tis\ta\tquick\tway\tto\tget\ta\trough\tsense\tof\twhich\tof\tyour\tvariables\tare\tcorrelated\t(unless\nyou\tspend\thours\ttweaking\tmatplotlib\tto\tdisplay\tthings\texactly\tthe\tway\tyou\twant\tthem\nto,\tin\twhich\tcase\tit\u2019s\tnot\ta\tquick\tway).",
    "188": "Cleaning\tand\tMunging\n\nReal-world\tdata\tis\tdirty.\tOften\tyou\u2019ll\thave\tto\tdo\tsome\twork\ton\tit\tbefore\tyou\tcan\tuse\tit.\nWe\u2019ve\tseen\texamples\tof\tthis\tin\tChapter\t9.\tWe\thave\tto\tconvert\tstrings\tto\tfloats\tor\tints\nbefore\twe\tcan\tuse\tthem.\tPreviously,\twe\tdid\tthat\tright\tbefore\tusing\tthe\tdata:\n\n\t\t\t\t\t\t\t\tclosing_price\t=\tfloat(row[2])\n\nBut\tit\u2019s\tprobably\tless\terror-prone\tto\tdo\tthe\tparsing\ton\tthe\tway\tin,\twhich\twe\tcan\tdo\tby\ncreating\ta\tfunction\tthat\twraps\tcsv.reader.\tWe\u2019ll\tgive\tit\ta\tlist\tof\tparsers,\teach\tspecifying\nhow\tto\tparse\tone\tof\tthe\tcolumns.\tAnd\twe\u2019ll\tuse\tNone\tto\trepresent\t\u201cdon\u2019t\tdo\tanything\tto\nthis\tcolumn\u201d:\n\ndef\tparse_row(input_row,\tparsers):\n\t\t\t\t\"\"\"given\ta\tlist\tof\tparsers\t(some\tof\twhich\tmay\tbe\tNone)\n\t\t\t\tapply\tthe\tappropriate\tone\tto\teach\telement\tof\tthe\tinput_row\"\"\"\n\n\t\t\t\treturn\t[parser(value)\tif\tparser\tis\tnot\tNone\telse\tvalue\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tvalue,\tparser\tin\tzip(input_row,\tparsers)]\n\ndef\tparse_rows_with(reader,\tparsers):\n\t\t\t\t\"\"\"wrap\ta\treader\tto\tapply\tthe\tparsers\tto\teach\tof\tits\trows\"\"\"\n\t\t\t\tfor\trow\tin\treader:\n\t\t\t\t\t\t\t\tyield\tparse_row(row,\tparsers)\n\nWhat\tif\tthere\u2019s\tbad\tdata?\tA\t\u201cfloat\u201d\tvalue\tthat\tdoesn\u2019t\tactually\trepresent\ta\tnumber?\tWe\u2019d\nusually\trather\tget\ta\tNone\tthan\tcrash\tour\tprogram.\tWe\tcan\tdo\tthis\twith\ta\thelper\tfunction:\n\ndef\ttry_or_none(f):\n\t\t\t\t\"\"\"wraps\tf\tto\treturn\tNone\tif\tf\traises\tan\texception\n\t\t\t\tassumes\tf\ttakes\tonly\tone\tinput\"\"\"\n\t\t\t\tdef\tf_or_none(x):\n\t\t\t\t\t\t\t\ttry:\treturn\tf(x)\n\t\t\t\t\t\t\t\texcept:\treturn\tNone\n\t\t\t\treturn\tf_or_none\n\nafter\twhich\twe\tcan\trewrite\tparse_row\tto\tuse\tit:\n\ndef\tparse_row(input_row,\tparsers):\n\t\t\t\treturn\t[try_or_none(parser)(value)\tif\tparser\tis\tnot\tNone\telse\tvalue\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tvalue,\tparser\tin\tzip(input_row,\tparsers)]\n\nFor\texample,\tif\twe\thave\tcomma-delimited\tstock\tprices\twith\tbad\tdata:\n\n6/20/2014,AAPL,90.91\n6/20/2014,MSFT,41.68\n6/20/3014,FB,64.5\n6/19/2014,AAPL,91.86\n6/19/2014,MSFT,n/a\n6/19/2014,FB,64.34\n\nwe\tcan\tnow\tread\tand\tparse\tin\ta\tsingle\tstep:\n\nimport\tdateutil.parser\ndata\t=\t[]",
    "189": "with\topen(\"comma_delimited_stock_prices.csv\",\t\"rb\")\tas\tf:\n\t\t\t\treader\t=\tcsv.reader(f)\n\t\t\t\tfor\tline\tin\tparse_rows_with(reader,\t[dateutil.parser.parse,\tNone,\tfloat]):\n\t\t\t\t\t\t\t\tdata.append(line)\n\nafter\twhich\twe\tjust\tneed\tto\tcheck\tfor\tNone\trows:\n\nfor\trow\tin\tdata:\n\t\t\t\tif\tany(x\tis\tNone\tfor\tx\tin\trow):\n\t\t\t\t\t\t\t\tprint\trow\n\nand\tdecide\twhat\twe\twant\tto\tdo\tabout\tthem.\t(Generally\tspeaking,\tthe\tthree\toptions\tare\tto\nget\trid\tof\tthem,\tto\tgo\tback\tto\tthe\tsource\tand\ttry\tto\tfix\tthe\tbad/missing\tdata,\tor\tto\tdo\nnothing\tand\tcross\tour\tfingers.)\n\nWe\tcould\tcreate\tsimilar\thelpers\tfor\tcsv.DictReader.\tIn\tthat\tcase,\tyou\u2019d\tprobably\twant\tto\nsupply\ta\tdict\tof\tparsers\tby\tfield\tname.\tFor\texample:\n\ndef\ttry_parse_field(field_name,\tvalue,\tparser_dict):\n\t\t\t\t\"\"\"try\tto\tparse\tvalue\tusing\tthe\tappropriate\tfunction\tfrom\tparser_dict\"\"\"\n\t\t\t\tparser\t=\tparser_dict.get(field_name)\t\t\t#\tNone\tif\tno\tsuch\tentry\n\t\t\t\tif\tparser\tis\tnot\tNone:\n\t\t\t\t\t\t\t\treturn\ttry_or_none(parser)(value)\n\t\t\t\telse:\n\t\t\t\t\t\t\t\treturn\tvalue\n\ndef\tparse_dict(input_dict,\tparser_dict):\n\t\t\t\treturn\t{\tfield_name\t:\ttry_parse_field(field_name,\tvalue,\tparser_dict)\n\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tfield_name,\tvalue\tin\tinput_dict.iteritems()\t}\n\nA\tgood\tnext\tstep\tis\tto\tcheck\tfor\toutliers,\tusing\ttechniques\tfrom\t\u201cExploring\tYour\tData\u201d\tor\nby\tad\thoc\tinvestigating.\tFor\texample,\tdid\tyou\tnotice\tthat\tone\tof\tthe\tdates\tin\tthe\tstocks\tfile\nhad\tthe\tyear\t3014?\tThat\twon\u2019t\t(necessarily)\tgive\tyou\tan\terror,\tbut\tit\u2019s\tquite\tplainly\twrong,\nand\tyou\u2019ll\tget\tscrewy\tresults\tif\tyou\tdon\u2019t\tcatch\tit.\tReal-world\tdata\tsets\thave\tmissing\ndecimal\tpoints,\textra\tzeroes,\ttypographical\terrors,\tand\tcountless\tother\tproblems\tthat\tit\u2019s\nyour\tjob\tto\tcatch.\t(Maybe\tit\u2019s\tnot\tofficially\tyour\tjob,\tbut\twho\telse\tis\tgoing\tto\tdo\tit?)",
    "190": "Manipulating\tData\n\nOne\tof\tthe\tmost\timportant\tskills\tof\ta\tdata\tscientist\tis\tmanipulating\tdata.\tIt\u2019s\tmore\tof\ta\ngeneral\tapproach\tthan\ta\tspecific\ttechnique,\tso\twe\u2019ll\tjust\twork\tthrough\ta\thandful\tof\nexamples\tto\tgive\tyou\tthe\tflavor\tof\tit.\n\nImagine\twe\u2019re\tworking\twith\tdicts\tof\tstock\tprices\tthat\tlook\tlike:\n\ndata\t=\t[\n\t\t\t\t{'closing_price':\t102.06,\n\t\t\t\t\t'date':\tdatetime.datetime(2014,\t8,\t29,\t0,\t0),\n\t\t\t\t\t'symbol':\t'AAPL'},\n\t\t\t\t#\t...\n]\n\nConceptually\twe\u2019ll\tthink\tof\tthem\tas\trows\t(as\tin\ta\tspreadsheet).\n\nLet\u2019s\tstart\tasking\tquestions\tabout\tthis\tdata.\tAlong\tthe\tway\twe\u2019ll\ttry\tto\tnotice\tpatterns\tin\nwhat\twe\u2019re\tdoing\tand\tabstract\tout\tsome\ttools\tto\tmake\tthe\tmanipulation\teasier.\n\nFor\tinstance,\tsuppose\twe\twant\tto\tknow\tthe\thighest-ever\tclosing\tprice\tfor\tAAPL.\tLet\u2019s\nbreak\tthis\tdown\tinto\tconcrete\tsteps:\n\n1.\t Restrict\tourselves\tto\tAAPL\trows.\n\n2.\t Grab\tthe\tclosing_price\tfrom\teach\trow.\n\n3.\t Take\tthe\tmax\tof\tthose\tprices.\n\nWe\tcan\tdo\tall\tthree\tat\tonce\tusing\ta\tlist\tcomprehension:\n\nmax_aapl_price\t=\tmax(row[\"closing_price\"]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\trow\tin\tdata\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\trow[\"symbol\"]\t==\t\"AAPL\")\n\nMore\tgenerally,\twe\tmight\twant\tto\tknow\tthe\thighest-ever\tclosing\tprice\tfor\teach\tstock\tin\nour\tdata\tset.\tOne\tway\tto\tdo\tthis\tis:\n\n1.\t Group\ttogether\tall\tthe\trows\twith\tthe\tsame\tsymbol.\n\n2.\t Within\teach\tgroup,\tdo\tthe\tsame\tas\tbefore:\n\n#\tgroup\trows\tby\tsymbol\nby_symbol\t=\tdefaultdict(list)\nfor\trow\tin\tdata:\n\t\t\t\tby_symbol[row[\"symbol\"]].append(row)\n\n#\tuse\ta\tdict\tcomprehension\tto\tfind\tthe\tmax\tfor\teach\tsymbol\nmax_price_by_symbol\t=\t{\tsymbol\t:\tmax(row[\"closing_price\"]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\trow\tin\tgrouped_rows)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tsymbol,\tgrouped_rows\tin\tby_symbol.iteritems()\t}\n\nThere\tare\tsome\tpatterns\there\talready.\tIn\tboth\texamples,\twe\tneeded\tto\tpull\tthe\nclosing_price\tvalue\tout\tof\tevery\tdict.\tSo\tlet\u2019s\tcreate\ta\tfunction\tto\tpick\ta\tfield\tout\tof\ta\ndict,\tand\tanother\tfunction\tto\tpluck\tthe\tsame\tfield\tout\tof\ta\tcollection\tof\tdicts:",
    "191": "def\tpicker(field_name):\n\t\t\t\t\"\"\"returns\ta\tfunction\tthat\tpicks\ta\tfield\tout\tof\ta\tdict\"\"\"\n\t\t\t\treturn\tlambda\trow:\trow[field_name]\n\ndef\tpluck(field_name,\trows):\n\t\t\t\t\"\"\"turn\ta\tlist\tof\tdicts\tinto\tthe\tlist\tof\tfield_name\tvalues\"\"\"\n\t\t\t\treturn\tmap(picker(field_name),\trows)\n\nWe\tcan\talso\tcreate\ta\tfunction\tto\tgroup\trows\tby\tthe\tresult\tof\ta\tgrouper\tfunction\tand\tto\noptionally\tapply\tsome\tsort\tof\tvalue_transform\tto\teach\tgroup:\n\ndef\tgroup_by(grouper,\trows,\tvalue_transform=None):\n\t\t\t\t#\tkey\tis\toutput\tof\tgrouper,\tvalue\tis\tlist\tof\trows\n\t\t\t\tgrouped\t=\tdefaultdict(list)\n\t\t\t\tfor\trow\tin\trows:\n\t\t\t\t\t\t\t\tgrouped[grouper(row)].append(row)\n\n\t\t\t\tif\tvalue_transform\tis\tNone:\n\t\t\t\t\t\t\t\treturn\tgrouped\n\t\t\t\telse:\n\t\t\t\t\t\t\t\treturn\t{\tkey\t:\tvalue_transform(rows)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tkey,\trows\tin\tgrouped.iteritems()\t}\n\nThis\tallows\tus\tto\trewrite\tour\tprevious\texamples\tquite\tsimply.\tFor\texample:\n\nmax_price_by_symbol\t=\tgroup_by(picker(\"symbol\"),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlambda\trows:\tmax(pluck(\"closing_price\",\trows)))\n\nWe\tcan\tnow\tstart\tto\task\tmore\tcomplicated\tthings,\tlike\twhat\tare\tthe\tlargest\tand\tsmallest\none-day\tpercent\tchanges\tin\tour\tdata\tset.\tThe\tpercent\tchange\tis\tprice_today\t/\nprice_yesterday\t-\t1,\twhich\tmeans\twe\tneed\tsome\tway\tof\tassociating\ttoday\u2019s\tprice\tand\nyesterday\u2019s\tprice.\tOne\tapproach\tis\tto\tgroup\tthe\tprices\tby\tsymbol,\tthen,\twithin\teach\tgroup:\n\n1.\t Order\tthe\tprices\tby\tdate.\n\n2.\t Use\tzip\tto\tget\tpairs\t(previous,\tcurrent).\n\n3.\t Turn\tthe\tpairs\tinto\tnew\t\u201cpercent\tchange\u201d\trows.\n\nWe\u2019ll\tstart\tby\twriting\ta\tfunction\tto\tdo\tall\tthe\twithin-each-group\twork:\n\ndef\tpercent_price_change(yesterday,\ttoday):\n\t\t\t\treturn\ttoday[\"closing_price\"]\t/\tyesterday[\"closing_price\"]\t-\t1\n\ndef\tday_over_day_changes(grouped_rows):\n\t\t\t\t#\tsort\tthe\trows\tby\tdate\n\t\t\t\tordered\t=\tsorted(grouped_rows,\tkey=picker(\"date\"))\n\n\t\t\t\t#\tzip\twith\tan\toffset\tto\tget\tpairs\tof\tconsecutive\tdays\n\t\t\t\treturn\t[{\t\"symbol\"\t:\ttoday[\"symbol\"],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"date\"\t:\ttoday[\"date\"],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"change\"\t:\tpercent_price_change(yesterday,\ttoday)\t}\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tyesterday,\ttoday\tin\tzip(ordered,\tordered[1:])]\n\nThen\twe\tcan\tjust\tuse\tthis\tas\tthe\tvalue_transform\tin\ta\tgroup_by:",
    "192": "#\tkey\tis\tsymbol,\tvalue\tis\tlist\tof\t\"change\"\tdicts\nchanges_by_symbol\t=\tgroup_by(picker(\"symbol\"),\tdata,\tday_over_day_changes)\n\n#\tcollect\tall\t\"change\"\tdicts\tinto\tone\tbig\tlist\nall_changes\t=\t[change\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tchanges\tin\tchanges_by_symbol.values()\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tchange\tin\tchanges]\n\nAt\twhich\tpoint\tit\u2019s\teasy\tto\tfind\tthe\tlargest\tand\tsmallest:\n\nmax(all_changes,\tkey=picker(\"change\"))\n#\t{'change':\t0.3283582089552237,\n#\t\t'date':\tdatetime.datetime(1997,\t8,\t6,\t0,\t0),\n#\t\t'symbol':\t'AAPL'}\n#\tsee,\te.g.\thttp://news.cnet.com/2100-1001-202143.html\n\nmin(all_changes,\tkey=picker(\"change\"))\n#\t{'change':\t-0.5193370165745856,\n#\t\t'date':\tdatetime.datetime(2000,\t9,\t29,\t0,\t0),\n#\t\t'symbol':\t'AAPL'}\n#\tsee,\te.g.\thttp://money.cnn.com/2000/09/29/markets/techwrap/\n\nWe\tcan\tnow\tuse\tthis\tnew\tall_changes\tdata\tset\tto\tfind\twhich\tmonth\tis\tthe\tbest\tto\tinvest\tin\ntech\tstocks.\tFirst\twe\tgroup\tthe\tchanges\tby\tmonth;\tthen\twe\tcompute\tthe\toverall\tchange\nwithin\teach\tgroup.\n\nOnce\tagain,\twe\twrite\tan\tappropriate\tvalue_transform\tand\tthen\tuse\tgroup_by:\n\n#\tto\tcombine\tpercent\tchanges,\twe\tadd\t1\tto\teach,\tmultiply\tthem,\tand\tsubtract\t1\n#\tfor\tinstance,\tif\twe\tcombine\t+10%\tand\t-20%,\tthe\toverall\tchange\tis\n#\t\t\t\t(1\t+\t10%)\t*\t(1\t-\t20%)\t-\t1\t=\t1.1\t*\t.8\t-\t1\t=\t-12%\ndef\tcombine_pct_changes(pct_change1,\tpct_change2):\n\t\t\t\treturn\t(1\t+\tpct_change1)\t*\t(1\t+\tpct_change2)\t-\t1\n\ndef\toverall_change(changes):\n\t\t\t\treturn\treduce(combine_pct_changes,\tpluck(\"change\",\tchanges))\n\noverall_change_by_month\t=\tgroup_by(lambda\trow:\trow['date'].month,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tall_changes,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\toverall_change)\n\nWe\u2019ll\tbe\tdoing\tthese\tsorts\tof\tmanipulations\tthroughout\tthe\tbook,\tusually\twithout\tcalling\ntoo\tmuch\texplicit\tattention\tto\tthem.",
    "193": "Rescaling\n\nMany\ttechniques\tare\tsensitive\tto\tthe\tscale\tof\tyour\tdata.\tFor\texample,\timagine\tthat\tyou\nhave\ta\tdata\tset\tconsisting\tof\tthe\theights\tand\tweights\tof\thundreds\tof\tdata\tscientists,\tand\nthat\tyou\tare\ttrying\tto\tidentify\tclusters\tof\tbody\tsizes.\n\nIntuitively,\twe\u2019d\tlike\tclusters\tto\trepresent\tpoints\tnear\teach\tother,\twhich\tmeans\tthat\twe\nneed\tsome\tnotion\tof\tdistance\tbetween\tpoints.\tWe\talready\thave\ta\tEuclidean\tdistance\nfunction,\tso\ta\tnatural\tapproach\tmight\tbe\tto\ttreat\t(height,\tweight)\tpairs\tas\tpoints\tin\ttwo-\ndimensional\tspace.\tConsider\tthe\tpeople\tlisted\tin\tTable\t10-1.\n\nTable\t10-1.\tHeights\tand\tWeights\n\nPerson Height\t(inches) Height\t(centimeters) Weight\n\nA\n\nB\n\nC\n\n63\tinches\n\n160\tcm\n\n150\tpounds\n\n67\tinches\n\n170.2\tcm\n\n160\tpounds\n\n70\tinches\n\n177.8\tcm\n\n171\tpounds\n\nIf\twe\tmeasure\theight\tin\tinches,\tthen\tB\u2019s\tnearest\tneighbor\tis\tA:\n\na_to_b\t=\tdistance([63,\t150],\t[67,\t160])\t\t\t\t\t\t\t\t#\t10.77\na_to_c\t=\tdistance([63,\t150],\t[70,\t171])\t\t\t\t\t\t\t\t#\t22.14\nb_to_c\t=\tdistance([67,\t160],\t[70,\t171])\t\t\t\t\t\t\t\t#\t11.40\n\nHowever,\tif\twe\tmeasure\theight\tin\tcentimeters,\tthen\tB\u2019s\tnearest\tneighbor\tis\tinstead\tC:\n\na_to_b\t=\tdistance([160,\t150],\t[170.2,\t160])\t\t\t\t#\t14.28\na_to_c\t=\tdistance([160,\t150],\t[177.8,\t171])\t\t\t\t#\t27.53\nb_to_c\t=\tdistance([170.2,\t160],\t[177.8,\t171])\t\t#\t13.37\n\nObviously\tit\u2019s\tproblematic\tif\tchanging\tunits\tcan\tchange\tresults\tlike\tthis.\tFor\tthis\treason,\nwhen\tdimensions\taren\u2019t\tcomparable\twith\tone\tanother,\twe\twill\tsometimes\trescale\tour\tdata\nso\tthat\teach\tdimension\thas\tmean\t0\tand\tstandard\tdeviation\t1.\tThis\teffectively\tgets\trid\tof\nthe\tunits,\tconverting\teach\tdimension\tto\t\u201cstandard\tdeviations\tfrom\tthe\tmean.\u201d\n\nTo\tstart\twith,\twe\u2019ll\tneed\tto\tcompute\tthe\tmean\tand\tthe\tstandard_deviation\tfor\teach\ncolumn:\n\ndef\tscale(data_matrix):\n\t\t\t\t\"\"\"returns\tthe\tmeans\tand\tstandard\tdeviations\tof\teach\tcolumn\"\"\"\n\t\t\t\tnum_rows,\tnum_cols\t=\tshape(data_matrix)\n\t\t\t\tmeans\t=\t[mean(get_column(data_matrix,j))\n\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tj\tin\trange(num_cols)]\n\t\t\t\tstdevs\t=\t[standard_deviation(get_column(data_matrix,j))\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tj\tin\trange(num_cols)]\n\t\t\t\treturn\tmeans,\tstdevs\n\nAnd\tthen\tuse\tthem\tto\tcreate\ta\tnew\tdata\tmatrix:",
    "194": "def\trescale(data_matrix):\n\t\t\t\t\"\"\"rescales\tthe\tinput\tdata\tso\tthat\teach\tcolumn\n\t\t\t\thas\tmean\t0\tand\tstandard\tdeviation\t1\n\t\t\t\tleaves\talone\tcolumns\twith\tno\tdeviation\"\"\"\n\t\t\t\tmeans,\tstdevs\t=\tscale(data_matrix)\n\n\t\t\t\tdef\trescaled(i,\tj):\n\t\t\t\t\t\t\t\tif\tstdevs[j]\t>\t0:\n\t\t\t\t\t\t\t\t\t\t\t\treturn\t(data_matrix[i][j]\t-\tmeans[j])\t/\tstdevs[j]\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t\t\treturn\tdata_matrix[i][j]\n\n\t\t\t\tnum_rows,\tnum_cols\t=\tshape(data_matrix)\n\t\t\t\treturn\tmake_matrix(num_rows,\tnum_cols,\trescaled)\n\nAs\talways,\tyou\tneed\tto\tuse\tyour\tjudgment.\tIf\tyou\twere\tto\ttake\ta\thuge\tdata\tset\tof\theights\nand\tweights\tand\tfilter\tit\tdown\tto\tonly\tthe\tpeople\twith\theights\tbetween\t69.5\tinches\tand\n70.5\tinches,\tit\u2019s\tquite\tlikely\t(depending\ton\tthe\tquestion\tyou\u2019re\ttrying\tto\tanswer)\tthat\tthe\nvariation\tremaining\tis\tsimply\tnoise,\tand\tyou\tmight\tnot\twant\tto\tput\tits\tstandard\tdeviation\non\tequal\tfooting\twith\tother\tdimensions\u2019\tdeviations.",
    "195": "Dimensionality\tReduction\n\nSometimes\tthe\t\u201cactual\u201d\t(or\tuseful)\tdimensions\tof\tthe\tdata\tmight\tnot\tcorrespond\tto\tthe\ndimensions\twe\thave.\tFor\texample,\tconsider\tthe\tdata\tset\tpictured\tin\tFigure\t10-5.\n\nFigure\t10-5.\tData\twith\tthe\t\u201cwrong\u201d\taxes\n\nMost\tof\tthe\tvariation\tin\tthe\tdata\tseems\tto\tbe\talong\ta\tsingle\tdimension\tthat\tdoesn\u2019t\ncorrespond\tto\teither\tthe\tx-axis\tor\tthe\ty-axis.\n\nWhen\tthis\tis\tthe\tcase,\twe\tcan\tuse\ta\ttechnique\tcalled\tprincipal\tcomponent\tanalysis\tto\nextract\tone\tor\tmore\tdimensions\tthat\tcapture\tas\tmuch\tof\tthe\tvariation\tin\tthe\tdata\tas\npossible.\n\nNOTE\n\nIn\tpractice,\tyou\twouldn\u2019t\tuse\tthis\ttechnique\ton\tsuch\ta\tlow-dimensional\tdata\tset.\tDimensionality\treduction\nis\tmostly\tuseful\twhen\tyour\tdata\tset\thas\ta\tlarge\tnumber\tof\tdimensions\tand\tyou\twant\tto\tfind\ta\tsmall\tsubset\nthat\tcaptures\tmost\tof\tthe\tvariation.\tUnfortunately,\tthat\tcase\tis\tdifficult\tto\tillustrate\tin\ta\ttwo-dimensional\nbook\tformat.\n\nAs\ta\tfirst\tstep,\twe\u2019ll\tneed\tto\ttranslate\tthe\tdata\tso\tthat\teach\tdimension\thas\tmean\tzero:\n\ndef\tde_mean_matrix(A):\n\t\t\t\t\"\"\"returns\tthe\tresult\tof\tsubtracting\tfrom\tevery\tvalue\tin\tA\tthe\tmean\n\t\t\t\tvalue\tof\tits\tcolumn.\tthe\tresulting\tmatrix\thas\tmean\t0\tin\tevery\tcolumn\"\"\"\n\t\t\t\tnr,\tnc\t=\tshape(A)\n\t\t\t\tcolumn_means,\t_\t=\tscale(A)\n\t\t\t\treturn\tmake_matrix(nr,\tnc,\tlambda\ti,\tj:\tA[i][j]\t-\tcolumn_means[j])",
    "196": "(If\twe\tdon\u2019t\tdo\tthis,\tour\ttechniques\tare\tlikely\tto\tidentify\tthe\tmean\titself\trather\tthan\tthe\nvariation\tin\tthe\tdata.)\n\nFigure\t10-6\tshows\tthe\texample\tdata\tafter\tde-meaning.\n\nFigure\t10-6.\tData\tafter\tde-meaning\n\nNow,\tgiven\ta\tde-meaned\tmatrix\tX,\twe\tcan\task\twhich\tis\tthe\tdirection\tthat\tcaptures\tthe\ngreatest\tvariance\tin\tthe\tdata?\n\nSpecifically,\tgiven\ta\tdirection\td\t(a\tvector\tof\tmagnitude\t1),\teach\trow\tx\tin\tthe\tmatrix\nextends\tdot(x,\td)\tin\tthe\td\tdirection.\tAnd\tevery\tnonzero\tvector\tw\tdetermines\ta\tdirection\nif\twe\trescale\tit\tto\thave\tmagnitude\t1:\n\ndef\tdirection(w):\n\t\t\t\tmag\t=\tmagnitude(w)\n\t\t\t\treturn\t[w_i\t/\tmag\tfor\tw_i\tin\tw]\n\nTherefore,\tgiven\ta\tnonzero\tvector\tw,\twe\tcan\tcompute\tthe\tvariance\tof\tour\tdata\tset\tin\tthe\ndirection\tdetermined\tby\tw:\n\ndef\tdirectional_variance_i(x_i,\tw):\n\t\t\t\t\"\"\"the\tvariance\tof\tthe\trow\tx_i\tin\tthe\tdirection\tdetermined\tby\tw\"\"\"\n\t\t\t\treturn\tdot(x_i,\tdirection(w))\t**\t2\n\ndef\tdirectional_variance(X,\tw):\n\t\t\t\t\"\"\"the\tvariance\tof\tthe\tdata\tin\tthe\tdirection\tdetermined\tw\"\"\"\n\t\t\t\treturn\tsum(directional_variance_i(x_i,\tw)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tx_i\tin\tX)",
    "197": "We\u2019d\tlike\tto\tfind\tthe\tdirection\tthat\tmaximizes\tthis\tvariance.\tWe\tcan\tdo\tthis\tusing\tgradient\ndescent,\tas\tsoon\tas\twe\thave\tthe\tgradient\tfunction:\n\ndef\tdirectional_variance_gradient_i(x_i,\tw):\n\t\t\t\t\"\"\"the\tcontribution\tof\trow\tx_i\tto\tthe\tgradient\tof\n\t\t\t\tthe\tdirection-w\tvariance\"\"\"\n\t\t\t\tprojection_length\t=\tdot(x_i,\tdirection(w))\n\t\t\t\treturn\t[2\t*\tprojection_length\t*\tx_ij\tfor\tx_ij\tin\tx_i]\n\ndef\tdirectional_variance_gradient(X,\tw):\n\t\t\t\treturn\tvector_sum(directional_variance_gradient_i(x_i,w)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tx_i\tin\tX)\n\nThe\tfirst\tprincipal\tcomponent\tis\tjust\tthe\tdirection\tthat\tmaximizes\tthe\ndirectional_variance\tfunction:\n\ndef\tfirst_principal_component(X):\n\t\t\t\tguess\t=\t[1\tfor\t_\tin\tX[0]]\n\t\t\t\tunscaled_maximizer\t=\tmaximize_batch(\n\t\t\t\t\t\t\t\tpartial(directional_variance,\tX),\t\t\t\t\t\t\t\t\t\t\t#\tis\tnow\ta\tfunction\tof\tw\n\t\t\t\t\t\t\t\tpartial(directional_variance_gradient,\tX),\t\t#\tis\tnow\ta\tfunction\tof\tw\n\t\t\t\t\t\t\t\tguess)\n\t\t\t\treturn\tdirection(unscaled_maximizer)\n\nOr,\tif\tyou\u2019d\trather\tuse\tstochastic\tgradient\tdescent:\n\n#\there\tthere\tis\tno\t\"y\",\tso\twe\tjust\tpass\tin\ta\tvector\tof\tNones\n#\tand\tfunctions\tthat\tignore\tthat\tinput\ndef\tfirst_principal_component_sgd(X):\n\t\t\t\tguess\t=\t[1\tfor\t_\tin\tX[0]]\n\t\t\t\tunscaled_maximizer\t=\tmaximize_stochastic(\n\t\t\t\t\t\t\t\tlambda\tx,\t_,\tw:\tdirectional_variance_i(x,\tw),\n\t\t\t\t\t\t\t\tlambda\tx,\t_,\tw:\tdirectional_variance_gradient_i(x,\tw),\n\t\t\t\t\t\t\t\tX,\n\t\t\t\t\t\t\t\t[None\tfor\t_\tin\tX],\t\t\t#\tthe\tfake\t\"y\"\n\t\t\t\t\t\t\t\tguess)\n\t\t\t\treturn\tdirection(unscaled_maximizer)\n\nOn\tthe\tde-meaned\tdata\tset,\tthis\treturns\tthe\tdirection\t[0.924,\t0.383],\twhich\tdoes\tappear\nto\tcapture\tthe\tprimary\taxis\talong\twhich\tour\tdata\tvaries\t(Figure\t10-7).",
    "198": "Figure\t10-7.\tFirst\tprincipal\tcomponent\n\nOnce\twe\u2019ve\tfound\tthe\tdirection\tthat\u2019s\tthe\tfirst\tprincipal\tcomponent,\twe\tcan\tproject\tour\ndata\tonto\tit\tto\tfind\tthe\tvalues\tof\tthat\tcomponent:\n\ndef\tproject(v,\tw):\n\t\t\t\t\"\"\"return\tthe\tprojection\tof\tv\tonto\tthe\tdirection\tw\"\"\"\n\t\t\t\tprojection_length\t=\tdot(v,\tw)\n\t\t\t\treturn\tscalar_multiply(projection_length,\tw)\n\nIf\twe\twant\tto\tfind\tfurther\tcomponents,\twe\tfirst\tremove\tthe\tprojections\tfrom\tthe\tdata:\n\ndef\tremove_projection_from_vector(v,\tw):\n\t\t\t\t\"\"\"projects\tv\tonto\tw\tand\tsubtracts\tthe\tresult\tfrom\tv\"\"\"\n\t\t\t\treturn\tvector_subtract(v,\tproject(v,\tw))\n\ndef\tremove_projection(X,\tw):\n\t\t\t\t\"\"\"for\teach\trow\tof\tX\n\t\t\t\tprojects\tthe\trow\tonto\tw,\tand\tsubtracts\tthe\tresult\tfrom\tthe\trow\"\"\"\n\t\t\t\treturn\t[remove_projection_from_vector(x_i,\tw)\tfor\tx_i\tin\tX]\n\nBecause\tthis\texample\tdata\tset\tis\tonly\ttwo-dimensional,\tafter\twe\tremove\tthe\tfirst\ncomponent,\twhat\u2019s\tleft\twill\tbe\teffectively\tone-dimensional\t(Figure\t10-8).",
    "199": "Figure\t10-8.\tData\tafter\tremoving\tfirst\tprincipal\tcomponent\n\nAt\tthat\tpoint,\twe\tcan\tfind\tthe\tnext\tprincipal\tcomponent\tby\trepeating\tthe\tprocess\ton\tthe\nresult\tof\tremove_projection\t(Figure\t10-9).\n\nOn\ta\thigher-dimensional\tdata\tset,\twe\tcan\titeratively\tfind\tas\tmany\tcomponents\tas\twe\twant:\n\ndef\tprincipal_component_analysis(X,\tnum_components):\n\t\t\t\tcomponents\t=\t[]\n\t\t\t\tfor\t_\tin\trange(num_components):\n\t\t\t\t\t\t\t\tcomponent\t=\tfirst_principal_component(X)\n\t\t\t\t\t\t\t\tcomponents.append(component)\n\t\t\t\t\t\t\t\tX\t=\tremove_projection(X,\tcomponent)\n\n\t\t\t\treturn\tcomponents\n\nWe\tcan\tthen\ttransform\tour\tdata\tinto\tthe\tlower-dimensional\tspace\tspanned\tby\tthe\ncomponents:\n\ndef\ttransform_vector(v,\tcomponents):\n\t\t\t\treturn\t[dot(v,\tw)\tfor\tw\tin\tcomponents]\n\ndef\ttransform(X,\tcomponents):\n\t\t\t\treturn\t[transform_vector(x_i,\tcomponents)\tfor\tx_i\tin\tX]\n\nThis\ttechnique\tis\tvaluable\tfor\ta\tcouple\tof\treasons.\tFirst,\tit\tcan\thelp\tus\tclean\tour\tdata\tby\neliminating\tnoise\tdimensions\tand\tconsolidating\tdimensions\tthat\tare\thighly\tcorrelated.",
    "200": "Figure\t10-9.\tFirst\ttwo\tprincipal\tcomponents\n\nSecond,\tafter\textracting\ta\tlow-dimensional\trepresentation\tof\tour\tdata,\twe\tcan\tuse\ta\tvariety\nof\ttechniques\tthat\tdon\u2019t\twork\tas\twell\ton\thigh-dimensional\tdata.\tWe\u2019ll\tsee\texamples\tof\nsuch\ttechniques\tthroughout\tthe\tbook.\n\nAt\tthe\tsame\ttime,\twhile\tit\tcan\thelp\tyou\tbuild\tbetter\tmodels,\tit\tcan\talso\tmake\tthose\tmodels\nharder\tto\tinterpret.\tIt\u2019s\teasy\tto\tunderstand\tconclusions\tlike\t\u201cevery\textra\tyear\tof\texperience\nadds\tan\taverage\tof\t$10k\tin\tsalary.\u201d\tIt\u2019s\tmuch\tharder\tto\tmake\tsense\tof\t\u201cevery\tincrease\tof\n0.1\tin\tthe\tthird\tprincipal\tcomponent\tadds\tan\taverage\tof\t$10k\tin\tsalary.\u201d",
    "201": "For\tFurther\tExploration\n\nAs\twe\tmentioned\tat\tthe\tend\tof\tChapter\t9,\tpandas\tis\tprobably\tthe\tprimary\tPython\ttool\nfor\tcleaning,\tmunging,\tmanipulating,\tand\tworking\twith\tdata.\tAll\tthe\texamples\twe\tdid\nby\thand\tin\tthis\tchapter\tcould\tbe\tdone\tmuch\tmore\tsimply\tusing\tpandas.\tPython\tfor\nData\tAnalysis\t(O\u2019Reilly)\tis\tprobably\tthe\tbest\tway\tto\tlearn\tpandas.\n\nscikit-learn\thas\ta\twide\tvariety\tof\tmatrix\tdecomposition\tfunctions,\tincluding\tPCA.",
    "202": "",
    "203": "Chapter\t11.\tMachine\tLearning\n\nI\tam\talways\tready\tto\tlearn\talthough\tI\tdo\tnot\talways\tlike\tbeing\ttaught.\n\nWinston\tChurchill\n\nMany\tpeople\timagine\tthat\tdata\tscience\tis\tmostly\tmachine\tlearning\tand\tthat\tdata\tscientists\nmostly\tbuild\tand\ttrain\tand\ttweak\tmachine-learning\tmodels\tall\tday\tlong.\t(Then\tagain,\nmany\tof\tthose\tpeople\tdon\u2019t\tactually\tknow\twhat\tmachine\tlearning\tis.)\tIn\tfact,\tdata\tscience\nis\tmostly\tturning\tbusiness\tproblems\tinto\tdata\tproblems\tand\tcollecting\tdata\tand\nunderstanding\tdata\tand\tcleaning\tdata\tand\tformatting\tdata,\tafter\twhich\tmachine\tlearning\tis\nalmost\tan\tafterthought.\tEven\tso,\tit\u2019s\tan\tinteresting\tand\tessential\tafterthought\tthat\tyou\npretty\tmuch\thave\tto\tknow\tabout\tin\torder\tto\tdo\tdata\tscience.",
    "204": "Modeling\n\nBefore\twe\tcan\ttalk\tabout\tmachine\tlearning\twe\tneed\tto\ttalk\tabout\tmodels.\n\nWhat\tis\ta\tmodel?\tIt\u2019s\tsimply\ta\tspecification\tof\ta\tmathematical\t(or\tprobabilistic)\nrelationship\tthat\texists\tbetween\tdifferent\tvariables.\n\nFor\tinstance,\tif\tyou\u2019re\ttrying\tto\traise\tmoney\tfor\tyour\tsocial\tnetworking\tsite,\tyou\tmight\nbuild\ta\tbusiness\tmodel\t(likely\tin\ta\tspreadsheet)\tthat\ttakes\tinputs\tlike\t\u201cnumber\tof\tusers\u201d\nand\t\u201cad\trevenue\tper\tuser\u201d\tand\t\u201cnumber\tof\temployees\u201d\tand\toutputs\tyour\tannual\tprofit\tfor\nthe\tnext\tseveral\tyears.\tA\tcookbook\trecipe\tentails\ta\tmodel\tthat\trelates\tinputs\tlike\t\u201cnumber\nof\teaters\u201d\tand\t\u201chungriness\u201d\tto\tquantities\tof\tingredients\tneeded.\tAnd\tif\tyou\u2019ve\tever\nwatched\tpoker\ton\ttelevision,\tyou\tknow\tthat\tthey\testimate\teach\tplayer\u2019s\t\u201cwin\tprobability\u201d\nin\treal\ttime\tbased\ton\ta\tmodel\tthat\ttakes\tinto\taccount\tthe\tcards\tthat\thave\tbeen\trevealed\tso\nfar\tand\tthe\tdistribution\tof\tcards\tin\tthe\tdeck.\n\nThe\tbusiness\tmodel\tis\tprobably\tbased\ton\tsimple\tmathematical\trelationships:\tprofit\tis\nrevenue\tminus\texpenses,\trevenue\tis\tunits\tsold\ttimes\taverage\tprice,\tand\tso\ton.\tThe\trecipe\nmodel\tis\tprobably\tbased\ton\ttrial\tand\terror\t\u2014\tsomeone\twent\tin\ta\tkitchen\tand\ttried\tdifferent\ncombinations\tof\tingredients\tuntil\tthey\tfound\tone\tthey\tliked.\tAnd\tthe\tpoker\tmodel\tis\tbased\non\tprobability\ttheory,\tthe\trules\tof\tpoker,\tand\tsome\treasonably\tinnocuous\tassumptions\nabout\tthe\trandom\tprocess\tby\twhich\tcards\tare\tdealt.",
    "205": "What\tIs\tMachine\tLearning?\n\nEveryone\thas\ther\town\texact\tdefinition,\tbut\twe\u2019ll\tuse\tmachine\tlearning\tto\trefer\tto\tcreating\nand\tusing\tmodels\tthat\tare\tlearned\tfrom\tdata.\tIn\tother\tcontexts\tthis\tmight\tbe\tcalled\npredictive\tmodeling\tor\tdata\tmining,\tbut\twe\twill\tstick\twith\tmachine\tlearning.\tTypically,\nour\tgoal\twill\tbe\tto\tuse\texisting\tdata\tto\tdevelop\tmodels\tthat\twe\tcan\tuse\tto\tpredict\tvarious\noutcomes\tfor\tnew\tdata,\tsuch\tas:\n\nPredicting\twhether\tan\temail\tmessage\tis\tspam\tor\tnot\n\nPredicting\twhether\ta\tcredit\tcard\ttransaction\tis\tfraudulent\n\nPredicting\twhich\tadvertisement\ta\tshopper\tis\tmost\tlikely\tto\tclick\ton\n\nPredicting\twhich\tfootball\tteam\tis\tgoing\tto\twin\tthe\tSuper\tBowl\n\nWe\u2019ll\tlook\tat\tboth\tsupervised\tmodels\t(in\twhich\tthere\tis\ta\tset\tof\tdata\tlabeled\twith\tthe\ncorrect\tanswers\tto\tlearn\tfrom),\tand\tunsupervised\tmodels\t(in\twhich\tthere\tare\tno\tsuch\nlabels).\tThere\tare\tvarious\tother\ttypes\tlike\tsemisupervised\t(in\twhich\tonly\tsome\tof\tthe\tdata\nare\tlabeled)\tand\tonline\t(in\twhich\tthe\tmodel\tneeds\tto\tcontinuously\tadjust\tto\tnewly\tarriving\ndata)\tthat\twe\twon\u2019t\tcover\tin\tthis\tbook.\n\nNow,\tin\teven\tthe\tsimplest\tsituation\tthere\tare\tentire\tuniverses\tof\tmodels\tthat\tmight\ndescribe\tthe\trelationship\twe\u2019re\tinterested\tin.\tIn\tmost\tcases\twe\twill\tourselves\tchoose\ta\nparameterized\tfamily\tof\tmodels\tand\tthen\tuse\tdata\tto\tlearn\tparameters\tthat\tare\tin\tsome\nway\toptimal.\n\nFor\tinstance,\twe\tmight\tassume\tthat\ta\tperson\u2019s\theight\tis\t(roughly)\ta\tlinear\tfunction\tof\this\nweight\tand\tthen\tuse\tdata\tto\tlearn\twhat\tthat\tlinear\tfunction\tis.\tOr\twe\tmight\tassume\tthat\ta\ndecision\ttree\tis\ta\tgood\tway\tto\tdiagnose\twhat\tdiseases\tour\tpatients\thave\tand\tthen\tuse\tdata\nto\tlearn\tthe\t\u201coptimal\u201d\tsuch\ttree.\tThroughout\tthe\trest\tof\tthe\tbook\twe\u2019ll\tbe\tinvestigating\ndifferent\tfamilies\tof\tmodels\tthat\twe\tcan\tlearn.\n\nBut\tbefore\twe\tcan\tdo\tthat,\twe\tneed\tto\tbetter\tunderstand\tthe\tfundamentals\tof\tmachine\nlearning.\tFor\tthe\trest\tof\tthe\tchapter,\twe\u2019ll\tdiscuss\tsome\tof\tthose\tbasic\tconcepts,\tbefore\twe\nmove\ton\tto\tthe\tmodels\tthemselves.",
    "206": "Overfitting\tand\tUnderfitting\n\nA\tcommon\tdanger\tin\tmachine\tlearning\tis\toverfitting\t\u2014\tproducing\ta\tmodel\tthat\tperforms\nwell\ton\tthe\tdata\tyou\ttrain\tit\ton\tbut\tthat\tgeneralizes\tpoorly\tto\tany\tnew\tdata.\tThis\tcould\ninvolve\tlearning\tnoise\tin\tthe\tdata.\tOr\tit\tcould\tinvolve\tlearning\tto\tidentify\tspecific\tinputs\nrather\tthan\twhatever\tfactors\tare\tactually\tpredictive\tfor\tthe\tdesired\toutput.\n\nThe\tother\tside\tof\tthis\tis\tunderfitting,\tproducing\ta\tmodel\tthat\tdoesn\u2019t\tperform\twell\teven\ton\nthe\ttraining\tdata,\talthough\ttypically\twhen\tthis\thappens\tyou\tdecide\tyour\tmodel\tisn\u2019t\tgood\nenough\tand\tkeep\tlooking\tfor\ta\tbetter\tone.\n\nFigure\t11-1.\tOverfitting\tand\tunderfitting\n\nIn\tFigure\t11-1,\tI\u2019ve\tfit\tthree\tpolynomials\tto\ta\tsample\tof\tdata.\t(Don\u2019t\tworry\tabout\thow;\nwe\u2019ll\tget\tto\tthat\tin\tlater\tchapters.)\n\nThe\thorizontal\tline\tshows\tthe\tbest\tfit\tdegree\t0\t(i.e.,\tconstant)\tpolynomial.\tIt\tseverely\nunderfits\tthe\ttraining\tdata.\tThe\tbest\tfit\tdegree\t9\t(i.e.,\t10-parameter)\tpolynomial\tgoes\nthrough\tevery\ttraining\tdata\tpoint\texactly,\tbut\tit\tvery\tseverely\toverfits\t\u2014\tif\twe\twere\tto\npick\ta\tfew\tmore\tdata\tpoints\tit\twould\tquite\tlikely\tmiss\tthem\tby\ta\tlot.\tAnd\tthe\tdegree\t1\tline\nstrikes\ta\tnice\tbalance\t\u2014\tit\u2019s\tpretty\tclose\tto\tevery\tpoint,\tand\t(if\tthese\tdata\tare\nrepresentative)\tthe\tline\twill\tlikely\tbe\tclose\tto\tnew\tdata\tpoints\tas\twell.\n\nClearly\tmodels\tthat\tare\ttoo\tcomplex\tlead\tto\toverfitting\tand\tdon\u2019t\tgeneralize\twell\tbeyond",
    "207": "the\tdata\tthey\twere\ttrained\ton.\tSo\thow\tdo\twe\tmake\tsure\tour\tmodels\taren\u2019t\ttoo\tcomplex?\nThe\tmost\tfundamental\tapproach\tinvolves\tusing\tdifferent\tdata\tto\ttrain\tthe\tmodel\tand\tto\ttest\nthe\tmodel.\n\nThe\tsimplest\tway\tto\tdo\tthis\tis\tto\tsplit\tyour\tdata\tset,\tso\tthat\t(for\texample)\ttwo-thirds\tof\tit\tis\nused\tto\ttrain\tthe\tmodel,\tafter\twhich\twe\tmeasure\tthe\tmodel\u2019s\tperformance\ton\tthe\nremaining\tthird:\n\ndef\tsplit_data(data,\tprob):\n\t\t\t\t\"\"\"split\tdata\tinto\tfractions\t[prob,\t1\t-\tprob]\"\"\"\n\t\t\t\tresults\t=\t[],\t[]\n\t\t\t\tfor\trow\tin\tdata:\n\t\t\t\t\t\t\t\tresults[0\tif\trandom.random()\t<\tprob\telse\t1].append(row)\n\t\t\t\treturn\tresults\n\nOften,\twe\u2019ll\thave\ta\tmatrix\tx\tof\tinput\tvariables\tand\ta\tvector\ty\tof\toutput\tvariables.\tIn\tthat\ncase,\twe\tneed\tto\tmake\tsure\tto\tput\tcorresponding\tvalues\ttogether\tin\teither\tthe\ttraining\tdata\nor\tthe\ttest\tdata:\n\ndef\ttrain_test_split(x,\ty,\ttest_pct):\n\t\t\t\tdata\t=\tzip(x,\ty)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tpair\tcorresponding\tvalues\n\t\t\t\ttrain,\ttest\t=\tsplit_data(data,\t1\t-\ttest_pct)\t\t#\tsplit\tthe\tdata\tset\tof\tpairs\n\t\t\t\tx_train,\ty_train\t=\tzip(*train)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tmagical\tun-zip\ttrick\n\t\t\t\tx_test,\ty_test\t=\tzip(*test)\n\t\t\t\treturn\tx_train,\tx_test,\ty_train,\ty_test\n\nso\tthat\tyou\tmight\tdo\tsomething\tlike:\n\nmodel\t=\tSomeKindOfModel()\nx_train,\tx_test,\ty_train,\ty_test\t=\ttrain_test_split(xs,\tys,\t0.33)\nmodel.train(x_train,\ty_train)\nperformance\t=\tmodel.test(x_test,\ty_test)\n\nIf\tthe\tmodel\twas\toverfit\tto\tthe\ttraining\tdata,\tthen\tit\twill\thopefully\tperform\treally\tpoorly\non\tthe\t(completely\tseparate)\ttest\tdata.\tSaid\tdifferently,\tif\tit\tperforms\twell\ton\tthe\ttest\tdata,\nthen\tyou\tcan\tbe\tmore\tconfident\tthat\tit\u2019s\tfitting\trather\tthan\toverfitting.\n\nHowever,\tthere\tare\ta\tcouple\tof\tways\tthis\tcan\tgo\twrong.\n\nThe\tfirst\tis\tif\tthere\tare\tcommon\tpatterns\tin\tthe\ttest\tand\ttrain\tdata\tthat\twouldn\u2019t\tgeneralize\nto\ta\tlarger\tdata\tset.\n\nFor\texample,\timagine\tthat\tyour\tdata\tset\tconsists\tof\tuser\tactivity,\tone\trow\tper\tuser\tper\nweek.\tIn\tsuch\ta\tcase,\tmost\tusers\twill\tappear\tin\tboth\tthe\ttraining\tdata\tand\tthe\ttest\tdata,\tand\ncertain\tmodels\tmight\tlearn\tto\tidentify\tusers\trather\tthan\tdiscover\trelationships\tinvolving\nattributes.\tThis\tisn\u2019t\ta\thuge\tworry,\talthough\tit\tdid\thappen\tto\tme\tonce.\n\nA\tbigger\tproblem\tis\tif\tyou\tuse\tthe\ttest/train\tsplit\tnot\tjust\tto\tjudge\ta\tmodel\tbut\talso\tto\nchoose\tfrom\tamong\tmany\tmodels.\tIn\tthat\tcase,\talthough\teach\tindividual\tmodel\tmay\tnot\nbe\toverfit,\tthe\t\u201cchoose\ta\tmodel\tthat\tperforms\tbest\ton\tthe\ttest\tset\u201d\tis\ta\tmeta-training\tthat\nmakes\tthe\ttest\tset\tfunction\tas\ta\tsecond\ttraining\tset.\t(Of\tcourse\tthe\tmodel\tthat\tperformed\nbest\ton\tthe\ttest\tset\tis\tgoing\tto\tperform\twell\ton\tthe\ttest\tset.)",
    "208": "In\tsuch\ta\tsituation,\tyou\tshould\tsplit\tthe\tdata\tinto\tthree\tparts:\ta\ttraining\tset\tfor\tbuilding\nmodels,\ta\tvalidation\tset\tfor\tchoosing\tamong\ttrained\tmodels,\tand\ta\ttest\tset\tfor\tjudging\tthe\nfinal\tmodel.",
    "209": "Correctness\n\nWhen\tI\u2019m\tnot\tdoing\tdata\tscience,\tI\tdabble\tin\tmedicine.\tAnd\tin\tmy\tspare\ttime\tI\u2019ve\tcome\nup\twith\ta\tcheap,\tnoninvasive\ttest\tthat\tcan\tbe\tgiven\tto\ta\tnewborn\tbaby\tthat\tpredicts\t\u2014\nwith\tgreater\tthan\t98%\taccuracy\u2009\t\u2014\t\u2009whether\tthe\tnewborn\twill\tever\tdevelop\tleukemia.\tMy\nlawyer\thas\tconvinced\tme\tthe\ttest\tis\tunpatentable,\tso\tI\u2019ll\tshare\twith\tyou\tthe\tdetails\there:\npredict\tleukemia\tif\tand\tonly\tif\tthe\tbaby\tis\tnamed\tLuke\t(which\tsounds\tsort\tof\tlike\n\u201cleukemia\u201d).\n\nAs\twe\u2019ll\tsee\tbelow,\tthis\ttest\tis\tindeed\tmore\tthan\t98%\taccurate.\tNonetheless,\tit\u2019s\tan\nincredibly\tstupid\ttest,\tand\ta\tgood\tillustration\tof\twhy\twe\tdon\u2019t\ttypically\tuse\t\u201caccuracy\u201d\tto\nmeasure\thow\tgood\ta\tmodel\tis.\n\nImagine\tbuilding\ta\tmodel\tto\tmake\ta\tbinary\tjudgment.\tIs\tthis\temail\tspam?\tShould\twe\thire\nthis\tcandidate?\tIs\tthis\tair\ttraveler\tsecretly\ta\tterrorist?\n\nGiven\ta\tset\tof\tlabeled\tdata\tand\tsuch\ta\tpredictive\tmodel,\tevery\tdata\tpoint\tlies\tin\tone\tof\nfour\tcategories:\n\nTrue\tpositive:\t\u201cThis\tmessage\tis\tspam,\tand\twe\tcorrectly\tpredicted\tspam.\u201d\n\nFalse\tpositive\t(Type\t1\tError):\t\u201cThis\tmessage\tis\tnot\tspam,\tbut\twe\tpredicted\tspam.\u201d\n\nFalse\tnegative\t(Type\t2\tError):\t\u201cThis\tmessage\tis\tspam,\tbut\twe\tpredicted\tnot\tspam.\u201d\n\nTrue\tnegative:\t\u201cThis\tmessage\tis\tnot\tspam,\tand\twe\tcorrectly\tpredicted\tnot\tspam.\u201d\n\nWe\toften\trepresent\tthese\tas\tcounts\tin\ta\tconfusion\tmatrix:\n\nSpam\n\nnot\tSpam\n\npredict\t\u201cSpam\u201d\n\nTrue\tPositive\n\nFalse\tPositive\n\npredict\t\u201cNot\tSpam\u201d False\tNegative True\tNegative\n\nLet\u2019s\tsee\thow\tmy\tleukemia\ttest\tfits\tinto\tthis\tframework.\tThese\tdays\tapproximately\t5\nbabies\tout\tof\t1,000\tare\tnamed\tLuke.\tAnd\tthe\tlifetime\tprevalence\tof\tleukemia\tis\tabout\n1.4%,\tor\t14\tout\tof\tevery\t1,000\tpeople.\n\nIf\twe\tbelieve\tthese\ttwo\tfactors\tare\tindependent\tand\tapply\tmy\t\u201cLuke\tis\tfor\tleukemia\u201d\ttest\nto\t1\tmillion\tpeople,\twe\u2019d\texpect\tto\tsee\ta\tconfusion\tmatrix\tlike:\n\nleukemia no\tleukemia total\n\n\u201cLuke\u201d\n\n70\n\n4,930\n\n5,000\n\nnot\t\u201cLuke\u201d 13,930\n\n981,070\n\n995,000\n\ntotal\n\n14,000\n\n986,000\n\n1,000,000",
    "210": "We\tcan\tthen\tuse\tthese\tto\tcompute\tvarious\tstatistics\tabout\tmodel\tperformance.\tFor\nexample,\taccuracy\tis\tdefined\tas\tthe\tfraction\tof\tcorrect\tpredictions:\n\ndef\taccuracy(tp,\tfp,\tfn,\ttn):\n\t\t\t\tcorrect\t=\ttp\t+\ttn\n\t\t\t\ttotal\t=\ttp\t+\tfp\t+\tfn\t+\ttn\n\t\t\t\treturn\tcorrect\t/\ttotal\n\nprint\taccuracy(70,\t4930,\t13930,\t981070)\t\t\t\t\t#\t0.98114\n\nThat\tseems\tlike\ta\tpretty\timpressive\tnumber.\tBut\tclearly\tthis\tis\tnot\ta\tgood\ttest,\twhich\nmeans\tthat\twe\tprobably\tshouldn\u2019t\tput\ta\tlot\tof\tcredence\tin\traw\taccuracy.\n\nIt\u2019s\tcommon\tto\tlook\tat\tthe\tcombination\tof\tprecision\tand\trecall.\tPrecision\tmeasures\thow\naccurate\tour\tpositive\tpredictions\twere:\n\ndef\tprecision(tp,\tfp,\tfn,\ttn):\n\t\t\t\treturn\ttp\t/\t(tp\t+\tfp)\n\nprint\tprecision(70,\t4930,\t13930,\t981070)\t\t\t\t#\t0.014\n\nAnd\trecall\tmeasures\twhat\tfraction\tof\tthe\tpositives\tour\tmodel\tidentified:\n\ndef\trecall(tp,\tfp,\tfn,\ttn):\n\t\t\t\treturn\ttp\t/\t(tp\t+\tfn)\n\nprint\trecall(70,\t4930,\t13930,\t981070)\t\t\t\t\t\t\t#\t0.005\n\nThese\tare\tboth\tterrible\tnumbers,\treflecting\tthat\tthis\tis\ta\tterrible\tmodel.\n\nSometimes\tprecision\tand\trecall\tare\tcombined\tinto\tthe\tF1\tscore,\twhich\tis\tdefined\tas:\n\ndef\tf1_score(tp,\tfp,\tfn,\ttn):\n\t\t\t\tp\t=\tprecision(tp,\tfp,\tfn,\ttn)\n\t\t\t\tr\t=\trecall(tp,\tfp,\tfn,\ttn)\n\n\t\t\t\treturn\t2\t*\tp\t*\tr\t/\t(p\t+\tr)\n\nThis\tis\tthe\tharmonic\tmean\tof\tprecision\tand\trecall\tand\tnecessarily\tlies\tbetween\tthem.\n\nUsually\tthe\tchoice\tof\ta\tmodel\tinvolves\ta\ttrade-off\tbetween\tprecision\tand\trecall.\tA\tmodel\nthat\tpredicts\t\u201cyes\u201d\twhen\tit\u2019s\teven\ta\tlittle\tbit\tconfident\twill\tprobably\thave\ta\thigh\trecall\tbut\na\tlow\tprecision;\ta\tmodel\tthat\tpredicts\t\u201cyes\u201d\tonly\twhen\tit\u2019s\textremely\tconfident\tis\tlikely\tto\nhave\ta\tlow\trecall\tand\ta\thigh\tprecision.\n\nAlternatively,\tyou\tcan\tthink\tof\tthis\tas\ta\ttrade-off\tbetween\tfalse\tpositives\tand\tfalse\nnegatives.\tSaying\t\u201cyes\u201d\ttoo\toften\twill\tgive\tyou\tlots\tof\tfalse\tpositives;\tsaying\t\u201cno\u201d\ttoo\noften\twill\tgive\tyou\tlots\tof\tfalse\tnegatives.\n\nImagine\tthat\tthere\twere\t10\trisk\tfactors\tfor\tleukemia,\tand\tthat\tthe\tmore\tof\tthem\tyou\thad\nthe\tmore\tlikely\tyou\twere\tto\tdevelop\tleukemia.\tIn\tthat\tcase\tyou\tcan\timagine\ta\tcontinuum\nof\ttests:\t\u201cpredict\tleukemia\tif\tat\tleast\tone\trisk\tfactor,\u201d\t\u201cpredict\tleukemia\tif\tat\tleast\ttwo\trisk\nfactors,\u201d\tand\tso\ton.\tAs\tyou\tincrease\tthe\tthreshhold,\tyou\tincrease\tthe\ttest\u2019s\tprecision\t(since\npeople\twith\tmore\trisk\tfactors\tare\tmore\tlikely\tto\tdevelop\tthe\tdisease),\tand\tyou\tdecrease\tthe",
    "211": "test\u2019s\trecall\t(since\tfewer\tand\tfewer\tof\tthe\teventual\tdisease-sufferers\twill\tmeet\tthe\nthreshhold).\tIn\tcases\tlike\tthis,\tchoosing\tthe\tright\tthreshhold\tis\ta\tmatter\tof\tfinding\tthe\tright\ntrade-off.",
    "212": "The\tBias-Variance\tTrade-off\n\nAnother\tway\tof\tthinking\tabout\tthe\toverfitting\tproblem\tis\tas\ta\ttrade-off\tbetween\tbias\tand\nvariance.\n\nBoth\tare\tmeasures\tof\twhat\twould\thappen\tif\tyou\twere\tto\tretrain\tyour\tmodel\tmany\ttimes\ton\ndifferent\tsets\tof\ttraining\tdata\t(from\tthe\tsame\tlarger\tpopulation).\n\nFor\texample,\tthe\tdegree\t0\tmodel\tin\t\u201cOverfitting\tand\tUnderfitting\u201d\twill\tmake\ta\tlot\tof\nmistakes\tfor\tpretty\tmuch\tany\ttraining\tset\t(drawn\tfrom\tthe\tsame\tpopulation),\twhich\tmeans\nthat\tit\thas\ta\thigh\tbias.\tHowever,\tany\ttwo\trandomly\tchosen\ttraining\tsets\tshould\tgive\tpretty\nsimilar\tmodels\t(since\tany\ttwo\trandomly\tchosen\ttraining\tsets\tshould\thave\tpretty\tsimilar\naverage\tvalues).\tSo\twe\tsay\tthat\tit\thas\ta\tlow\tvariance.\tHigh\tbias\tand\tlow\tvariance\ttypically\ncorrespond\tto\tunderfitting.\n\nOn\tthe\tother\thand,\tthe\tdegree\t9\tmodel\tfit\tthe\ttraining\tset\tperfectly.\tIt\thas\tvery\tlow\tbias\tbut\nvery\thigh\tvariance\t(since\tany\ttwo\ttraining\tsets\twould\tlikely\tgive\trise\tto\tvery\tdifferent\nmodels).\tThis\tcorresponds\tto\toverfitting.\n\nThinking\tabout\tmodel\tproblems\tthis\tway\tcan\thelp\tyou\tfigure\tout\twhat\tdo\twhen\tyour\nmodel\tdoesn\u2019t\twork\tso\twell.\n\nIf\tyour\tmodel\thas\thigh\tbias\t(which\tmeans\tit\tperforms\tpoorly\teven\ton\tyour\ttraining\tdata)\nthen\tone\tthing\tto\ttry\tis\tadding\tmore\tfeatures.\tGoing\tfrom\tthe\tdegree\t0\tmodel\tin\n\u201cOverfitting\tand\tUnderfitting\u201d\tto\tthe\tdegree\t1\tmodel\twas\ta\tbig\timprovement.\n\nIf\tyour\tmodel\thas\thigh\tvariance,\tthen\tyou\tcan\tsimilarly\tremove\tfeatures.\tBut\tanother\nsolution\tis\tto\tobtain\tmore\tdata\t(if\tyou\tcan).",
    "213": "Figure\t11-2.\tReducing\tvariance\twith\tmore\tdata\n\nIn\tFigure\t11-2,\twe\tfit\ta\tdegree\t9\tpolynomial\tto\tdifferent\tsize\tsamples.\tThe\tmodel\tfit\tbased\non\t10\tdata\tpoints\tis\tall\tover\tthe\tplace,\tas\twe\tsaw\tbefore.\tIf\twe\tinstead\ttrained\ton\t100\tdata\npoints,\tthere\u2019s\tmuch\tless\toverfitting.\tAnd\tthe\tmodel\ttrained\tfrom\t1,000\tdata\tpoints\tlooks\nvery\tsimilar\tto\tthe\tdegree\t1\tmodel.\n\nHolding\tmodel\tcomplexity\tconstant,\tthe\tmore\tdata\tyou\thave,\tthe\tharder\tit\tis\tto\toverfit.\n\nOn\tthe\tother\thand,\tmore\tdata\twon\u2019t\thelp\twith\tbias.\tIf\tyour\tmodel\tdoesn\u2019t\tuse\tenough\nfeatures\tto\tcapture\tregularities\tin\tthe\tdata,\tthrowing\tmore\tdata\tat\tit\twon\u2019t\thelp.",
    "214": "Feature\tExtraction\tand\tSelection\n\nAs\twe\tmentioned,\twhen\tyour\tdata\tdoesn\u2019t\thave\tenough\tfeatures,\tyour\tmodel\tis\tlikely\tto\nunderfit.\tAnd\twhen\tyour\tdata\thas\ttoo\tmany\tfeatures,\tit\u2019s\teasy\tto\toverfit.\tBut\twhat\tare\nfeatures\tand\twhere\tdo\tthey\tcome\tfrom?\n\nFeatures\tare\twhatever\tinputs\twe\tprovide\tto\tour\tmodel.\n\nIn\tthe\tsimplest\tcase,\tfeatures\tare\tsimply\tgiven\tto\tyou.\tIf\tyou\twant\tto\tpredict\tsomeone\u2019s\nsalary\tbased\ton\ther\tyears\tof\texperience,\tthen\tyears\tof\texperience\tis\tthe\tonly\tfeature\tyou\nhave.\n\n(Although,\tas\twe\tsaw\tin\t\u201cOverfitting\tand\tUnderfitting\u201d,\tyou\tmight\talso\tconsider\tadding\nyears\tof\texperience\tsquared,\tcubed,\tand\tso\ton\tif\tthat\thelps\tyou\tbuild\ta\tbetter\tmodel.)\n\nThings\tbecome\tmore\tinteresting\tas\tyour\tdata\tbecomes\tmore\tcomplicated.\tImagine\ttrying\nto\tbuild\ta\tspam\tfilter\tto\tpredict\twhether\tan\temail\tis\tjunk\tor\tnot.\tMost\tmodels\twon\u2019t\tknow\nwhat\tto\tdo\twith\ta\traw\temail,\twhich\tis\tjust\ta\tcollection\tof\ttext.\tYou\u2019ll\thave\tto\textract\nfeatures.\tFor\texample:\n\nDoes\tthe\temail\tcontain\tthe\tword\t\u201cViagra\u201d?\n\nHow\tmany\ttimes\tdoes\tthe\tletter\td\tappear?\n\nWhat\twas\tthe\tdomain\tof\tthe\tsender?\n\nThe\tfirst\tis\tsimply\ta\tyes\tor\tno,\twhich\twe\ttypically\tencode\tas\ta\t1\tor\t0.\tThe\tsecond\tis\ta\nnumber.\tAnd\tthe\tthird\tis\ta\tchoice\tfrom\ta\tdiscrete\tset\tof\toptions.\n\nPretty\tmuch\talways,\twe\u2019ll\textract\tfeatures\tfrom\tour\tdata\tthat\tfall\tinto\tone\tof\tthese\tthree\ncategories.\tWhat\u2019s\tmore,\tthe\ttype\tof\tfeatures\twe\thave\tconstrains\tthe\ttype\tof\tmodels\twe\ncan\tuse.\n\nThe\tNaive\tBayes\tclassifier\twe\u2019ll\tbuild\tin\tChapter\t13\tis\tsuited\tto\tyes-or-no\tfeatures,\tlike\nthe\tfirst\tone\tin\tthe\tpreceding\tlist.\n\nRegression\tmodels,\tas\twe\u2019ll\tstudy\tin\tChapter\t14\tand\tChapter\t16,\trequire\tnumeric\tfeatures\n(which\tcould\tinclude\tdummy\tvariables\tthat\tare\t0s\tand\t1s).\n\nAnd\tdecision\ttrees,\twhich\twe\u2019ll\tlook\tat\tin\tChapter\t17,\tcan\tdeal\twith\tnumeric\tor\ncategorical\tdata.\n\nAlthough\tin\tthe\tspam\tfilter\texample\twe\tlooked\tfor\tways\tto\tcreate\tfeatures,\tsometimes\nwe\u2019ll\tinstead\tlook\tfor\tways\tto\tremove\tfeatures.\n\nFor\texample,\tyour\tinputs\tmight\tbe\tvectors\tof\tseveral\thundred\tnumbers.\tDepending\ton\tthe\nsituation,\tit\tmight\tbe\tappropriate\tto\tdistill\tthese\tdown\tto\thandful\tof\timportant\tdimensions\n(as\tin\t\u201cDimensionality\tReduction\u201d)\tand\tuse\tonly\tthose\tsmall\tnumber\tof\tfeatures.\tOr\tit\nmight\tbe\tappropriate\tto\tuse\ta\ttechnique\t(like\tregularization,\twhich\twe\u2019ll\tlook\tat\tin\n\u201cRegularization\u201d)\tthat\tpenalizes\tmodels\tthe\tmore\tfeatures\tthey\tuse.",
    "215": "How\tdo\twe\tchoose\tfeatures?\tThat\u2019s\twhere\ta\tcombination\tof\texperience\tand\tdomain\nexpertise\tcomes\tinto\tplay.\tIf\tyou\u2019ve\treceived\tlots\tof\temails,\tthen\tyou\tprobably\thave\ta\nsense\tthat\tthe\tpresence\tof\tcertain\twords\tmight\tbe\ta\tgood\tindicator\tof\tspamminess.\tAnd\nyou\tmight\talso\thave\ta\tsense\tthat\tthe\tnumber\tof\td\u2019s\tis\tlikely\tnot\ta\tgood\tindicator\tof\nspamminess.\tBut\tin\tgeneral\tyou\u2019ll\thave\tto\ttry\tdifferent\tthings,\twhich\tis\tpart\tof\tthe\tfun.",
    "216": "For\tFurther\tExploration\n\nKeep\treading!\tThe\tnext\tseveral\tchapters\tare\tabout\tdifferent\tfamilies\tof\tmachine-\nlearning\tmodels.\n\nThe\tCoursera\tMachine\tLearning\tcourse\tis\tthe\toriginal\tMOOC\tand\tis\ta\tgood\tplace\tto\nget\ta\tdeeper\tunderstanding\tof\tthe\tbasics\tof\tmachine\tlearning.\tThe\tCaltech\tMachine\nLearning\tMOOC\tis\talso\tgood.\n\nThe\tElements\tof\tStatistical\tLearning\tis\ta\tsomewhat\tcanonical\ttextbook\tthat\tcan\tbe\ndownloaded\tonline\tfor\tfree.\tBut\tbe\twarned:\tit\u2019s\tvery\tmathy.",
    "217": "",
    "218": "Chapter\t12.\tk-Nearest\tNeighbors\n\nIf\tyou\twant\tto\tannoy\tyour\tneighbors,\ttell\tthe\ttruth\tabout\tthem.\n\nPietro\tAretino\n\nImagine\tthat\tyou\u2019re\ttrying\tto\tpredict\thow\tI\u2019m\tgoing\tto\tvote\tin\tthe\tnext\tpresidential\nelection.\tIf\tyou\tknow\tnothing\telse\tabout\tme\t(and\tif\tyou\thave\tthe\tdata),\tone\tsensible\napproach\tis\tto\tlook\tat\thow\tmy\tneighbors\tare\tplanning\tto\tvote.\tLiving\tin\tdowntown\nSeattle,\tas\tI\tdo,\tmy\tneighbors\tare\tinvariably\tplanning\tto\tvote\tfor\tthe\tDemocratic\ncandidate,\twhich\tsuggests\tthat\t\u201cDemocratic\tcandidate\u201d\tis\ta\tgood\tguess\tfor\tme\tas\twell.\n\nNow\timagine\tyou\tknow\tmore\tabout\tme\tthan\tjust\tgeography\t\u2014\tperhaps\tyou\tknow\tmy\tage,\nmy\tincome,\thow\tmany\tkids\tI\thave,\tand\tso\ton.\tTo\tthe\textent\tmy\tbehavior\tis\tinfluenced\t(or\ncharacterized)\tby\tthose\tthings,\tlooking\tjust\tat\tmy\tneighbors\twho\tare\tclose\tto\tme\tamong\nall\tthose\tdimensions\tseems\tlikely\tto\tbe\tan\teven\tbetter\tpredictor\tthan\tlooking\tat\tall\tmy\nneighbors.\tThis\tis\tthe\tidea\tbehind\tnearest\tneighbors\tclassification.",
    "219": "The\tModel\n\nNearest\tneighbors\tis\tone\tof\tthe\tsimplest\tpredictive\tmodels\tthere\tis.\tIt\tmakes\tno\nmathematical\tassumptions,\tand\tit\tdoesn\u2019t\trequire\tany\tsort\tof\theavy\tmachinery.\tThe\tonly\nthings\tit\trequires\tare:\n\nSome\tnotion\tof\tdistance\n\nAn\tassumption\tthat\tpoints\tthat\tare\tclose\tto\tone\tanother\tare\tsimilar\n\nMost\tof\tthe\ttechniques\twe\u2019ll\tlook\tat\tin\tthis\tbook\tlook\tat\tthe\tdata\tset\tas\ta\twhole\tin\torder\tto\nlearn\tpatterns\tin\tthe\tdata.\tNearest\tneighbors,\ton\tthe\tother\thand,\tquite\tconsciously\tneglects\na\tlot\tof\tinformation,\tsince\tthe\tprediction\tfor\teach\tnew\tpoint\tdepends\tonly\ton\tthe\thandful\nof\tpoints\tclosest\tto\tit.\n\nWhat\u2019s\tmore,\tnearest\tneighbors\tis\tprobably\tnot\tgoing\tto\thelp\tyou\tunderstand\tthe\tdrivers\nof\twhatever\tphenomenon\tyou\u2019re\tlooking\tat.\tPredicting\tmy\tvotes\tbased\ton\tmy\tneighbors\u2019\nvotes\tdoesn\u2019t\ttell\tyou\tmuch\tabout\twhat\tcauses\tme\tto\tvote\tthe\tway\tI\tdo,\twhereas\tsome\nalternative\tmodel\tthat\tpredicted\tmy\tvote\tbased\ton\t(say)\tmy\tincome\tand\tmarital\tstatus\tvery\nwell\tmight.\n\nIn\tthe\tgeneral\tsituation,\twe\thave\tsome\tdata\tpoints\tand\twe\thave\ta\tcorresponding\tset\tof\nlabels.\tThe\tlabels\tcould\tbe\tTrue\tand\tFalse,\tindicating\twhether\teach\tinput\tsatisfies\tsome\ncondition\tlike\t\u201cis\tspam?\u201d\tor\t\u201cis\tpoisonous?\u201d\tor\t\u201cwould\tbe\tenjoyable\tto\twatch?\u201d\tOr\tthey\ncould\tbe\tcategories,\tlike\tmovie\tratings\t(G,\tPG,\tPG-13,\tR,\tNC-17).\tOr\tthey\tcould\tbe\tthe\nnames\tof\tpresidential\tcandidates.\tOr\tthey\tcould\tbe\tfavorite\tprogramming\tlanguages.\n\nIn\tour\tcase,\tthe\tdata\tpoints\twill\tbe\tvectors,\twhich\tmeans\tthat\twe\tcan\tuse\tthe\tdistance\nfunction\tfrom\tChapter\t4.\n\nLet\u2019s\tsay\twe\u2019ve\tpicked\ta\tnumber\tk\tlike\t3\tor\t5.\tThen\twhen\twe\twant\tto\tclassify\tsome\tnew\ndata\tpoint,\twe\tfind\tthe\tk\tnearest\tlabeled\tpoints\tand\tlet\tthem\tvote\ton\tthe\tnew\toutput.\n\nTo\tdo\tthis,\twe\u2019ll\tneed\ta\tfunction\tthat\tcounts\tvotes.\tOne\tpossibility\tis:\n\ndef\traw_majority_vote(labels):\n\t\t\t\tvotes\t=\tCounter(labels)\n\t\t\t\twinner,\t_\t=\tvotes.most_common(1)[0]\n\t\t\t\treturn\twinner\n\nBut\tthis\tdoesn\u2019t\tdo\tanything\tintelligent\twith\tties.\tFor\texample,\timagine\twe\u2019re\trating\nmovies\tand\tthe\tfive\tnearest\tmovies\tare\trated\tG,\tG,\tPG,\tPG,\tand\tR.\tThen\tG\thas\ttwo\tvotes\nand\tPG\talso\thas\ttwo\tvotes.\tIn\tthat\tcase,\twe\thave\tseveral\toptions:\n\nPick\tone\tof\tthe\twinners\tat\trandom.\n\nWeight\tthe\tvotes\tby\tdistance\tand\tpick\tthe\tweighted\twinner.\n\nReduce\tk\tuntil\twe\tfind\ta\tunique\twinner.",
    "220": "We\u2019ll\timplement\tthe\tthird:\n\ndef\tmajority_vote(labels):\n\t\t\t\t\"\"\"assumes\tthat\tlabels\tare\tordered\tfrom\tnearest\tto\tfarthest\"\"\"\n\t\t\t\tvote_counts\t=\tCounter(labels)\n\t\t\t\twinner,\twinner_count\t=\tvote_counts.most_common(1)[0]\n\t\t\t\tnum_winners\t=\tlen([count\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tcount\tin\tvote_counts.values()\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tcount\t==\twinner_count])\n\n\t\t\t\tif\tnum_winners\t==\t1:\n\t\t\t\t\t\t\t\treturn\twinner\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tunique\twinner,\tso\treturn\tit\n\t\t\t\telse:\n\t\t\t\t\t\t\t\treturn\tmajority_vote(labels[:-1])\t#\ttry\tagain\twithout\tthe\tfarthest\n\nThis\tapproach\tis\tsure\tto\twork\teventually,\tsince\tin\tthe\tworst\tcase\twe\tgo\tall\tthe\tway\tdown\nto\tjust\tone\tlabel,\tat\twhich\tpoint\tthat\tone\tlabel\twins.\n\nWith\tthis\tfunction\tit\u2019s\teasy\tto\tcreate\ta\tclassifier:\n\ndef\tknn_classify(k,\tlabeled_points,\tnew_point):\n\t\t\t\t\"\"\"each\tlabeled\tpoint\tshould\tbe\ta\tpair\t(point,\tlabel)\"\"\"\n\n\t\t\t\t#\torder\tthe\tlabeled\tpoints\tfrom\tnearest\tto\tfarthest\n\t\t\t\tby_distance\t=\tsorted(labeled_points,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkey=lambda\t(point,\t_):\tdistance(point,\tnew_point))\n\n\t\t\t\t#\tfind\tthe\tlabels\tfor\tthe\tk\tclosest\n\t\t\t\tk_nearest_labels\t=\t[label\tfor\t_,\tlabel\tin\tby_distance[:k]]\n\n\t\t\t\t#\tand\tlet\tthem\tvote\n\t\t\t\treturn\tmajority_vote(k_nearest_labels)\n\nLet\u2019s\ttake\ta\tlook\tat\thow\tthis\tworks.",
    "221": "Example:\tFavorite\tLanguages\n\nThe\tresults\tof\tthe\tfirst\tDataSciencester\tuser\tsurvey\tare\tback,\tand\twe\u2019ve\tfound\tthe\npreferred\tprogramming\tlanguages\tof\tour\tusers\tin\ta\tnumber\tof\tlarge\tcities:\n\n#\teach\tentry\tis\t([longitude,\tlatitude],\tfavorite_language)\n\ncities\t=\t[([-122.3\t,\t47.53],\t\"Python\"),\t\t#\tSeattle\n\t\t\t\t\t\t\t\t\t\t([\t-96.85,\t32.85],\t\"Java\"),\t\t\t\t#\tAustin\n\t\t\t\t\t\t\t\t\t\t([\t-89.33,\t43.13],\t\"R\"),\t\t\t\t\t\t\t#\tMadison\n\t\t\t\t\t\t\t\t\t\t#\t...\tand\tso\ton\n]\n\nThe\tVP\tof\tCommunity\tEngagement\twants\tto\tknow\tif\twe\tcan\tuse\tthese\tresults\tto\tpredict\nthe\tfavorite\tprogramming\tlanguages\tfor\tplaces\tthat\tweren\u2019t\tpart\tof\tour\tsurvey.\n\nAs\tusual,\ta\tgood\tfirst\tstep\tis\tplotting\tthe\tdata\t(Figure\t12-1):\n\n#\tkey\tis\tlanguage,\tvalue\tis\tpair\t(longitudes,\tlatitudes)\nplots\t=\t{\t\"Java\"\t:\t([],\t[]),\t\"Python\"\t:\t([],\t[]),\t\"R\"\t:\t([],\t[])\t}\n\n#\twe\twant\teach\tlanguage\tto\thave\ta\tdifferent\tmarker\tand\tcolor\nmarkers\t=\t{\t\"Java\"\t:\t\"o\",\t\"Python\"\t:\t\"s\",\t\"R\"\t:\t\"^\"\t}\ncolors\t\t=\t{\t\"Java\"\t:\t\"r\",\t\"Python\"\t:\t\"b\",\t\"R\"\t:\t\"g\"\t}\n\nfor\t(longitude,\tlatitude),\tlanguage\tin\tcities:\n\t\t\t\tplots[language][0].append(longitude)\n\t\t\t\tplots[language][1].append(latitude)\n\n#\tcreate\ta\tscatter\tseries\tfor\teach\tlanguage\nfor\tlanguage,\t(x,\ty)\tin\tplots.iteritems():\n\t\t\t\tplt.scatter(x,\ty,\tcolor=colors[language],\tmarker=markers[language],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlabel=language,\tzorder=10)\n\nplot_state_borders(plt)\t\t\t\t\t\t#\tpretend\twe\thave\ta\tfunction\tthat\tdoes\tthis\n\nplt.legend(loc=0)\t\t\t\t\t\t\t\t\t\t\t\t#\tlet\tmatplotlib\tchoose\tthe\tlocation\nplt.axis([-130,-60,20,55])\t\t\t#\tset\tthe\taxes\n\nplt.title(\"Favorite\tProgramming\tLanguages\")\nplt.show()",
    "222": "Figure\t12-1.\tFavorite\tprogramming\tlanguages\n\nNOTE\n\nYou\tmay\thave\tnoticed\tthe\tcall\tto\tplot_state_borders(),\ta\tfunction\tthat\twe\thaven\u2019t\tactually\tdefined.\nThere\u2019s\tan\timplementation\ton\tthe\tbook\u2019s\tGitHub\tpage,\tbut\tit\u2019s\ta\tgood\texercise\tto\ttry\tto\tdo\tit\tyourself:\n\n1.\t Search\tthe\tWeb\tfor\tsomething\tlike\tstate\tboundaries\tlatitude\tlongitude.\n\n2.\t Convert\twhatever\tdata\tyou\tcan\tfind\tinto\ta\tlist\tof\tsegments\t[(long1,\tlat1),\t(long2,\tlat2)].\n\n3.\t Use\tplt.plot()\tto\tdraw\tthe\tsegments.\n\nSince\tit\tlooks\tlike\tnearby\tplaces\ttend\tto\tlike\tthe\tsame\tlanguage,\tk-nearest\tneighbors\tseems\nlike\ta\treasonable\tchoice\tfor\ta\tpredictive\tmodel.\n\nTo\tstart\twith,\tlet\u2019s\tlook\tat\twhat\thappens\tif\twe\ttry\tto\tpredict\teach\tcity\u2019s\tpreferred\tlanguage\nusing\tits\tneighbors\tother\tthan\titself:\n\n#\ttry\tseveral\tdifferent\tvalues\tfor\tk\nfor\tk\tin\t[1,\t3,\t5,\t7]:\n\t\t\t\tnum_correct\t=\t0\n\n\t\t\t\tfor\tcity\tin\tcities:\n\t\t\t\t\t\t\t\tlocation,\tactual_language\t=\tcity\n\t\t\t\t\t\t\t\tother_cities\t=\t[other_city\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tother_city\tin\tcities\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tother_city\t!=\tcity]\n\n\t\t\t\t\t\t\t\tpredicted_language\t=\tknn_classify(k,\tother_cities,\tlocation)",
    "223": "if\tpredicted_language\t==\tactual_language:\n\t\t\t\t\t\t\t\t\t\t\t\tnum_correct\t+=\t1\n\n\t\t\t\tprint\tk,\t\"neighbor[s]:\",\tnum_correct,\t\"correct\tout\tof\",\tlen(cities)\n\nIt\tlooks\tlike\t3-nearest\tneighbors\tperforms\tthe\tbest,\tgiving\tthe\tcorrect\tresult\tabout\t59%\tof\nthe\ttime:\n\n1\tneighbor[s]:\t40\tcorrect\tout\tof\t75\n3\tneighbor[s]:\t44\tcorrect\tout\tof\t75\n5\tneighbor[s]:\t41\tcorrect\tout\tof\t75\n7\tneighbor[s]:\t35\tcorrect\tout\tof\t75\n\nNow\twe\tcan\tlook\tat\twhat\tregions\twould\tget\tclassified\tto\twhich\tlanguages\tunder\teach\nnearest\tneighbors\tscheme.\tWe\tcan\tdo\tthat\tby\tclassifying\tan\tentire\tgrid\tworth\tof\tpoints,\nand\tthen\tplotting\tthem\tas\twe\tdid\tthe\tcities:\n\nplots\t=\t{\t\"Java\"\t:\t([],\t[]),\t\"Python\"\t:\t([],\t[]),\t\"R\"\t:\t([],\t[])\t}\n\nk\t=\t1\t#\tor\t3,\tor\t5,\tor\u2026\n\nfor\tlongitude\tin\trange(-130,\t-60):\n\t\t\t\tfor\tlatitude\tin\trange(20,\t55):\n\t\t\t\t\t\t\t\tpredicted_language\t=\tknn_classify(k,\tcities,\t[longitude,\tlatitude])\n\t\t\t\t\t\t\t\tplots[predicted_language][0].append(longitude)\n\t\t\t\t\t\t\t\tplots[predicted_language][1].append(latitude)\n\nFor\tinstance,\tFigure\t12-2\tshows\twhat\thappens\twhen\twe\tlook\tat\tjust\tthe\tnearest\tneighbor\n(k\t=\t1).\n\nWe\tsee\tlots\tof\tabrupt\tchanges\tfrom\tone\tlanguage\tto\tanother\twith\tsharp\tboundaries.\tAs\twe\nincrease\tthe\tnumber\tof\tneighbors\tto\tthree,\twe\tsee\tsmoother\tregions\tfor\teach\tlanguage\n(Figure\t12-3).\n\nAnd\tas\twe\tincrease\tthe\tneighbors\tto\tfive,\tthe\tboundaries\tget\tsmoother\tstill\t(Figure\t12-4).\n\nHere\tour\tdimensions\tare\troughly\tcomparable,\tbut\tif\tthey\tweren\u2019t\tyou\tmight\twant\tto\nrescale\tthe\tdata\tas\twe\tdid\tin\t\u201cRescaling\u201d.",
    "224": "Figure\t12-2.\t1-Nearest\tneighbor\tprogramming\tlanguages",
    "225": "The\tCurse\tof\tDimensionality\n\nk-nearest\tneighbors\truns\tinto\ttrouble\tin\thigher\tdimensions\tthanks\tto\tthe\t\u201ccurse\tof\ndimensionality,\u201d\twhich\tboils\tdown\tto\tthe\tfact\tthat\thigh-dimensional\tspaces\tare\tvast.\nPoints\tin\thigh-dimensional\tspaces\ttend\tnot\tto\tbe\tclose\tto\tone\tanother\tat\tall.\tOne\tway\tto\nsee\tthis\tis\tby\trandomly\tgenerating\tpairs\tof\tpoints\tin\tthe\td-dimensional\t\u201cunit\tcube\u201d\tin\ta\nvariety\tof\tdimensions,\tand\tcalculating\tthe\tdistances\tbetween\tthem.\n\nFigure\t12-3.\t3-Nearest\tneighbor\tprogramming\tlanguages\n\nGenerating\trandom\tpoints\tshould\tbe\tsecond\tnature\tby\tnow:\n\ndef\trandom_point(dim):\n\t\t\t\treturn\t[random.random()\tfor\t_\tin\trange(dim)]\n\nas\tis\twriting\ta\tfunction\tto\tgenerate\tthe\tdistances:\n\ndef\trandom_distances(dim,\tnum_pairs):\n\t\t\t\treturn\t[distance(random_point(dim),\trandom_point(dim))\n\t\t\t\t\t\t\t\t\t\t\t\tfor\t_\tin\trange(num_pairs)]",
    "226": "Figure\t12-4.\t5-Nearest\tneighbor\tprogramming\tlanguages\n\nFor\tevery\tdimension\tfrom\t1\tto\t100,\twe\u2019ll\tcompute\t10,000\tdistances\tand\tuse\tthose\tto\ncompute\tthe\taverage\tdistance\tbetween\tpoints\tand\tthe\tminimum\tdistance\tbetween\tpoints\tin\neach\tdimension\t(Figure\t12-5):\n\ndimensions\t=\trange(1,\t101)\n\navg_distances\t=\t[]\nmin_distances\t=\t[]\n\nrandom.seed(0)\nfor\tdim\tin\tdimensions:\n\t\tdistances\t=\trandom_distances(dim,\t10000)\t\t#\t10,000\trandom\tpairs\n\t\tavg_distances.append(mean(distances))\t\t\t\t\t#\ttrack\tthe\taverage\n\t\tmin_distances.append(min(distances))\t\t\t\t\t\t#\ttrack\tthe\tminimum",
    "227": "Figure\t12-5.\tThe\tcurse\tof\tdimensionality\n\nAs\tthe\tnumber\tof\tdimensions\tincreases,\tthe\taverage\tdistance\tbetween\tpoints\tincreases.\nBut\twhat\u2019s\tmore\tproblematic\tis\tthe\tratio\tbetween\tthe\tclosest\tdistance\tand\tthe\taverage\ndistance\t(Figure\t12-6):\n\nmin_avg_ratio\t=\t[min_dist\t/\tavg_dist\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tmin_dist,\tavg_dist\tin\tzip(min_distances,\tavg_distances)]",
    "228": "Figure\t12-6.\tThe\tcurse\tof\tdimensionality\tagain\n\nIn\tlow-dimensional\tdata\tsets,\tthe\tclosest\tpoints\ttend\tto\tbe\tmuch\tcloser\tthan\taverage.\tBut\ntwo\tpoints\tare\tclose\tonly\tif\tthey\u2019re\tclose\tin\tevery\tdimension,\tand\tevery\textra\tdimension\n\u2014\teven\tif\tjust\tnoise\t\u2014\tis\tanother\topportunity\tfor\teach\tpoint\tto\tbe\tfurther\taway\tfrom\nevery\tother\tpoint.\tWhen\tyou\thave\ta\tlot\tof\tdimensions,\tit\u2019s\tlikely\tthat\tthe\tclosest\tpoints\naren\u2019t\tmuch\tcloser\tthan\taverage,\twhich\tmeans\tthat\ttwo\tpoints\tbeing\tclose\tdoesn\u2019t\tmean\nvery\tmuch\t(unless\tthere\u2019s\ta\tlot\tof\tstructure\tin\tyour\tdata\tthat\tmakes\tit\tbehave\tas\tif\tit\twere\nmuch\tlower-dimensional).\n\nA\tdifferent\tway\tof\tthinking\tabout\tthe\tproblem\tinvolves\tthe\tsparsity\tof\thigher-dimensional\nspaces.\n\nIf\tyou\tpick\t50\trandom\tnumbers\tbetween\t0\tand\t1,\tyou\u2019ll\tprobably\tget\ta\tpretty\tgood\tsample\nof\tthe\tunit\tinterval\t(Figure\t12-7).",
    "229": "Figure\t12-7.\tFifty\trandom\tpoints\tin\tone\tdimension\n\nIf\tyou\tpick\t50\trandom\tpoints\tin\tthe\tunit\tsquare,\tyou\u2019ll\tget\tless\tcoverage\t(Figure\t12-8).",
    "230": "Figure\t12-8.\tFifty\trandom\tpoints\tin\ttwo\tdimensions\n\nAnd\tin\tthree\tdimensions\tless\tstill\t(Figure\t12-9).\n\nmatplotlib\tdoesn\u2019t\tgraph\tfour\tdimensions\twell,\tso\tthat\u2019s\tas\tfar\tas\twe\u2019ll\tgo,\tbut\tyou\tcan\nsee\talready\tthat\tthere\tare\tstarting\tto\tbe\tlarge\tempty\tspaces\twith\tno\tpoints\tnear\tthem.\tIn\nmore\tdimensions\t\u2014\tunless\tyou\tget\texponentially\tmore\tdata\t\u2014\tthose\tlarge\tempty\tspaces\nrepresent\tregions\tfar\tfrom\tall\tthe\tpoints\tyou\twant\tto\tuse\tin\tyour\tpredictions.\n\nSo\tif\tyou\u2019re\ttrying\tto\tuse\tnearest\tneighbors\tin\thigher\tdimensions,\tit\u2019s\tprobably\ta\tgood\tidea\nto\tdo\tsome\tkind\tof\tdimensionality\treduction\tfirst.",
    "231": "Figure\t12-9.\tFifty\trandom\tpoints\tin\tthree\tdimensions",
    "232": "For\tFurther\tExploration\n\nscikit-learn\thas\tmany\tnearest\tneighbor\tmodels.",
    "233": "",
    "234": "Chapter\t13.\tNaive\tBayes\n\nIt\tis\twell\tfor\tthe\theart\tto\tbe\tnaive\tand\tfor\tthe\tmind\tnot\tto\tbe.\n\nAnatole\tFrance\n\nA\tsocial\tnetwork\tisn\u2019t\tmuch\tgood\tif\tpeople\tcan\u2019t\tnetwork.\tAccordingly,\tDataSciencester\nhas\ta\tpopular\tfeature\tthat\tallows\tmembers\tto\tsend\tmessages\tto\tother\tmembers.\tAnd\twhile\nmost\tof\tyour\tmembers\tare\tresponsible\tcitizens\twho\tsend\tonly\twell-received\t\u201chow\u2019s\tit\ngoing?\u201d\tmessages,\ta\tfew\tmiscreants\tpersistently\tspam\tother\tmembers\tabout\tget-rich\nschemes,\tno-prescription-required\tpharmaceuticals,\tand\tfor-profit\tdata\tscience\ncredentialing\tprograms.\tYour\tusers\thave\tbegun\tto\tcomplain,\tand\tso\tthe\tVP\tof\tMessaging\nhas\tasked\tyou\tto\tuse\tdata\tscience\tto\tfigure\tout\ta\tway\tto\tfilter\tout\tthese\tspam\tmessages.",
    "235": "A\tReally\tDumb\tSpam\tFilter\n\nImagine\ta\t\u201cuniverse\u201d\tthat\tconsists\tof\treceiving\ta\tmessage\tchosen\trandomly\tfrom\tall\npossible\tmessages.\tLet\tS\tbe\tthe\tevent\t\u201cthe\tmessage\tis\tspam\u201d\tand\tV\tbe\tthe\tevent\t\u201cthe\nmessage\tcontains\tthe\tword\tviagra.\u201d\tThen\tBayes\u2019s\tTheorem\ttells\tus\tthat\tthe\tprobability\nthat\tthe\tmessage\tis\tspam\tconditional\ton\tcontaining\tthe\tword\tviagra\tis:\n\nThe\tnumerator\tis\tthe\tprobability\tthat\ta\tmessage\tis\tspam\tand\tcontains\tviagra,\twhile\tthe\ndenominator\tis\tjust\tthe\tprobability\tthat\ta\tmessage\tcontains\tviagra.\tHence\tyou\tcan\tthink\tof\nthis\tcalculation\tas\tsimply\trepresenting\tthe\tproportion\tof\tviagra\tmessages\tthat\tare\tspam.\n\nIf\twe\thave\ta\tlarge\tcollection\tof\tmessages\twe\tknow\tare\tspam,\tand\ta\tlarge\tcollection\tof\n\nmessages\twe\tknow\tare\tnot\tspam,\tthen\twe\tcan\teasily\testimate\t\n\n\tand\t\n\n.\tIf\twe\tfurther\tassume\tthat\tany\tmessage\tis\tequally\tlikely\tto\tbe\tspam\tor\tnot-\n\nspam\t(so\tthat\t\n\n),\tthen:\n\nFor\texample,\tif\t50%\tof\tspam\tmessages\thave\tthe\tword\tviagra,\tbut\tonly\t1%\tof\tnonspam\nmessages\tdo,\tthen\tthe\tprobability\tthat\tany\tgiven\tviagra-containing\temail\tis\tspam\tis:",
    "236": "A\tMore\tSophisticated\tSpam\tFilter\n\nImagine\tnow\tthat\twe\thave\ta\tvocabulary\tof\tmany\twords\t\nthe\trealm\tof\tprobability\ttheory,\twe\u2019ll\twrite\t\n\n\tfor\tthe\tevent\t\u201ca\tmessage\tcontains\tthe\tword\t\n\n.\tTo\tmove\tthis\tinto\n\n.\u201d\tAlso\timagine\tthat\t(through\tsome\tunspecified-at-this-point\tprocess)\twe\u2019ve\tcome\tup\n\nwith\tan\testimate\t\nand\ta\tsimilar\testimate\t\nthe\tith\tword.\n\n\tfor\tthe\tprobability\tthat\ta\tspam\tmessage\tcontains\tthe\tith\tword,\n\tfor\tthe\tprobability\tthat\ta\tnonspam\tmessage\tcontains\n\nThe\tkey\tto\tNaive\tBayes\tis\tmaking\tthe\t(big)\tassumption\tthat\tthe\tpresences\t(or\tabsences)\tof\neach\tword\tare\tindependent\tof\tone\tanother,\tconditional\ton\ta\tmessage\tbeing\tspam\tor\tnot.\nIntuitively,\tthis\tassumption\tmeans\tthat\tknowing\twhether\ta\tcertain\tspam\tmessage\tcontains\nthe\tword\t\u201cviagra\u201d\tgives\tyou\tno\tinformation\tabout\twhether\tthat\tsame\tmessage\tcontains\tthe\nword\t\u201crolex.\u201d\tIn\tmath\tterms,\tthis\tmeans\tthat:\n\nThis\tis\tan\textreme\tassumption.\t(There\u2019s\ta\treason\tthe\ttechnique\thas\t\u201cnaive\u201d\tin\tits\tname.)\nImagine\tthat\tour\tvocabulary\tconsists\tonly\tof\tthe\twords\t\u201cviagra\u201d\tand\t\u201crolex,\u201d\tand\tthat\thalf\nof\tall\tspam\tmessages\tare\tfor\t\u201ccheap\tviagra\u201d\tand\tthat\tthe\tother\thalf\tare\tfor\t\u201cauthentic\nrolex.\u201d\tIn\tthis\tcase,\tthe\tNaive\tBayes\testimate\tthat\ta\tspam\tmessage\tcontains\tboth\t\u201cviagra\u201d\nand\t\u201crolex\u201d\tis:\n\nsince\twe\u2019ve\tassumed\taway\tthe\tknowledge\tthat\t\u201cviagra\u201d\tand\t\u201crolex\u201d\tactually\tnever\toccur\ntogether.\tDespite\tthe\tunrealisticness\tof\tthis\tassumption,\tthis\tmodel\toften\tperforms\twell\nand\tis\tused\tin\tactual\tspam\tfilters.\n\nThe\tsame\tBayes\u2019s\tTheorem\treasoning\twe\tused\tfor\tour\t\u201cviagra-only\u201d\tspam\tfilter\ttells\tus\nthat\twe\tcan\tcalculate\tthe\tprobability\ta\tmessage\tis\tspam\tusing\tthe\tequation:\n\nThe\tNaive\tBayes\tassumption\tallows\tus\tto\tcompute\teach\tof\tthe\tprobabilities\ton\tthe\tright\nsimply\tby\tmultiplying\ttogether\tthe\tindividual\tprobability\testimates\tfor\teach\tvocabulary\nword.\n\nIn\tpractice,\tyou\tusually\twant\tto\tavoid\tmultiplying\tlots\tof\tprobabilities\ttogether,\tto\tavoid\ta\nproblem\tcalled\tunderflow,\tin\twhich\tcomputers\tdon\u2019t\tdeal\twell\twith\tfloating-point\tnumbers\nthat\tare\ttoo\tclose\tto\tzero.\tRecalling\tfrom\talgebra\tthat\t\n\ncompute\t\n\n\tas\tthe\tequivalent\t(but\tfloating-point-friendlier):\n\n\tand\tthat\t\n\n,\twe\tusually",
    "237": ",\tthe\nThe\tonly\tchallenge\tleft\tis\tcoming\tup\twith\testimates\tfor\t\nprobabilities\tthat\ta\tspam\tmessage\t(or\tnonspam\tmessage)\tcontains\tthe\tword\t\n.\tIf\twe\thave\na\tfair\tnumber\tof\t\u201ctraining\u201d\tmessages\tlabeled\tas\tspam\tand\tnot-spam,\tan\tobvious\tfirst\ttry\tis\nto\testimate\t\n\n\tsimply\tas\tthe\tfraction\tof\tspam\tmessages\tcontaining\tword\t\n\n\tand\t\n\n.\n\nThis\tcauses\ta\tbig\tproblem,\tthough.\tImagine\tthat\tin\tour\ttraining\tset\tthe\tvocabulary\tword\n\n\u201cdata\u201d\tonly\toccurs\tin\tnonspam\tmessages.\tThen\twe\u2019d\testimate\t\n.\nThe\tresult\tis\tthat\tour\tNaive\tBayes\tclassifier\twould\talways\tassign\tspam\tprobability\t0\tto\nany\tmessage\tcontaining\tthe\tword\t\u201cdata,\u201d\teven\ta\tmessage\tlike\t\u201cdata\ton\tcheap\tviagra\tand\nauthentic\trolex\twatches.\u201d\tTo\tavoid\tthis\tproblem,\twe\tusually\tuse\tsome\tkind\tof\tsmoothing.\n\nIn\tparticular,\twe\u2019ll\tchoose\ta\tpseudocount\t\u2014\tk\t\u2014\tand\testimate\tthe\tprobability\tof\tseeing\nthe\tith\tword\tin\ta\tspam\tas:\n\nSimilarly\tfor\t\n.\tThat\tis,\twhen\tcomputing\tthe\tspam\tprobabilities\tfor\tthe\tith\nword,\twe\tassume\twe\talso\tsaw\tk\tadditional\tspams\tcontaining\tthe\tword\tand\tk\tadditional\nspams\tnot\tcontaining\tthe\tword.\n\nFor\texample,\tif\t\u201cdata\u201d\toccurs\tin\t0/98\tspam\tdocuments,\tand\tif\tk\tis\t1,\twe\testimate\t\n\n\tas\t1/100\t=\t0.01,\twhich\tallows\tour\tclassifier\tto\tstill\tassign\tsome\tnonzero\n\nspam\tprobability\tto\tmessages\tthat\tcontain\tthe\tword\t\u201cdata.\u201d",
    "238": "Implementation\n\nNow\twe\thave\tall\tthe\tpieces\twe\tneed\tto\tbuild\tour\tclassifier.\tFirst,\tlet\u2019s\tcreate\ta\tsimple\nfunction\tto\ttokenize\tmessages\tinto\tdistinct\twords.\tWe\u2019ll\tfirst\tconvert\teach\tmessage\tto\nlowercase;\tuse\tre.findall()\tto\textract\t\u201cwords\u201d\tconsisting\tof\tletters,\tnumbers,\tand\napostrophes;\tand\tfinally\tuse\tset()\tto\tget\tjust\tthe\tdistinct\twords:\n\ndef\ttokenize(message):\n\t\t\t\tmessage\t=\tmessage.lower()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tconvert\tto\tlowercase\n\t\t\t\tall_words\t=\tre.findall(\"[a-z0-9']+\",\tmessage)\t\t\t#\textract\tthe\twords\n\t\t\t\treturn\tset(all_words)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tremove\tduplicates\n\nOur\tsecond\tfunction\twill\tcount\tthe\twords\tin\ta\tlabeled\ttraining\tset\tof\tmessages.\tWe\u2019ll\thave\nit\treturn\ta\tdictionary\twhose\tkeys\tare\twords,\tand\twhose\tvalues\tare\ttwo-element\tlists\n[spam_count,\tnon_spam_count]\tcorresponding\tto\thow\tmany\ttimes\twe\tsaw\tthat\tword\tin\nboth\tspam\tand\tnonspam\tmessages:\n\ndef\tcount_words(training_set):\n\t\t\t\t\"\"\"training\tset\tconsists\tof\tpairs\t(message,\tis_spam)\"\"\"\n\t\t\t\tcounts\t=\tdefaultdict(lambda:\t[0,\t0])\n\t\t\t\tfor\tmessage,\tis_spam\tin\ttraining_set:\n\t\t\t\t\t\t\t\tfor\tword\tin\ttokenize(message):\n\t\t\t\t\t\t\t\t\t\t\t\tcounts[word][0\tif\tis_spam\telse\t1]\t+=\t1\n\t\t\t\treturn\tcounts\n\nOur\tnext\tstep\tis\tto\tturn\tthese\tcounts\tinto\testimated\tprobabilities\tusing\tthe\tsmoothing\twe\ndescribed\tbefore.\tOur\tfunction\twill\treturn\ta\tlist\tof\ttriplets\tcontaining\teach\tword,\tthe\nprobability\tof\tseeing\tthat\tword\tin\ta\tspam\tmessage,\tand\tthe\tprobability\tof\tseeing\tthat\tword\nin\ta\tnonspam\tmessage:\n\ndef\tword_probabilities(counts,\ttotal_spams,\ttotal_non_spams,\tk=0.5):\n\t\t\t\t\"\"\"turn\tthe\tword_counts\tinto\ta\tlist\tof\ttriplets\n\t\t\t\tw,\tp(w\t|\tspam)\tand\tp(w\t|\t~spam)\"\"\"\n\t\t\t\treturn\t[(w,\n\t\t\t\t\t\t\t\t\t\t\t\t\t(spam\t+\tk)\t/\t(total_spams\t+\t2\t*\tk),\n\t\t\t\t\t\t\t\t\t\t\t\t\t(non_spam\t+\tk)\t/\t(total_non_spams\t+\t2\t*\tk))\n\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tw,\t(spam,\tnon_spam)\tin\tcounts.iteritems()]\n\nThe\tlast\tpiece\tis\tto\tuse\tthese\tword\tprobabilities\t(and\tour\tNaive\tBayes\tassumptions)\tto\nassign\tprobabilities\tto\tmessages:\n\ndef\tspam_probability(word_probs,\tmessage):\n\t\t\t\tmessage_words\t=\ttokenize(message)\n\t\t\t\tlog_prob_if_spam\t=\tlog_prob_if_not_spam\t=\t0.0\n\n\t\t\t\t#\titerate\tthrough\teach\tword\tin\tour\tvocabulary\n\t\t\t\tfor\tword,\tprob_if_spam,\tprob_if_not_spam\tin\tword_probs:\n\n\t\t\t\t\t\t\t\t#\tif\t*word*\tappears\tin\tthe\tmessage,\n\t\t\t\t\t\t\t\t#\tadd\tthe\tlog\tprobability\tof\tseeing\tit\n\t\t\t\t\t\t\t\tif\tword\tin\tmessage_words:\n\t\t\t\t\t\t\t\t\t\t\t\tlog_prob_if_spam\t+=\tmath.log(prob_if_spam)\n\t\t\t\t\t\t\t\t\t\t\t\tlog_prob_if_not_spam\t+=\tmath.log(prob_if_not_spam)\n\n\t\t\t\t\t\t\t\t#\tif\t*word*\tdoesn't\tappear\tin\tthe\tmessage\n\t\t\t\t\t\t\t\t#\tadd\tthe\tlog\tprobability\tof\t_not_\tseeing\tit\n\t\t\t\t\t\t\t\t#\twhich\tis\tlog(1\t-\tprobability\tof\tseeing\tit)\n\t\t\t\t\t\t\t\telse:",
    "239": "log_prob_if_spam\t+=\tmath.log(1.0\t-\tprob_if_spam)\n\t\t\t\t\t\t\t\t\t\t\t\tlog_prob_if_not_spam\t+=\tmath.log(1.0\t-\tprob_if_not_spam)\n\n\t\t\t\tprob_if_spam\t=\tmath.exp(log_prob_if_spam)\n\t\t\t\tprob_if_not_spam\t=\tmath.exp(log_prob_if_not_spam)\n\t\t\t\treturn\tprob_if_spam\t/\t(prob_if_spam\t+\tprob_if_not_spam)\n\nWe\tcan\tput\tthis\tall\ttogether\tinto\tour\tNaive\tBayes\tClassifier:\n\nclass\tNaiveBayesClassifier:\n\n\t\t\t\tdef\t__init__(self,\tk=0.5):\n\t\t\t\t\t\t\t\tself.k\t=\tk\n\t\t\t\t\t\t\t\tself.word_probs\t=\t[]\n\n\t\t\t\tdef\ttrain(self,\ttraining_set):\n\n\t\t\t\t\t\t\t\t#\tcount\tspam\tand\tnon-spam\tmessages\n\t\t\t\t\t\t\t\tnum_spams\t=\tlen([is_spam\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tmessage,\tis_spam\tin\ttraining_set\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tis_spam])\n\t\t\t\t\t\t\t\tnum_non_spams\t=\tlen(training_set)\t-\tnum_spams\n\n\t\t\t\t\t\t\t\t#\trun\ttraining\tdata\tthrough\tour\t\"pipeline\"\n\t\t\t\t\t\t\t\tword_counts\t=\tcount_words(training_set)\n\t\t\t\t\t\t\t\tself.word_probs\t=\tword_probabilities(word_counts,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnum_spams,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnum_non_spams,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.k)\n\n\t\t\t\tdef\tclassify(self,\tmessage):\n\t\t\t\t\t\t\t\treturn\tspam_probability(self.word_probs,\tmessage)",
    "240": "Testing\tOur\tModel\n\nA\tgood\t(if\tsomewhat\told)\tdata\tset\tis\tthe\tSpamAssassin\tpublic\tcorpus.\tWe\u2019ll\tlook\tat\tthe\nfiles\tprefixed\twith\t20021010.\t(On\tWindows,\tyou\tmight\tneed\ta\tprogram\tlike\t7-Zip\tto\ndecompress\tand\textract\tthem.)\n\nAfter\textracting\tthe\tdata\t(to,\tsay,\tC:\\spam)\tyou\tshould\thave\tthree\tfolders:\tspam,\neasy_ham,\tand\thard_ham.\tEach\tfolder\tcontains\tmany\temails,\teach\tcontained\tin\ta\tsingle\nfile.\tTo\tkeep\tthings\treally\tsimple,\twe\u2019ll\tjust\tlook\tat\tthe\tsubject\tlines\tof\teach\temail.\n\nHow\tdo\twe\tidentify\tthe\tsubject\tline?\tLooking\tthrough\tthe\tfiles,\tthey\tall\tseem\tto\tstart\twith\n\u201cSubject:\u201d.\tSo\twe\u2019ll\tlook\tfor\tthat:\n\nimport\tglob,\tre\n\n#\tmodify\tthe\tpath\twith\twherever\tyou've\tput\tthe\tfiles\npath\t=\tr\"C:\\spam\\*\\*\"\n\ndata\t=\t[]\n\n#\tglob.glob\treturns\tevery\tfilename\tthat\tmatches\tthe\twildcarded\tpath\nfor\tfn\tin\tglob.glob(path):\n\t\t\t\tis_spam\t=\t\"ham\"\tnot\tin\tfn\n\n\t\t\t\twith\topen(fn,'r')\tas\tfile:\n\t\t\t\t\t\t\t\tfor\tline\tin\tfile:\n\t\t\t\t\t\t\t\t\t\t\t\tif\tline.startswith(\"Subject:\"):\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tremove\tthe\tleading\t\"Subject:\t\"\tand\tkeep\twhat's\tleft\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsubject\t=\tre.sub(r\"^Subject:\t\",\t\"\",\tline).strip()\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata.append((subject,\tis_spam))\n\nNow\twe\tcan\tsplit\tthe\tdata\tinto\ttraining\tdata\tand\ttest\tdata,\tand\tthen\twe\u2019re\tready\tto\tbuild\ta\nclassifier:\n\nrandom.seed(0)\t\t\t\t\t\t#\tjust\tso\tyou\tget\tthe\tsame\tanswers\tas\tme\ntrain_data,\ttest_data\t=\tsplit_data(data,\t0.75)\n\nclassifier\t=\tNaiveBayesClassifier()\nclassifier.train(train_data)\n\nAnd\tthen\twe\tcan\tcheck\thow\tour\tmodel\tdoes:\n\n#\ttriplets\t(subject,\tactual\tis_spam,\tpredicted\tspam\tprobability)\nclassified\t=\t[(subject,\tis_spam,\tclassifier.classify(subject))\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tsubject,\tis_spam\tin\ttest_data]\n\n#\tassume\tthat\tspam_probability\t>\t0.5\tcorresponds\tto\tspam\tprediction\n#\tand\tcount\tthe\tcombinations\tof\t(actual\tis_spam,\tpredicted\tis_spam)\ncounts\t=\tCounter((is_spam,\tspam_probability\t>\t0.5)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\t_,\tis_spam,\tspam_probability\tin\tclassified)\n\nThis\tgives\t101\ttrue\tpositives\t(spam\tclassified\tas\t\u201cspam\u201d),\t33\tfalse\tpositives\t(ham\nclassified\tas\t\u201cspam\u201d),\t704\ttrue\tnegatives\t(ham\tclassified\tas\t\u201cham\u201d),\tand\t38\tfalse\tnegatives\n(spam\tclassified\tas\t\u201cham\u201d).\tThis\tmeans\tour\tprecision\tis\t101\t/\t(101\t+\t33)\t=\t75%,\tand\tour\nrecall\tis\t101\t/\t(101\t+\t38)\t=\t73%,\twhich\tare\tnot\tbad\tnumbers\tfor\tsuch\ta\tsimple\tmodel.\n\nIt\u2019s\talso\tinteresting\tto\tlook\tat\tthe\tmost\tmisclassified:",
    "241": "#\tsort\tby\tspam_probability\tfrom\tsmallest\tto\tlargest\nclassified.sort(key=lambda\trow:\trow[2])\n\n#\tthe\thighest\tpredicted\tspam\tprobabilities\tamong\tthe\tnon-spams\nspammiest_hams\t=\tfilter(lambda\trow:\tnot\trow[1],\tclassified)[-5:]\n\n#\tthe\tlowest\tpredicted\tspam\tprobabilities\tamong\tthe\tactual\tspams\nhammiest_spams\t=\tfilter(lambda\trow:\trow[1],\tclassified)[:5]\n\nThe\ttwo\tspammiest\thams\tboth\thave\tthe\twords\t\u201cneeded\u201d\t(77\ttimes\tmore\tlikely\tto\tappear\tin\nspam),\t\u201cinsurance\u201d\t(30\ttimes\tmore\tlikely\tto\tappear\tin\tspam),\tand\t\u201cimportant\u201d\t(10\ttimes\nmore\tlikely\tto\tappear\tin\tspam).\n\nThe\thammiest\tspam\tis\ttoo\tshort\t(\u201cRe:\tgirls\u201d)\tto\tmake\tmuch\tof\ta\tjudgment,\tand\tthe\nsecond-hammiest\tis\ta\tcredit\tcard\tsolicitation\tmost\tof\twhose\twords\tweren\u2019t\tin\tthe\ttraining\nset.\n\nWe\tcan\tsimilarly\tlook\tat\tthe\tspammiest\twords:\n\ndef\tp_spam_given_word(word_prob):\n\t\t\t\t\"\"\"uses\tbayes's\ttheorem\tto\tcompute\tp(spam\t|\tmessage\tcontains\tword)\"\"\"\n\n\t\t\t\t#\tword_prob\tis\tone\tof\tthe\ttriplets\tproduced\tby\tword_probabilities\n\t\t\t\tword,\tprob_if_spam,\tprob_if_not_spam\t=\tword_prob\n\t\t\t\treturn\tprob_if_spam\t/\t(prob_if_spam\t+\tprob_if_not_spam)\n\nwords\t=\tsorted(classifier.word_probs,\tkey=p_spam_given_word)\n\nspammiest_words\t=\twords[-5:]\nhammiest_words\t=\twords[:5]\n\nThe\tspammiest\twords\tare\t\u201cmoney,\u201d\t\u201csystemworks,\u201d\t\u201crates,\u201d\t\u201csale,\u201d\tand\t\u201cyear,\u201d\tall\tof\nwhich\tseem\trelated\tto\ttrying\tto\tget\tpeople\tto\tbuy\tthings.\tAnd\tthe\thammiest\twords\tare\n\u201cspambayes,\u201d\t\u201cusers,\u201d\t\u201crazor,\u201d\t\u201czzzzteana,\u201d\tand\t\u201csadev,\u201d\tmost\tof\twhich\tseem\trelated\tto\nspam\tprevention,\toddly\tenough.\n\nHow\tcould\twe\tget\tbetter\tperformance?\tOne\tobvious\tway\twould\tbe\tto\tget\tmore\tdata\tto\ntrain\ton.\tThere\tare\ta\tnumber\tof\tways\tto\timprove\tthe\tmodel\tas\twell.\tHere\tare\tsome\npossibilities\tthat\tyou\tmight\ttry:\n\nLook\tat\tthe\tmessage\tcontent,\tnot\tjust\tthe\tsubject\tline.\tYou\u2019ll\thave\tto\tbe\tcareful\thow\nyou\tdeal\twith\tthe\tmessage\theaders.\n\nOur\tclassifier\ttakes\tinto\taccount\tevery\tword\tthat\tappears\tin\tthe\ttraining\tset,\teven\twords\nthat\tappear\tonly\tonce.\tModify\tthe\tclassifier\tto\taccept\tan\toptional\tmin_count\tthreshhold\nand\tignore\ttokens\tthat\tdon\u2019t\tappear\tat\tleast\tthat\tmany\ttimes.\n\nThe\ttokenizer\thas\tno\tnotion\tof\tsimilar\twords\t(e.g.,\t\u201ccheap\u201d\tand\t\u201ccheapest\u201d).\tModify\nthe\tclassifier\tto\ttake\tan\toptional\tstemmer\tfunction\tthat\tconverts\twords\tto\tequivalence\nclasses\tof\twords.\tFor\texample,\ta\treally\tsimple\tstemmer\tfunction\tmight\tbe:\n\ndef\tdrop_final_s(word):\n\t\t\t\treturn\tre.sub(\"s$\",\t\"\",\tword)",
    "242": "Creating\ta\tgood\tstemmer\tfunction\tis\thard.\tPeople\tfrequently\tuse\tthe\tPorter\tStemmer.\n\nAlthough\tour\tfeatures\tare\tall\tof\tthe\tform\t\u201cmessage\tcontains\tword\t\nreason\twhy\tthis\thas\tto\tbe\tthe\tcase.\tIn\tour\timplementation,\twe\tcould\tadd\textra\tfeatures\nlike\t\u201cmessage\tcontains\ta\tnumber\u201d\tby\tcreating\tphony\ttokens\tlike\tcontains:number\tand\nmodifying\tthe\ttokenizer\tto\temit\tthem\twhen\tappropriate.\n\n,\u201d\tthere\u2019s\tno",
    "243": "For\tFurther\tExploration\n\nPaul\tGraham\u2019s\tarticles\t\u201cA\tPlan\tfor\tSpam\u201d\tand\t\u201cBetter\tBayesian\tFiltering\u201d\t(are\ninteresting\tand)\tgive\tmore\tinsight\tinto\tthe\tideas\tbehind\tbuilding\tspam\tfilters.\n\nscikit-learn\tcontains\ta\tBernoulliNB\tmodel\tthat\timplements\tthe\tsame\tNaive\tBayes\nalgorithm\twe\timplemented\there,\tas\twell\tas\tother\tvariations\ton\tthe\tmodel.",
    "244": "",
    "245": "Chapter\t14.\tSimple\tLinear\tRegression\n\nArt,\tlike\tmorality,\tconsists\tin\tdrawing\tthe\tline\tsomewhere.\n\nG.\tK.\tChesterton\n\nIn\tChapter\t5,\twe\tused\tthe\tcorrelation\tfunction\tto\tmeasure\tthe\tstrength\tof\tthe\tlinear\nrelationship\tbetween\ttwo\tvariables.\tFor\tmost\tapplications,\tknowing\tthat\tsuch\ta\tlinear\nrelationship\texists\tisn\u2019t\tenough.\tWe\u2019ll\twant\tto\tbe\table\tto\tunderstand\tthe\tnature\tof\tthe\nrelationship.\tThis\tis\twhere\twe\u2019ll\tuse\tsimple\tlinear\tregression.",
    "246": "The\tModel\n\nRecall\tthat\twe\twere\tinvestigating\tthe\trelationship\tbetween\ta\tDataSciencester\tuser\u2019s\nnumber\tof\tfriends\tand\tthe\tamount\tof\ttime\the\tspent\ton\tthe\tsite\teach\tday.\tLet\u2019s\tassume\tthat\nyou\u2019ve\tconvinced\tyourself\tthat\thaving\tmore\tfriends\tcauses\tpeople\tto\tspend\tmore\ttime\ton\nthe\tsite,\trather\tthan\tone\tof\tthe\talternative\texplanations\twe\tdiscussed.\n\nThe\tVP\tof\tEngagement\tasks\tyou\tto\tbuild\ta\tmodel\tdescribing\tthis\trelationship.\tSince\tyou\nfound\ta\tpretty\tstrong\tlinear\trelationship,\ta\tnatural\tplace\tto\tstart\tis\ta\tlinear\tmodel.\n\nIn\tparticular,\tyou\thypothesize\tthat\tthere\tare\tconstants\t\n\n\t(alpha)\tand\t\n\n\t(beta)\tsuch\tthat:\n\n\tis\tthe\tnumber\tof\tminutes\tuser\ti\tspends\ton\tthe\tsite\tdaily,\t\n\nwhere\t\nfriends\tuser\ti\thas,\tand\t\nare\tother\tfactors\tnot\taccounted\tfor\tby\tthis\tsimple\tmodel.\n\n\tis\ta\t(hopefully\tsmall)\terror\tterm\trepresenting\tthe\tfact\tthat\tthere\n\n\tis\tthe\tnumber\tof\n\nAssuming\twe\u2019ve\tdetermined\tsuch\tan\talpha\tand\tbeta,\tthen\twe\tmake\tpredictions\tsimply\nwith:\n\ndef\tpredict(alpha,\tbeta,\tx_i):\n\t\t\t\treturn\tbeta\t*\tx_i\t+\talpha\n\nHow\tdo\twe\tchoose\talpha\tand\tbeta?\tWell,\tany\tchoice\tof\talpha\tand\tbeta\tgives\tus\ta\npredicted\toutput\tfor\teach\tinput\tx_i.\tSince\twe\tknow\tthe\tactual\toutput\ty_i\twe\tcan\tcompute\nthe\terror\tfor\teach\tpair:\n\ndef\terror(alpha,\tbeta,\tx_i,\ty_i):\n\t\t\t\t\"\"\"the\terror\tfrom\tpredicting\tbeta\t*\tx_i\t+\talpha\n\t\t\t\twhen\tthe\tactual\tvalue\tis\ty_i\"\"\"\n\t\t\t\treturn\ty_i\t-\tpredict(alpha,\tbeta,\tx_i)\n\nWhat\twe\u2019d\treally\tlike\tto\tknow\tis\tthe\ttotal\terror\tover\tthe\tentire\tdata\tset.\tBut\twe\tdon\u2019t\twant\nto\tjust\tadd\tthe\terrors\t\u2014\tif\tthe\tprediction\tfor\tx_1\tis\ttoo\thigh\tand\tthe\tprediction\tfor\tx_2\tis\ntoo\tlow,\tthe\terrors\tmay\tjust\tcancel\tout.\n\nSo\tinstead\twe\tadd\tup\tthe\tsquared\terrors:\n\ndef\tsum_of_squared_errors(alpha,\tbeta,\tx,\ty):\n\t\t\t\treturn\tsum(error(alpha,\tbeta,\tx_i,\ty_i)\t**\t2\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tx_i,\ty_i\tin\tzip(x,\ty))\n\nThe\tleast\tsquares\tsolution\tis\tto\tchoose\tthe\talpha\tand\tbeta\tthat\tmake\nsum_of_squared_errors\tas\tsmall\tas\tpossible.\n\nUsing\tcalculus\t(or\ttedious\talgebra),\tthe\terror-minimizing\talpha\tand\tbeta\tare\tgiven\tby:\n\ndef\tleast_squares_fit(x,\ty):",
    "247": "\"\"\"given\ttraining\tvalues\tfor\tx\tand\ty,\n\t\t\t\tfind\tthe\tleast-squares\tvalues\tof\talpha\tand\tbeta\"\"\"\n\t\t\t\tbeta\t=\tcorrelation(x,\ty)\t*\tstandard_deviation(y)\t/\tstandard_deviation(x)\n\t\t\t\talpha\t=\tmean(y)\t-\tbeta\t*\tmean(x)\n\t\t\t\treturn\talpha,\tbeta\n\nWithout\tgoing\tthrough\tthe\texact\tmathematics,\tlet\u2019s\tthink\tabout\twhy\tthis\tmight\tbe\ta\nreasonable\tsolution.\tThe\tchoice\tof\talpha\tsimply\tsays\tthat\twhen\twe\tsee\tthe\taverage\tvalue\nof\tthe\tindependent\tvariable\tx,\twe\tpredict\tthe\taverage\tvalue\tof\tthe\tdependent\tvariable\ty.\n\nThe\tchoice\tof\tbeta\tmeans\tthat\twhen\tthe\tinput\tvalue\tincreases\tby\nstandard_deviation(x),\tthe\tprediction\tincreases\tby\tcorrelation(x,\ty)\t*\nstandard_deviation(y).\tIn\tthe\tcase\twhen\tx\tand\ty\tare\tperfectly\tcorrelated,\ta\tone\tstandard\ndeviation\tincrease\tin\tx\tresults\tin\ta\tone-standard-deviation-of-y\tincrease\tin\tthe\tprediction.\nWhen\tthey\u2019re\tperfectly\tanticorrelated,\tthe\tincrease\tin\tx\tresults\tin\ta\tdecrease\tin\tthe\nprediction.\tAnd\twhen\tthe\tcorrelation\tis\tzero,\tbeta\tis\tzero,\twhich\tmeans\tthat\tchanges\tin\tx\ndon\u2019t\taffect\tthe\tprediction\tat\tall.\n\nIt\u2019s\teasy\tto\tapply\tthis\tto\tthe\toutlierless\tdata\tfrom\tChapter\t5:\n\nalpha,\tbeta\t=\tleast_squares_fit(num_friends_good,\tdaily_minutes_good)\n\nThis\tgives\tvalues\tof\talpha\t=\t22.95\tand\tbeta\t=\t0.903.\tSo\tour\tmodel\tsays\tthat\twe\texpect\ta\nuser\twith\tn\tfriends\tto\tspend\t22.95\t+\tn\t*\t0.903\tminutes\ton\tthe\tsite\teach\tday.\tThat\tis,\twe\npredict\tthat\ta\tuser\twith\tno\tfriends\ton\tDataSciencester\twould\tstill\tspend\tabout\t23\tminutes\ta\nday\ton\tthe\tsite.\tAnd\tfor\teach\tadditional\tfriend,\twe\texpect\ta\tuser\tto\tspend\talmost\ta\tminute\nmore\ton\tthe\tsite\teach\tday.\n\nIn\tFigure\t14-1,\twe\tplot\tthe\tprediction\tline\tto\tget\ta\tsense\tof\thow\twell\tthe\tmodel\tfits\tthe\nobserved\tdata.",
    "248": "Figure\t14-1.\tOur\tsimple\tlinear\tmodel\n\nOf\tcourse,\twe\tneed\ta\tbetter\tway\tto\tfigure\tout\thow\twell\twe\u2019ve\tfit\tthe\tdata\tthan\tstaring\tat\nthe\tgraph.\tA\tcommon\tmeasure\tis\tthe\tcoefficient\tof\tdetermination\t(or\tR-squared),\twhich\nmeasures\tthe\tfraction\tof\tthe\ttotal\tvariation\tin\tthe\tdependent\tvariable\tthat\tis\tcaptured\tby\tthe\nmodel:\n\ndef\ttotal_sum_of_squares(y):\n\t\t\t\t\"\"\"the\ttotal\tsquared\tvariation\tof\ty_i's\tfrom\ttheir\tmean\"\"\"\n\t\t\t\treturn\tsum(v\t**\t2\tfor\tv\tin\tde_mean(y))\n\ndef\tr_squared(alpha,\tbeta,\tx,\ty):\n\t\t\t\t\"\"\"the\tfraction\tof\tvariation\tin\ty\tcaptured\tby\tthe\tmodel,\twhich\tequals\n\t\t\t\t1\t-\tthe\tfraction\tof\tvariation\tin\ty\tnot\tcaptured\tby\tthe\tmodel\"\"\"\n\n\t\t\t\treturn\t1.0\t-\t(sum_of_squared_errors(alpha,\tbeta,\tx,\ty)\t/\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttotal_sum_of_squares(y))\n\nr_squared(alpha,\tbeta,\tnum_friends_good,\tdaily_minutes_good)\t\t\t\t\t\t#\t0.329\n\nNow,\twe\tchose\tthe\talpha\tand\tbeta\tthat\tminimized\tthe\tsum\tof\tthe\tsquared\tprediction\nerrors.\tOne\tlinear\tmodel\twe\tcould\thave\tchosen\tis\t\u201calways\tpredict\tmean(y)\u201d\n(corresponding\tto\talpha\t=\tmean(y)\tand\tbeta\t=\t0),\twhose\tsum\tof\tsquared\terrors\texactly\nequals\tits\ttotal\tsum\tof\tsquares.\tThis\tmeans\tan\tR-squared\tof\tzero,\twhich\tindicates\ta\tmodel\nthat\t(obviously,\tin\tthis\tcase)\tperforms\tno\tbetter\tthan\tjust\tpredicting\tthe\tmean.",
    "249": "Clearly,\tthe\tleast\tsquares\tmodel\tmust\tbe\tat\tleast\tas\tgood\tas\tthat\tone,\twhich\tmeans\tthat\tthe\nsum\tof\tthe\tsquared\terrors\tis\tat\tmost\tthe\ttotal\tsum\tof\tsquares,\twhich\tmeans\tthat\tthe\tR-\nsquared\tmust\tbe\tat\tleast\tzero.\tAnd\tthe\tsum\tof\tsquared\terrors\tmust\tbe\tat\tleast\t0,\twhich\nmeans\tthat\tthe\tR-squared\tcan\tbe\tat\tmost\t1.\n\nThe\thigher\tthe\tnumber,\tthe\tbetter\tour\tmodel\tfits\tthe\tdata.\tHere\twe\tcalculate\tan\tR-squared\nof\t0.329,\twhich\ttells\tus\tthat\tour\tmodel\tis\tonly\tsort\tof\tokay\tat\tfitting\tthe\tdata,\tand\tthat\nclearly\tthere\tare\tother\tfactors\tat\tplay.",
    "250": "Using\tGradient\tDescent\n\nIf\twe\twrite\ttheta\t=\t[alpha,\tbeta],\tthen\twe\tcan\talso\tsolve\tthis\tusing\tgradient\tdescent:\n\ndef\tsquared_error(x_i,\ty_i,\ttheta):\n\t\t\t\talpha,\tbeta\t=\ttheta\n\t\t\t\treturn\terror(alpha,\tbeta,\tx_i,\ty_i)\t**\t2\n\ndef\tsquared_error_gradient(x_i,\ty_i,\ttheta):\n\t\t\t\talpha,\tbeta\t=\ttheta\n\t\t\t\treturn\t[-2\t*\terror(alpha,\tbeta,\tx_i,\ty_i),\t\t\t\t\t\t\t#\talpha\tpartial\tderivative\n\t\t\t\t\t\t\t\t\t\t\t\t-2\t*\terror(alpha,\tbeta,\tx_i,\ty_i)\t*\tx_i]\t#\tbeta\tpartial\tderivative\n\n#\tchoose\trandom\tvalue\tto\tstart\nrandom.seed(0)\ntheta\t=\t[random.random(),\trandom.random()]\nalpha,\tbeta\t=\tminimize_stochastic(squared_error,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsquared_error_gradient,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnum_friends_good,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdaily_minutes_good,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttheta,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t0.0001)\nprint\talpha,\tbeta\n\nUsing\tthe\tsame\tdata\twe\tget\talpha\t=\t22.93,\tbeta\t=\t0.905,\twhich\tare\tvery\tclose\tto\tthe\texact\nanswers.",
    "251": "Maximum\tLikelihood\tEstimation\n\nWhy\tchoose\tleast\tsquares?\tOne\tjustification\tinvolves\tmaximum\tlikelihood\testimation.\n\nImagine\tthat\twe\thave\ta\tsample\tof\tdata\t\ndepends\ton\tsome\tunknown\tparameter\t\n\n:\n\n\tthat\tcomes\tfrom\ta\tdistribution\tthat\n\nIf\twe\tdidn\u2019t\tknow\ttheta,\twe\tcould\tturn\taround\tand\tthink\tof\tthis\tquantity\tas\tthe\tlikelihood\nof\t\n\n\tgiven\tthe\tsample:\n\nUnder\tthis\tapproach,\tthe\tmost\tlikely\t\nfunction;\tthat\tis,\tthe\tvalue\tthat\tmakes\tthe\tobserved\tdata\tthe\tmost\tprobable.\tIn\tthe\tcase\tof\ta\ncontinuous\tdistribution,\tin\twhich\twe\thave\ta\tprobability\tdistribution\tfunction\trather\tthan\ta\nprobability\tmass\tfunction,\twe\tcan\tdo\tthe\tsame\tthing.\n\n\tis\tthe\tvalue\tthat\tmaximizes\tthis\tlikelihood\n\nBack\tto\tregression.\tOne\tassumption\tthat\u2019s\toften\tmade\tabout\tthe\tsimple\tregression\tmodel\nis\tthat\tthe\tregression\terrors\tare\tnormally\tdistributed\twith\tmean\t0\tand\tsome\t(known)\nstandard\tdeviation\t\ny_i)\tis:\n\n.\tIf\tthat\u2019s\tthe\tcase,\tthen\tthe\tlikelihood\tbased\ton\tseeing\ta\tpair\t(x_i,\n\nThe\tlikelihood\tbased\ton\tthe\tentire\tdata\tset\tis\tthe\tproduct\tof\tthe\tindividual\tlikelihoods,\nwhich\tis\tlargest\tprecisely\twhen\talpha\tand\tbeta\tare\tchosen\tto\tminimize\tthe\tsum\tof\nsquared\terrors.\tThat\tis,\tin\tthis\tcase\t(and\twith\tthese\tassumptions),\tminimizing\tthe\tsum\tof\nsquared\terrors\tis\tequivalent\tto\tmaximizing\tthe\tlikelihood\tof\tthe\tobserved\tdata.",
    "252": "For\tFurther\tExploration\n\nContinue\treading\tabout\tmultiple\tregression\tin\tChapter\t15!",
    "253": "",
    "254": "Chapter\t15.\tMultiple\tRegression\n\nI\tdon\u2019t\tlook\tat\ta\tproblem\tand\tput\tvariables\tin\tthere\tthat\tdon\u2019t\taffect\tit.\n\nBill\tParcells\n\nAlthough\tthe\tVP\tis\tpretty\timpressed\twith\tyour\tpredictive\tmodel,\tshe\tthinks\tyou\tcan\tdo\nbetter.\tTo\tthat\tend,\tyou\u2019ve\tcollected\tadditional\tdata:\tfor\teach\tof\tyour\tusers,\tyou\tknow\thow\nmany\thours\the\tworks\teach\tday,\tand\twhether\the\thas\ta\tPhD.\tYou\u2019d\tlike\tto\tuse\tthis\nadditional\tdata\tto\timprove\tyour\tmodel.\n\nAccordingly,\tyou\thypothesize\ta\tlinear\tmodel\twith\tmore\tindependent\tvariables:\n\nObviously,\twhether\ta\tuser\thas\ta\tPhD\tis\tnot\ta\tnumber,\tbut\t\u2014\tas\twe\tmentioned\tin\nChapter\t11\t\u2014\twe\tcan\tintroduce\ta\tdummy\tvariable\tthat\tequals\t1\tfor\tusers\twith\tPhDs\tand\t0\nfor\tusers\twithout,\tafter\twhich\tit\u2019s\tjust\tas\tnumeric\tas\tthe\tother\tvariables.",
    "255": "The\tModel\n\nRecall\tthat\tin\tChapter\t14\twe\tfit\ta\tmodel\tof\tthe\tform:\n\nNow\timagine\tthat\teach\tinput\t\n\n\tis\tnot\ta\tsingle\tnumber\tbut\trather\ta\tvector\tof\tk\tnumbers\t\n\n.\tThe\tmultiple\tregression\tmodel\tassumes\tthat:\n\nIn\tmultiple\tregression\tthe\tvector\tof\tparameters\tis\tusually\tcalled\t\ninclude\tthe\tconstant\tterm\tas\twell,\twhich\twe\tcan\tachieve\tby\tadding\ta\tcolumn\tof\tones\tto\nour\tdata:\n\n.\tWe\u2019ll\twant\tthis\tto\n\nbeta\t=\t[alpha,\tbeta_1,\t...,\tbeta_k]\n\nand:\n\nx_i\t=\t[1,\tx_i1,\t...,\tx_ik]\n\nThen\tour\tmodel\tis\tjust:\n\ndef\tpredict(x_i,\tbeta):\n\t\t\t\t\"\"\"assumes\tthat\tthe\tfirst\telement\tof\teach\tx_i\tis\t1\"\"\"\n\t\t\t\treturn\tdot(x_i,\tbeta)\n\nIn\tthis\tparticular\tcase,\tour\tindependent\tvariable\tx\twill\tbe\ta\tlist\tof\tvectors,\teach\tof\twhich\nlooks\tlike\tthis:\n\n[1,\t\t\t\t#\tconstant\tterm\n\t49,\t\t\t#\tnumber\tof\tfriends\n\t4,\t\t\t\t#\twork\thours\tper\tday\n\t0]\t\t\t\t#\tdoesn't\thave\tPhD",
    "256": "Further\tAssumptions\tof\tthe\tLeast\tSquares\tModel\n\nThere\tare\ta\tcouple\tof\tfurther\tassumptions\tthat\tare\trequired\tfor\tthis\tmodel\t(and\tour\nsolution)\tto\tmake\tsense.\n\nThe\tfirst\tis\tthat\tthe\tcolumns\tof\tx\tare\tlinearly\tindependent\t\u2014\tthat\tthere\u2019s\tno\tway\tto\twrite\nany\tone\tas\ta\tweighted\tsum\tof\tsome\tof\tthe\tothers.\tIf\tthis\tassumption\tfails,\tit\u2019s\timpossible\nto\testimate\tbeta.\tTo\tsee\tthis\tin\tan\textreme\tcase,\timagine\twe\thad\tan\textra\tfield\nnum_acquaintances\tin\tour\tdata\tthat\tfor\tevery\tuser\twas\texactly\tequal\tto\tnum_friends.\n\nThen,\tstarting\twith\tany\tbeta,\tif\twe\tadd\tany\tamount\tto\tthe\tnum_friends\tcoefficient\tand\nsubtract\tthat\tsame\tamount\tfrom\tthe\tnum_acquaintances\tcoefficient,\tthe\tmodel\u2019s\npredictions\twill\tremain\tunchanged.\tWhich\tmeans\tthat\tthere\u2019s\tno\tway\tto\tfind\tthe\ncoefficient\tfor\tnum_friends.\t(Usually\tviolations\tof\tthis\tassumption\twon\u2019t\tbe\tso\tobvious.)\n\nThe\tsecond\timportant\tassumption\tis\tthat\tthe\tcolumns\tof\tx\tare\tall\tuncorrelated\twith\tthe\n.\tIf\tthis\tfails\tto\tbe\tthe\tcase,\tour\testimates\tof\tbeta\twill\tbe\tsystematically\twrong.\nerrors\t\n\nFor\tinstance,\tin\tChapter\t14,\twe\tbuilt\ta\tmodel\tthat\tpredicted\tthat\teach\tadditional\tfriend\twas\nassociated\twith\tan\textra\t0.90\tdaily\tminutes\ton\tthe\tsite.\n\nImagine\tthat\tit\u2019s\talso\tthe\tcase\tthat:\n\nPeople\twho\twork\tmore\thours\tspend\tless\ttime\ton\tthe\tsite.\n\nPeople\twith\tmore\tfriends\ttend\tto\twork\tmore\thours.\n\nThat\tis,\timagine\tthat\tthe\t\u201cactual\u201d\tmodel\tis:\n\nand\tthat\twork\thours\tand\tfriends\tare\tpositively\tcorrelated.\tIn\tthat\tcase,\twhen\twe\tminimize\nthe\terrors\tof\tthe\tsingle\tvariable\tmodel:\n\nwe\twill\tunderestimate\t\n\n.\n\nThink\tabout\twhat\twould\thappen\tif\twe\tmade\tpredictions\tusing\tthe\tsingle\tvariable\tmodel\nwith\tthe\t\u201cactual\u201d\tvalue\tof\t\n.\t(That\tis,\tthe\tvalue\tthat\tarises\tfrom\tminimizing\tthe\terrors\tof\nwhat\twe\tcalled\tthe\t\u201cactual\u201d\tmodel.)\tThe\tpredictions\twould\ttend\tto\tbe\ttoo\tsmall\tfor\tusers\n\nwho\twork\tmany\thours\tand\ttoo\tlarge\tfor\tusers\twho\twork\tfew\thours,\tbecause\t\nwe\t\u201cforgot\u201d\tto\tinclude\tit.\tBecause\twork\thours\tis\tpositively\tcorrelated\twith\tnumber\tof\nfriends,\tthis\tmeans\tthe\tpredictions\ttend\tto\tbe\ttoo\tsmall\tfor\tusers\twith\tmany\tfriends\tand\ttoo\nlarge\tfor\tusers\twith\tfew\tfriends.\n\n\tand\n\nThe\tresult\tof\tthis\tis\tthat\twe\tcan\treduce\tthe\terrors\t(in\tthe\tsingle-variable\tmodel)\tby",
    "257": "decreasing\tour\testimate\tof\t\n\tis\tsmaller\tthan\nthe\t\u201cactual\u201d\tvalue.\tThat\tis,\tin\tthis\tcase\tthe\tsingle-variable\tleast\tsquares\tsolution\tis\tbiased\n.\tAnd,\tin\tgeneral,\twhenever\tthe\tindependent\tvariables\tare\tcorrelated\nto\tunderestimate\t\n\n,\twhich\tmeans\tthat\tthe\terror-minimizing\t\n\nwith\tthe\terrors\tlike\tthis,\tour\tleast\tsquares\tsolution\twill\tgive\tus\ta\tbiased\testimate\tof\t\n\n.",
    "258": "Fitting\tthe\tModel\n\nAs\twe\tdid\tin\tthe\tsimple\tlinear\tmodel,\twe\u2019ll\tchoose\tbeta\tto\tminimize\tthe\tsum\tof\tsquared\nerrors.\tFinding\tan\texact\tsolution\tis\tnot\tsimple\tto\tdo\tby\thand,\twhich\tmeans\twe\u2019ll\tneed\tto\nuse\tgradient\tdescent.\tWe\u2019ll\tstart\tby\tcreating\tan\terror\tfunction\tto\tminimize.\tFor\tstochastic\ngradient\tdescent,\twe\u2019ll\tjust\twant\tthe\tsquared\terror\tcorresponding\tto\ta\tsingle\tprediction:\n\ndef\terror(x_i,\ty_i,\tbeta):\n\t\t\t\treturn\ty_i\t-\tpredict(x_i,\tbeta)\n\ndef\tsquared_error(x_i,\ty_i,\tbeta):\n\t\t\t\treturn\terror(x_i,\ty_i,\tbeta)\t**\t2\n\nIf\tyou\tknow\tcalculus,\tyou\tcan\tcompute:\n\ndef\tsquared_error_gradient(x_i,\ty_i,\tbeta):\n\t\t\t\t\"\"\"the\tgradient\t(with\trespect\tto\tbeta)\n\t\t\t\tcorresponding\tto\tthe\tith\tsquared\terror\tterm\"\"\"\n\t\t\t\treturn\t[-2\t*\tx_ij\t*\terror(x_i,\ty_i,\tbeta)\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tx_ij\tin\tx_i]\n\nOtherwise,\tyou\u2019ll\tneed\tto\ttake\tmy\tword\tfor\tit.\n\nAt\tthis\tpoint,\twe\u2019re\tready\tto\tfind\tthe\toptimal\tbeta\tusing\tstochastic\tgradient\tdescent:\n\ndef\testimate_beta(x,\ty):\n\t\t\t\tbeta_initial\t=\t[random.random()\tfor\tx_i\tin\tx[0]]\n\t\t\t\treturn\tminimize_stochastic(squared_error,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsquared_error_gradient,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tx,\ty,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbeta_initial,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t0.001)\n\nrandom.seed(0)\nbeta\t=\testimate_beta(x,\tdaily_minutes_good)\t#\t[30.63,\t0.972,\t-1.868,\t0.911]\n\nThis\tmeans\tour\tmodel\tlooks\tlike:",
    "259": "Interpreting\tthe\tModel\n\nYou\tshould\tthink\tof\tthe\tcoefficients\tof\tthe\tmodel\tas\trepresenting\tall-else-being-equal\nestimates\tof\tthe\timpacts\tof\teach\tfactor.\tAll\telse\tbeing\tequal,\teach\tadditional\tfriend\ncorresponds\tto\tan\textra\tminute\tspent\ton\tthe\tsite\teach\tday.\tAll\telse\tbeing\tequal,\teach\nadditional\thour\tin\ta\tuser\u2019s\tworkday\tcorresponds\tto\tabout\ttwo\tfewer\tminutes\tspent\ton\tthe\nsite\teach\tday.\tAll\telse\tbeing\tequal,\thaving\ta\tPhD\tis\tassociated\twith\tspending\tan\textra\nminute\ton\tthe\tsite\teach\tday.\n\nWhat\tthis\tdoesn\u2019t\t(directly)\ttell\tus\tis\tanything\tabout\tthe\tinteractions\tamong\tthe\tvariables.\nIt\u2019s\tpossible\tthat\tthe\teffect\tof\twork\thours\tis\tdifferent\tfor\tpeople\twith\tmany\tfriends\tthan\tit\nis\tfor\tpeople\twith\tfew\tfriends.\tThis\tmodel\tdoesn\u2019t\tcapture\tthat.\tOne\tway\tto\thandle\tthis\ncase\tis\tto\tintroduce\ta\tnew\tvariable\tthat\tis\tthe\tproduct\tof\t\u201cfriends\u201d\tand\t\u201cwork\thours.\u201d\tThis\neffectively\tallows\tthe\t\u201cwork\thours\u201d\tcoefficient\tto\tincrease\t(or\tdecrease)\tas\tthe\tnumber\tof\nfriends\tincreases.\n\nOr\tit\u2019s\tpossible\tthat\tthe\tmore\tfriends\tyou\thave,\tthe\tmore\ttime\tyou\tspend\ton\tthe\tsite\tup\tto\ta\npoint,\tafter\twhich\tfurther\tfriends\tcause\tyou\tto\tspend\tless\ttime\ton\tthe\tsite.\t(Perhaps\twith\ntoo\tmany\tfriends\tthe\texperience\tis\tjust\ttoo\toverwhelming?)\tWe\tcould\ttry\tto\tcapture\tthis\tin\nour\tmodel\tby\tadding\tanother\tvariable\tthat\u2019s\tthe\tsquare\tof\tthe\tnumber\tof\tfriends.\n\nOnce\twe\tstart\tadding\tvariables,\twe\tneed\tto\tworry\tabout\twhether\ttheir\tcoefficients\n\u201cmatter.\u201d\tThere\tare\tno\tlimits\tto\tthe\tnumbers\tof\tproducts,\tlogs,\tsquares,\tand\thigher\tpowers\nwe\tcould\tadd.",
    "260": "Goodness\tof\tFit\n\nAgain\twe\tcan\tlook\tat\tthe\tR-squared,\twhich\thas\tnow\tincreased\tto\t0.68:\n\ndef\tmultiple_r_squared(x,\ty,\tbeta):\n\t\t\t\tsum_of_squared_errors\t=\tsum(error(x_i,\ty_i,\tbeta)\t**\t2\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tx_i,\ty_i\tin\tzip(x,\ty))\n\t\t\t\treturn\t1.0\t-\tsum_of_squared_errors\t/\ttotal_sum_of_squares(y)\n\nKeep\tin\tmind,\thowever,\tthat\tadding\tnew\tvariables\tto\ta\tregression\twill\tnecessarily\tincrease\nthe\tR-squared.\tAfter\tall,\tthe\tsimple\tregression\tmodel\tis\tjust\tthe\tspecial\tcase\tof\tthe\nmultiple\tregression\tmodel\twhere\tthe\tcoefficients\ton\t\u201cwork\thours\u201d\tand\t\u201cPhD\u201d\tboth\tequal\n0.\tThe\toptimal\tmultiple\tregression\tmodel\twill\tnecessarily\thave\tan\terror\tat\tleast\tas\tsmall\tas\nthat\tone.\n\nBecause\tof\tthis,\tin\ta\tmultiple\tregression,\twe\talso\tneed\tto\tlook\tat\tthe\tstandard\terrors\tof\tthe\ncoefficients,\twhich\tmeasure\thow\tcertain\twe\tare\tabout\tour\testimates\tof\teach\t\nregression\tas\ta\twhole\tmay\tfit\tour\tdata\tvery\twell,\tbut\tif\tsome\tof\tthe\tindependent\tvariables\nare\tcorrelated\t(or\tirrelevant),\ttheir\tcoefficients\tmight\tnot\tmean\tmuch.\n\n.\tThe\n\nThe\ttypical\tapproach\tto\tmeasuring\tthese\terrors\tstarts\twith\tanother\tassumption\t\u2014\tthat\tthe\n\tare\tindependent\tnormal\trandom\tvariables\twith\tmean\t0\tand\tsome\tshared\nerrors\t\n(unknown)\tstandard\tdeviation\t\n.\tIn\tthat\tcase,\twe\t(or,\tmore\tlikely,\tour\tstatistical\tsoftware)\ncan\tuse\tsome\tlinear\talgebra\tto\tfind\tthe\tstandard\terror\tof\teach\tcoefficient.\tThe\tlarger\tit\tis,\nthe\tless\tsure\tour\tmodel\tis\tabout\tthat\tcoefficient.\tUnfortunately,\twe\u2019re\tnot\tset\tup\tto\tdo\tthat\nkind\tof\tlinear\talgebra\tfrom\tscratch.",
    "261": "Digression:\tThe\tBootstrap\n\nImagine\twe\thave\ta\tsample\tof\tn\tdata\tpoints,\tgenerated\tby\tsome\t(unknown\tto\tus)\ndistribution:\n\ndata\t=\tget_sample(num_points=n)\n\nIn\tChapter\t5,\twe\twrote\ta\tfunction\tto\tcompute\tthe\tmedian\tof\tthe\tobserved\tdata,\twhich\twe\ncan\tuse\tas\tan\testimate\tof\tthe\tmedian\tof\tthe\tdistribution\titself.\n\nBut\thow\tconfident\tcan\twe\tbe\tabout\tour\testimate?\tIf\tall\tthe\tdata\tin\tthe\tsample\tare\tvery\nclose\tto\t100,\tthen\tit\tseems\tlikely\tthat\tthe\tactual\tmedian\tis\tclose\tto\t100.\tIf\tapproximately\nhalf\tthe\tdata\tin\tthe\tsample\tis\tclose\tto\t0\tand\tthe\tother\thalf\tis\tclose\tto\t200,\tthen\twe\tcan\u2019t\tbe\nnearly\tas\tcertain\tabout\tthe\tmedian.\n\nIf\twe\tcould\trepeatedly\tget\tnew\tsamples,\twe\tcould\tcompute\tthe\tmedian\tof\teach\tand\tlook\tat\nthe\tdistribution\tof\tthose\tmedians.\tUsually\twe\tcan\u2019t.\tWhat\twe\tcan\tdo\tinstead\tis\tbootstrap\nnew\tdata\tsets\tby\tchoosing\tn\tdata\tpoints\twith\treplacement\tfrom\tour\tdata\tand\tthen\tcompute\nthe\tmedians\tof\tthose\tsynthetic\tdata\tsets:\n\ndef\tbootstrap_sample(data):\n\t\t\t\t\"\"\"randomly\tsamples\tlen(data)\telements\twith\treplacement\"\"\"\n\t\t\t\treturn\t[random.choice(data)\tfor\t_\tin\tdata]\n\ndef\tbootstrap_statistic(data,\tstats_fn,\tnum_samples):\n\t\t\t\t\"\"\"evaluates\tstats_fn\ton\tnum_samples\tbootstrap\tsamples\tfrom\tdata\"\"\"\n\t\t\t\treturn\t[stats_fn(bootstrap_sample(data))\n\t\t\t\t\t\t\t\t\t\t\t\tfor\t_\tin\trange(num_samples)]\n\nFor\texample,\tconsider\tthe\ttwo\tfollowing\tdata\tsets:\n\n#\t101\tpoints\tall\tvery\tclose\tto\t100\nclose_to_100\t=\t[99.5\t+\trandom.random()\tfor\t_\tin\trange(101)]\n\n#\t101\tpoints,\t50\tof\tthem\tnear\t0,\t50\tof\tthem\tnear\t200\nfar_from_100\t=\t([99.5\t+\trandom.random()]\t+\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[random.random()\tfor\t_\tin\trange(50)]\t+\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[200\t+\trandom.random()\tfor\t_\tin\trange(50)])\n\nIf\tyou\tcompute\tthe\tmedian\tof\teach,\tboth\twill\tbe\tvery\tclose\tto\t100.\tHowever,\tif\tyou\tlook\nat:\n\nbootstrap_statistic(close_to_100,\tmedian,\t100)\n\nyou\twill\tmostly\tsee\tnumbers\treally\tclose\tto\t100.\tWhereas\tif\tyou\tlook\tat:\n\nbootstrap_statistic(far_from_100,\tmedian,\t100)\n\nyou\twill\tsee\ta\tlot\tof\tnumbers\tclose\tto\t0\tand\ta\tlot\tof\tnumbers\tclose\tto\t200.\n\nThe\tstandard_deviation\tof\tthe\tfirst\tset\tof\tmedians\tis\tclose\tto\t0,\twhile\tthe\nstandard_deviation\tof\tthe\tsecond\tset\tof\tmedians\tis\tclose\tto\t100.\t(This\textreme\ta\tcase",
    "262": "would\tbe\tpretty\teasy\tto\tfigure\tout\tby\tmanually\tinspecting\tthe\tdata,\tbut\tin\tgeneral\tthat\nwon\u2019t\tbe\ttrue.)",
    "263": "Standard\tErrors\tof\tRegression\tCoefficients\n\nWe\tcan\ttake\tthe\tsame\tapproach\tto\testimating\tthe\tstandard\terrors\tof\tour\tregression\ncoefficients.\tWe\trepeatedly\ttake\ta\tbootstrap_sample\tof\tour\tdata\tand\testimate\tbeta\tbased\non\tthat\tsample.\tIf\tthe\tcoefficient\tcorresponding\tto\tone\tof\tthe\tindependent\tvariables\t(say\nnum_friends)\tdoesn\u2019t\tvary\tmuch\tacross\tsamples,\tthen\twe\tcan\tbe\tconfident\tthat\tour\nestimate\tis\trelatively\ttight.\tIf\tthe\tcoefficient\tvaries\tgreatly\tacross\tsamples,\tthen\twe\tcan\u2019t\nbe\tat\tall\tconfident\tin\tour\testimate.\n\nThe\tonly\tsubtlety\tis\tthat,\tbefore\tsampling,\twe\u2019ll\tneed\tto\tzip\tour\tx\tdata\tand\ty\tdata\tto\tmake\nsure\tthat\tcorresponding\tvalues\tof\tthe\tindependent\tand\tdependent\tvariables\tare\tsampled\ntogether.\tThis\tmeans\tthat\tbootstrap_sample\twill\treturn\ta\tlist\tof\tpairs\t(x_i,\ty_i),\twhich\nwe\u2019ll\tneed\tto\treassemble\tinto\tan\tx_sample\tand\ta\ty_sample:\n\ndef\testimate_sample_beta(sample):\n\t\t\t\t\"\"\"sample\tis\ta\tlist\tof\tpairs\t(x_i,\ty_i)\"\"\"\n\t\t\t\tx_sample,\ty_sample\t=\tzip(*sample)\t#\tmagic\tunzipping\ttrick\n\t\t\t\treturn\testimate_beta(x_sample,\ty_sample)\n\nrandom.seed(0)\t#\tso\tthat\tyou\tget\tthe\tsame\tresults\tas\tme\n\nbootstrap_betas\t=\tbootstrap_statistic(zip(x,\tdaily_minutes_good),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\testimate_sample_beta,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t100)\n\nAfter\twhich\twe\tcan\testimate\tthe\tstandard\tdeviation\tof\teach\tcoefficient:\n\nbootstrap_standard_errors\t=\t[\n\t\t\t\tstandard_deviation([beta[i]\tfor\tbeta\tin\tbootstrap_betas])\n\t\t\t\tfor\ti\tin\trange(4)]\n\n#\t[1.174,\t\t\t\t#\tconstant\tterm,\tactual\terror\t=\t1.19\n#\t\t0.079,\t\t\t\t#\tnum_friends,\t\t\tactual\terror\t=\t0.080\n#\t\t0.131,\t\t\t\t#\tunemployed,\t\t\t\tactual\terror\t=\t0.127\n#\t\t0.990]\t\t\t\t#\tphd,\t\t\t\t\t\t\t\t\t\t\tactual\terror\t=\t0.998\n\nWe\tcan\tuse\tthese\tto\ttest\thypotheses\tsuch\tas\t\u201cdoes\t\n\n\tequal\tzero?\u201d\tUnder\tthe\tnull\n\nhypothesis\t\nstatistic:\n\n\t(and\twith\tour\tother\tassumptions\tabout\tthe\tdistribution\tof\t\n\n\t)\tthe\n\nwhich\tis\tour\testimate\tof\t\nStudent\u2019s\tt-distribution\twith\t\u201c\n\n\tdegrees\tof\tfreedom.\u201d\n\n\tdivided\tby\tour\testimate\tof\tits\tstandard\terror,\tfollows\ta\n\nIf\twe\thad\ta\tstudents_t_cdf\tfunction,\twe\tcould\tcompute\tp-values\tfor\teach\tleast-squares\ncoefficient\tto\tindicate\thow\tlikely\twe\twould\tbe\tto\tobserve\tsuch\ta\tvalue\tif\tthe\tactual\ncoefficient\twere\tzero.\tUnfortunately,\twe\tdon\u2019t\thave\tsuch\ta\tfunction.\t(Although\twe\twould\nif\twe\tweren\u2019t\tworking\tfrom\tscratch.)",
    "264": "However,\tas\tthe\tdegrees\tof\tfreedom\tget\tlarge,\tthe\tt-distribution\tgets\tcloser\tand\tcloser\tto\ta\nstandard\tnormal.\tIn\ta\tsituation\tlike\tthis,\twhere\tn\tis\tmuch\tlarger\tthan\tk,\twe\tcan\tuse\nnormal_cdf\tand\tstill\tfeel\tgood\tabout\tourselves:\n\ndef\tp_value(beta_hat_j,\tsigma_hat_j):\n\t\t\t\tif\tbeta_hat_j\t>\t0:\n\t\t\t\t\t\t\t\t#\tif\tthe\tcoefficient\tis\tpositive,\twe\tneed\tto\tcompute\ttwice\tthe\n\t\t\t\t\t\t\t\t#\tprobability\tof\tseeing\tan\teven\t*larger*\tvalue\n\t\t\t\t\t\t\t\treturn\t2\t*\t(1\t-\tnormal_cdf(beta_hat_j\t/\tsigma_hat_j))\n\t\t\t\telse:\n\t\t\t\t\t\t\t\t#\totherwise\ttwice\tthe\tprobability\tof\tseeing\ta\t*smaller*\tvalue\n\t\t\t\t\t\t\t\treturn\t2\t*\tnormal_cdf(beta_hat_j\t/\tsigma_hat_j)\n\np_value(30.63,\t1.174)\t\t\t\t#\t~0\t\t\t(constant\tterm)\np_value(0.972,\t0.079)\t\t\t\t#\t~0\t\t\t(num_friends)\np_value(-1.868,\t0.131)\t\t\t#\t~0\t\t\t(work_hours)\np_value(0.911,\t0.990)\t\t\t\t#\t0.36\t(phd)\n\n(In\ta\tsituation\tnot\tlike\tthis,\twe\twould\tprobably\tbe\tusing\tstatistical\tsoftware\tthat\tknows\nhow\tto\tcompute\tthe\tt-distribution,\tas\twell\tas\thow\tto\tcompute\tthe\texact\tstandard\terrors.)\n\nWhile\tmost\tof\tthe\tcoefficients\thave\tvery\tsmall\tp-values\t(suggesting\tthat\tthey\tare\tindeed\nnonzero),\tthe\tcoefficient\tfor\t\u201cPhD\u201d\tis\tnot\t\u201csignificantly\u201d\tdifferent\tfrom\tzero,\twhich\tmakes\nit\tlikely\tthat\tthe\tcoefficient\tfor\t\u201cPhD\u201d\tis\trandom\trather\tthan\tmeaningful.\n\nIn\tmore\telaborate\tregression\tscenarios,\tyou\tsometimes\twant\tto\ttest\tmore\telaborate\n\nhypotheses\tabout\tthe\tdata,\tsuch\tas\t\u201cat\tleast\tone\tof\tthe\t\n\n\tis\tnon-zero\u201d\tor\t\u201c\n\n\tequals\t\n\n\tand\n\n\tequals\t\n\n,\u201d\twhich\tyou\tcan\tdo\twith\tan\tF-test,\twhich,\talas,\tfalls\toutside\tthe\tscope\tof\tthis\n\nbook.",
    "265": "Regularization\n\nIn\tpractice,\tyou\u2019d\toften\tlike\tto\tapply\tlinear\tregression\tto\tdata\tsets\twith\tlarge\tnumbers\tof\nvariables.\tThis\tcreates\ta\tcouple\tof\textra\twrinkles.\tFirst,\tthe\tmore\tvariables\tyou\tuse,\tthe\nmore\tlikely\tyou\tare\tto\toverfit\tyour\tmodel\tto\tthe\ttraining\tset.\tAnd\tsecond,\tthe\tmore\nnonzero\tcoefficients\tyou\thave,\tthe\tharder\tit\tis\tto\tmake\tsense\tof\tthem.\tIf\tthe\tgoal\tis\tto\nexplain\tsome\tphenomenon,\ta\tsparse\tmodel\twith\tthree\tfactors\tmight\tbe\tmore\tuseful\tthan\ta\nslightly\tbetter\tmodel\twith\thundreds.\n\nRegularization\tis\tan\tapproach\tin\twhich\twe\tadd\tto\tthe\terror\tterm\ta\tpenalty\tthat\tgets\tlarger\nas\tbeta\tgets\tlarger.\tWe\tthen\tminimize\tthe\tcombined\terror\tand\tpenalty.\tThe\tmore\nimportance\twe\tplace\ton\tthe\tpenalty\tterm,\tthe\tmore\twe\tdiscourage\tlarge\tcoefficients.\n\nFor\texample,\tin\tridge\tregression,\twe\tadd\ta\tpenalty\tproportional\tto\tthe\tsum\tof\tthe\tsquares\nof\tthe\tbeta_i.\t(Except\tthat\ttypically\twe\tdon\u2019t\tpenalize\tbeta_0,\tthe\tconstant\tterm.)\n\n#\talpha\tis\ta\t*hyperparameter*\tcontrolling\thow\tharsh\tthe\tpenalty\tis\n#\tsometimes\tit's\tcalled\t\"lambda\"\tbut\tthat\talready\tmeans\tsomething\tin\tPython\ndef\tridge_penalty(beta,\talpha):\n\t\treturn\talpha\t*\tdot(beta[1:],\tbeta[1:])\n\ndef\tsquared_error_ridge(x_i,\ty_i,\tbeta,\talpha):\n\t\t\t\t\"\"\"estimate\terror\tplus\tridge\tpenalty\ton\tbeta\"\"\"\n\t\t\t\treturn\terror(x_i,\ty_i,\tbeta)\t**\t2\t+\tridge_penalty(beta,\talpha)\n\nwhich\tyou\tcan\tthen\tplug\tinto\tgradient\tdescent\tin\tthe\tusual\tway:\n\ndef\tridge_penalty_gradient(beta,\talpha):\n\t\t\t\t\"\"\"gradient\tof\tjust\tthe\tridge\tpenalty\"\"\"\n\t\t\t\treturn\t[0]\t+\t[2\t*\talpha\t*\tbeta_j\tfor\tbeta_j\tin\tbeta[1:]]\n\ndef\tsquared_error_ridge_gradient(x_i,\ty_i,\tbeta,\talpha):\n\t\t\t\t\"\"\"the\tgradient\tcorresponding\tto\tthe\tith\tsquared\terror\tterm\n\t\t\t\tincluding\tthe\tridge\tpenalty\"\"\"\n\t\t\t\treturn\tvector_add(squared_error_gradient(x_i,\ty_i,\tbeta),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tridge_penalty_gradient(beta,\talpha))\n\ndef\testimate_beta_ridge(x,\ty,\talpha):\n\t\t\t\t\"\"\"use\tgradient\tdescent\tto\tfit\ta\tridge\tregression\n\t\t\t\twith\tpenalty\talpha\"\"\"\n\t\t\t\tbeta_initial\t=\t[random.random()\tfor\tx_i\tin\tx[0]]\n\t\t\t\treturn\tminimize_stochastic(partial(squared_error_ridge,\talpha=alpha),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpartial(squared_error_ridge_gradient,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\talpha=alpha),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tx,\ty,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbeta_initial,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t0.001)\n\nWith\talpha\tset\tto\tzero,\tthere\u2019s\tno\tpenalty\tat\tall\tand\twe\tget\tthe\tsame\tresults\tas\tbefore:\n\nrandom.seed(0)\nbeta_0\t=\testimate_beta_ridge(x,\tdaily_minutes_good,\talpha=0.0)\n#\t[30.6,\t0.97,\t-1.87,\t0.91]\ndot(beta_0[1:],\tbeta_0[1:])\t#\t5.26\nmultiple_r_squared(x,\tdaily_minutes_good,\tbeta_0)\t#\t0.680\n\nAs\twe\tincrease\talpha,\tthe\tgoodness\tof\tfit\tgets\tworse,\tbut\tthe\tsize\tof\tbeta\tgets\tsmaller:",
    "266": "beta_0_01\t=\testimate_beta_ridge(x,\tdaily_minutes_good,\talpha=0.01)\n#\t[30.6,\t0.97,\t-1.86,\t0.89]\ndot(beta_0_01[1:],\tbeta_0_01[1:])\t\t#\t5.19\nmultiple_r_squared(x,\tdaily_minutes_good,\tbeta_0_01)\t\t#\t0.680\n\nbeta_0_1\t=\testimate_beta_ridge(x,\tdaily_minutes_good,\talpha=0.1)\n#\t[30.8,\t0.95,\t-1.84,\t0.54]\ndot(beta_0_1[1:],\tbeta_0_1[1:])\t\t#\t4.60\nmultiple_r_squared(x,\tdaily_minutes_good,\tbeta_0_1)\t\t#\t0.680\n\nbeta_1\t=\testimate_beta_ridge(x,\tdaily_minutes_good,\talpha=1)\n#\t[30.7,\t0.90,\t-1.69,\t0.085]\ndot(beta_1[1:],\tbeta_1[1:])\t\t#\t3.69\nmultiple_r_squared(x,\tdaily_minutes_good,\tbeta_1)\t\t#\t0.676\n\nbeta_10\t=\testimate_beta_ridge(x,\tdaily_minutes_good,\talpha=10)\n#\t[28.3,\t0.72,\t-0.91,\t-0.017]\ndot(beta_10[1:],\tbeta_10[1:])\t\t#\t1.36\nmultiple_r_squared(x,\tdaily_minutes_good,\tbeta_10)\t\t#\t0.573\n\nIn\tparticular,\tthe\tcoefficient\ton\t\u201cPhD\u201d\tvanishes\tas\twe\tincrease\tthe\tpenalty,\twhich\taccords\nwith\tour\tprevious\tresult\tthat\tit\twasn\u2019t\tsignificantly\tdifferent\tfrom\tzero.\n\nUsually\tyou\u2019d\twant\tto\trescale\tyour\tdata\tbefore\tusing\tthis\tapproach.\tAfter\tall,\tif\tyou\tchanged\tyears\tof\nexperience\tto\tcenturies\tof\texperience,\tits\tleast\tsquares\tcoefficient\twould\tincrease\tby\ta\tfactor\tof\t100\tand\nsuddenly\tget\tpenalized\tmuch\tmore,\teven\tthough\tit\u2019s\tthe\tsame\tmodel.\n\nNOTE\n\nAnother\tapproach\tis\tlasso\tregression,\twhich\tuses\tthe\tpenalty:\n\ndef\tlasso_penalty(beta,\talpha):\n\t\t\t\treturn\talpha\t*\tsum(abs(beta_i)\tfor\tbeta_i\tin\tbeta[1:])\n\nWhereas\tthe\tridge\tpenalty\tshrank\tthe\tcoefficients\toverall,\tthe\tlasso\tpenalty\ttends\tto\tforce\ncoefficients\tto\tbe\tzero,\twhich\tmakes\tit\tgood\tfor\tlearning\tsparse\tmodels.\tUnfortunately,\tit\u2019s\nnot\tamenable\tto\tgradient\tdescent,\twhich\tmeans\tthat\twe\twon\u2019t\tbe\table\tto\tsolve\tit\tfrom\nscratch.",
    "267": "For\tFurther\tExploration\n\nRegression\thas\ta\trich\tand\texpansive\ttheory\tbehind\tit.\tThis\tis\tanother\tplace\twhere\tyou\nshould\tconsider\treading\ta\ttextbook\tor\tat\tleast\ta\tlot\tof\tWikipedia\tarticles.\n\nscikit-learn\thas\ta\tlinear_model\tmodule\tthat\tprovides\ta\tLinearRegression\tmodel\nsimilar\tto\tours,\tas\twell\tas\tRidge\tregression,\tLasso\tregression,\tand\tother\ttypes\tof\nregularization\ttoo.\n\nStatsmodels\tis\tanother\tPython\tmodule\tthat\tcontains\t(among\tother\tthings)\tlinear\nregression\tmodels.",
    "268": "",
    "269": "Chapter\t16.\tLogistic\tRegression\n\nA\tlot\tof\tpeople\tsay\tthere\u2019s\ta\tfine\tline\tbetween\tgenius\tand\tinsanity.\tI\tdon\u2019t\tthink\tthere\u2019s\ta\nfine\tline,\tI\tactually\tthink\tthere\u2019s\ta\tyawning\tgulf.\n\nBill\tBailey\n\nIn\tChapter\t1,\twe\tbriefly\tlooked\tat\tthe\tproblem\tof\ttrying\tto\tpredict\twhich\tDataSciencester\nusers\tpaid\tfor\tpremium\taccounts.\tHere\twe\u2019ll\trevisit\tthat\tproblem.",
    "270": "The\tProblem\n\nWe\thave\tan\tanonymized\tdata\tset\tof\tabout\t200\tusers,\tcontaining\teach\tuser\u2019s\tsalary,\ther\nyears\tof\texperience\tas\ta\tdata\tscientist,\tand\twhether\tshe\tpaid\tfor\ta\tpremium\taccount\n(Figure\t16-1).\tAs\tis\tusual\twith\tcategorical\tvariables,\twe\trepresent\tthe\tdependent\tvariable\nas\teither\t0\t(no\tpremium\taccount)\tor\t1\t(premium\taccount).\n\nAs\tusual,\tour\tdata\tis\tin\ta\tmatrix\twhere\teach\trow\tis\ta\tlist\t[experience,\tsalary,\npaid_account].\tLet\u2019s\tturn\tit\tinto\tthe\tformat\twe\tneed:\n\nx\t=\t[[1]\t+\trow[:2]\tfor\trow\tin\tdata]\t\t#\teach\telement\tis\t[1,\texperience,\tsalary]\ny\t=\t[row[2]\tfor\trow\tin\tdata]\t\t\t\t\t\t\t\t\t#\teach\telement\tis\tpaid_account\n\nAn\tobvious\tfirst\tattempt\tis\tto\tuse\tlinear\tregression\tand\tfind\tthe\tbest\tmodel:\n\nFigure\t16-1.\tPaid\tand\tunpaid\tusers\n\nAnd\tcertainly,\tthere\u2019s\tnothing\tpreventing\tus\tfrom\tmodeling\tthe\tproblem\tthis\tway.\tThe\nresults\tare\tshown\tin\tFigure\t16-2:\n\nrescaled_x\t=\trescale(x)",
    "271": "beta\t=\testimate_beta(rescaled_x,\ty)\t\t#\t[0.26,\t0.43,\t-0.43]\npredictions\t=\t[predict(x_i,\tbeta)\tfor\tx_i\tin\trescaled_x]\n\nplt.scatter(predictions,\ty)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt.show()\n\nFigure\t16-2.\tUsing\tlinear\tregression\tto\tpredict\tpremium\taccounts\n\nBut\tthis\tapproach\tleads\tto\ta\tcouple\tof\timmediate\tproblems:\n\nWe\u2019d\tlike\tfor\tour\tpredicted\toutputs\tto\tbe\t0\tor\t1,\tto\tindicate\tclass\tmembership.\tIt\u2019s\tfine\nif\tthey\u2019re\tbetween\t0\tand\t1,\tsince\twe\tcan\tinterpret\tthese\tas\tprobabilities\t\u2014\tan\toutput\tof\n0.25\tcould\tmean\t25%\tchance\tof\tbeing\ta\tpaid\tmember.\tBut\tthe\toutputs\tof\tthe\tlinear\nmodel\tcan\tbe\thuge\tpositive\tnumbers\tor\teven\tnegative\tnumbers,\twhich\tit\u2019s\tnot\tclear\nhow\tto\tinterpret.\tIndeed,\there\ta\tlot\tof\tour\tpredictions\twere\tnegative.\n\nThe\tlinear\tregression\tmodel\tassumed\tthat\tthe\terrors\twere\tuncorrelated\twith\tthe\ncolumns\tof\tx.\tBut\there,\tthe\tregression\tcoefficent\tfor\texperience\tis\t0.43,\tindicating\tthat\nmore\texperience\tleads\tto\ta\tgreater\tlikelihood\tof\ta\tpremium\taccount.\tThis\tmeans\tthat\nour\tmodel\toutputs\tvery\tlarge\tvalues\tfor\tpeople\twith\tlots\tof\texperience.\tBut\twe\tknow\nthat\tthe\tactual\tvalues\tmust\tbe\tat\tmost\t1,\twhich\tmeans\tthat\tnecessarily\tvery\tlarge\noutputs\t(and\ttherefore\tvery\tlarge\tvalues\tof\texperience)\tcorrespond\tto\tvery\tlarge\nnegative\tvalues\tof\tthe\terror\tterm.\tBecause\tthis\tis\tthe\tcase,\tour\testimate\tof\tbeta\tis\nbiased.",
    "272": "What\twe\u2019d\tlike\tinstead\tis\tfor\tlarge\tpositive\tvalues\tof\tdot(x_i,\tbeta)\tto\tcorrespond\tto\nprobabilities\tclose\tto\t1,\tand\tfor\tlarge\tnegative\tvalues\tto\tcorrespond\tto\tprobabilities\tclose\nto\t0.\tWe\tcan\taccomplish\tthis\tby\tapplying\tanother\tfunction\tto\tthe\tresult.",
    "273": "The\tLogistic\tFunction\n\nIn\tthe\tcase\tof\tlogistic\tregression,\twe\tuse\tthe\tlogistic\tfunction,\tpictured\tin\tFigure\t16-3:\n\ndef\tlogistic(x):\n\t\t\t\treturn\t1.0\t/\t(1\t+\tmath.exp(-x))\n\nFigure\t16-3.\tThe\tlogistic\tfunction\n\nAs\tits\tinput\tgets\tlarge\tand\tpositive,\tit\tgets\tcloser\tand\tcloser\tto\t1.\tAs\tits\tinput\tgets\tlarge\nand\tnegative,\tit\tgets\tcloser\tand\tcloser\tto\t0.\tAdditionally,\tit\thas\tthe\tconvenient\tproperty\nthat\tits\tderivative\tis\tgiven\tby:\n\ndef\tlogistic_prime(x):\n\t\t\t\treturn\tlogistic(x)\t*\t(1\t-\tlogistic(x))\n\nwhich\twe\u2019ll\tmake\tuse\tof\tin\ta\tbit.\tWe\u2019ll\tuse\tthis\tto\tfit\ta\tmodel:\n\nwhere\tf\tis\tthe\tlogistic\tfunction.\n\nRecall\tthat\tfor\tlinear\tregression\twe\tfit\tthe\tmodel\tby\tminimizing\tthe\tsum\tof\tsquared\terrors,",
    "274": "which\tended\tup\tchoosing\tthe\t\n\n\tthat\tmaximized\tthe\tlikelihood\tof\tthe\tdata.\n\nHere\tthe\ttwo\taren\u2019t\tequivalent,\tso\twe\u2019ll\tuse\tgradient\tdescent\tto\tmaximize\tthe\tlikelihood\ndirectly.\tThis\tmeans\twe\tneed\tto\tcalculate\tthe\tlikelihood\tfunction\tand\tits\tgradient.\n\nGiven\tsome\t\n\n,\tour\tmodel\tsays\tthat\teach\t\n\n\tshould\tequal\t1\twith\tprobability\t\n\n\tand\t0\n\nwith\tprobability\t\n\n.\n\nIn\tparticular,\tthe\tpdf\tfor\t\n\n\tcan\tbe\twritten\tas:\n\nsince\tif\t\n\n\tis\t0,\tthis\tequals:\n\nand\tif\t\n\n\tis\t1,\tit\tequals:\n\nIt\tturns\tout\tthat\tit\u2019s\tactually\tsimpler\tto\tmaximize\tthe\tlog\tlikelihood:\n\nBecause\tlog\tis\tstrictly\tincreasing\tfunction,\tany\tbeta\tthat\tmaximizes\tthe\tlog\tlikelihood\talso\nmaximizes\tthe\tlikelihood,\tand\tvice\tversa.\n\ndef\tlogistic_log_likelihood_i(x_i,\ty_i,\tbeta):\n\t\t\t\tif\ty_i\t==\t1:\n\t\t\t\t\t\t\t\treturn\tmath.log(logistic(dot(x_i,\tbeta)))\n\t\t\t\telse:\n\t\t\t\t\t\t\t\treturn\tmath.log(1\t-\tlogistic(dot(x_i,\tbeta)))\n\nIf\twe\tassume\tdifferent\tdata\tpoints\tare\tindependent\tfrom\tone\tanother,\tthe\toverall\tlikelihood\nis\tjust\tthe\tproduct\tof\tthe\tindividual\tlikelihoods.\tWhich\tmeans\tthe\toverall\tlog\tlikelihood\tis\nthe\tsum\tof\tthe\tindividual\tlog\tlikelihoods:\n\ndef\tlogistic_log_likelihood(x,\ty,\tbeta):\n\t\t\t\treturn\tsum(logistic_log_likelihood_i(x_i,\ty_i,\tbeta)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tx_i,\ty_i\tin\tzip(x,\ty))\n\nA\tlittle\tbit\tof\tcalculus\tgives\tus\tthe\tgradient:\n\ndef\tlogistic_log_partial_ij(x_i,\ty_i,\tbeta,\tj):\n\t\t\t\t\"\"\"here\ti\tis\tthe\tindex\tof\tthe\tdata\tpoint,\n\t\t\t\tj\tthe\tindex\tof\tthe\tderivative\"\"\"\n\n\t\t\t\treturn\t(y_i\t-\tlogistic(dot(x_i,\tbeta)))\t*\tx_i[j]",
    "275": "def\tlogistic_log_gradient_i(x_i,\ty_i,\tbeta):\n\t\t\t\t\"\"\"the\tgradient\tof\tthe\tlog\tlikelihood\n\t\t\t\tcorresponding\tto\tthe\tith\tdata\tpoint\"\"\"\n\n\t\t\t\treturn\t[logistic_log_partial_ij(x_i,\ty_i,\tbeta,\tj)\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tj,\t_\tin\tenumerate(beta)]\n\ndef\tlogistic_log_gradient(x,\ty,\tbeta):\n\t\t\t\treturn\treduce(vector_add,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[logistic_log_gradient_i(x_i,\ty_i,\tbeta)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tx_i,\ty_i\tin\tzip(x,y)])\n\nat\twhich\tpoint\twe\thave\tall\tthe\tpieces\twe\tneed.",
    "276": "Applying\tthe\tModel\n\nWe\u2019ll\twant\tto\tsplit\tour\tdata\tinto\ta\ttraining\tset\tand\ta\ttest\tset:\n\nrandom.seed(0)\nx_train,\tx_test,\ty_train,\ty_test\t=\ttrain_test_split(rescaled_x,\ty,\t0.33)\n\n#\twant\tto\tmaximize\tlog\tlikelihood\ton\tthe\ttraining\tdata\nfn\t=\tpartial(logistic_log_likelihood,\tx_train,\ty_train)\ngradient_fn\t=\tpartial(logistic_log_gradient,\tx_train,\ty_train)\n\n#\tpick\ta\trandom\tstarting\tpoint\nbeta_0\t=\t[random.random()\tfor\t_\tin\trange(3)]\n\n#\tand\tmaximize\tusing\tgradient\tdescent\nbeta_hat\t=\tmaximize_batch(fn,\tgradient_fn,\tbeta_0)\n\nAlternatively,\tyou\tcould\tuse\tstochastic\tgradient\tdescent:\n\nbeta_hat\t=\tmaximize_stochastic(logistic_log_likelihood_i,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlogistic_log_gradient_i,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tx_train,\ty_train,\tbeta_0)\n\nEither\tway\twe\tfind\tapproximately:\n\nbeta_hat\t=\t[-1.90,\t4.05,\t-3.87]\n\nThese\tare\tcoefficients\tfor\tthe\trescaled\tdata,\tbut\twe\tcan\ttransform\tthem\tback\tto\tthe\noriginal\tdata\tas\twell:\n\nbeta_hat_unscaled\t=\t[7.61,\t1.42,\t-0.000249]\n\nUnfortunately,\tthese\tare\tnot\tas\teasy\tto\tinterpret\tas\tlinear\tregression\tcoefficients.\tAll\telse\nbeing\tequal,\tan\textra\tyear\tof\texperience\tadds\t1.42\tto\tthe\tinput\tof\tlogistic.\tAll\telse\tbeing\nequal,\tan\textra\t$10,000\tof\tsalary\tsubtracts\t2.49\tfrom\tthe\tinput\tof\tlogistic.\n\nThe\timpact\ton\tthe\toutput,\thowever,\tdepends\ton\tthe\tother\tinputs\tas\twell.\tIf\tdot(beta,\nx_i)\tis\talready\tlarge\t(corresponding\tto\ta\tprobability\tclose\tto\t1),\tincreasing\tit\teven\tby\ta\tlot\ncannot\taffect\tthe\tprobability\tvery\tmuch.\tIf\tit\u2019s\tclose\tto\t0,\tincreasing\tit\tjust\ta\tlittle\tmight\nincrease\tthe\tprobability\tquite\ta\tbit.\n\nWhat\twe\tcan\tsay\tis\tthat\t\u2014\tall\telse\tbeing\tequal\t\u2014\tpeople\twith\tmore\texperience\tare\tmore\nlikely\tto\tpay\tfor\taccounts.\tAnd\tthat\t\u2014\tall\telse\tbeing\tequal\t\u2014\tpeople\twith\thigher\tsalaries\nare\tless\tlikely\tto\tpay\tfor\taccounts.\t(This\twas\talso\tsomewhat\tapparent\twhen\twe\tplotted\tthe\ndata.)",
    "277": "Goodness\tof\tFit\n\nWe\thaven\u2019t\tyet\tused\tthe\ttest\tdata\tthat\twe\theld\tout.\tLet\u2019s\tsee\twhat\thappens\tif\twe\tpredict\npaid\taccount\twhenever\tthe\tprobability\texceeds\t0.5:\n\ntrue_positives\t=\tfalse_positives\t=\ttrue_negatives\t=\tfalse_negatives\t=\t0\n\nfor\tx_i,\ty_i\tin\tzip(x_test,\ty_test):\n\t\t\t\tpredict\t=\tlogistic(dot(beta_hat,\tx_i))\n\n\t\t\t\tif\ty_i\t==\t1\tand\tpredict\t>=\t0.5:\t\t#\tTP:\tpaid\tand\twe\tpredict\tpaid\n\t\t\t\t\t\t\t\ttrue_positives\t+=\t1\n\t\t\t\telif\ty_i\t==\t1:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tFN:\tpaid\tand\twe\tpredict\tunpaid\n\t\t\t\t\t\t\t\tfalse_negatives\t+=\t1\n\t\t\t\telif\tpredict\t>=\t0.5:\t\t\t\t\t\t\t\t\t\t\t\t\t#\tFP:\tunpaid\tand\twe\tpredict\tpaid\n\t\t\t\t\t\t\t\tfalse_positives\t+=\t1\n\t\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tTN:\tunpaid\tand\twe\tpredict\tunpaid\n\t\t\t\t\t\t\t\ttrue_negatives\t+=\t1\n\nprecision\t=\ttrue_positives\t/\t(true_positives\t+\tfalse_positives)\nrecall\t=\ttrue_positives\t/\t(true_positives\t+\tfalse_negatives)\n\nThis\tgives\ta\tprecision\tof\t93%\t(\u201cwhen\twe\tpredict\tpaid\taccount\twe\u2019re\tright\t93%\tof\tthe\ntime\u201d)\tand\ta\trecall\tof\t82%\t(\u201cwhen\ta\tuser\thas\ta\tpaid\taccount\twe\tpredict\tpaid\taccount\t82%\nof\tthe\ttime\u201d),\tboth\tof\twhich\tare\tpretty\trespectable\tnumbers.\n\nWe\tcan\talso\tplot\tthe\tpredictions\tversus\tthe\tactuals\t(Figure\t16-4),\twhich\talso\tshows\tthat\nthe\tmodel\tperforms\twell:\n\npredictions\t=\t[logistic(dot(beta_hat,\tx_i))\tfor\tx_i\tin\tx_test]\nplt.scatter(predictions,\ty_test)\nplt.xlabel(\"predicted\tprobability\")\nplt.ylabel(\"actual\toutcome\")\nplt.title(\"Logistic\tRegression\tPredicted\tvs.\tActual\")\nplt.show()",
    "278": "Figure\t16-4.\tLogistic\tregression\tpredicted\tversus\tactual",
    "279": "Support\tVector\tMachines\n\nThe\tset\tof\tpoints\twhere\tdot(beta_hat,\tx_i)\tequals\t0\tis\tthe\tboundary\tbetween\tour\nclasses.\tWe\tcan\tplot\tthis\tto\tsee\texactly\twhat\tour\tmodel\tis\tdoing\t(Figure\t16-5).\n\nThis\tboundary\tis\ta\thyperplane\tthat\tsplits\tthe\tparameter\tspace\tinto\ttwo\thalf-spaces\ncorresponding\tto\tpredict\tpaid\tand\tpredict\tunpaid.\tWe\tfound\tit\tas\ta\tside-effect\tof\tfinding\nthe\tmost\tlikely\tlogistic\tmodel.\n\nAn\talternative\tapproach\tto\tclassification\tis\tto\tjust\tlook\tfor\tthe\thyperplane\tthat\t\u201cbest\u201d\nseparates\tthe\tclasses\tin\tthe\ttraining\tdata.\tThis\tis\tthe\tidea\tbehind\tthe\tsupport\tvector\nmachine,\twhich\tfinds\tthe\thyperplane\tthat\tmaximizes\tthe\tdistance\tto\tthe\tnearest\tpoint\tin\neach\tclass\t(Figure\t16-6).\n\nFigure\t16-5.\tPaid\tand\tunpaid\tusers\twith\tdecision\tboundary\n\nFinding\tsuch\ta\thyperplane\tis\tan\toptimization\tproblem\tthat\tinvolves\ttechniques\tthat\tare\ttoo\nadvanced\tfor\tus.\tA\tdifferent\tproblem\tis\tthat\ta\tseparating\thyperplane\tmight\tnot\texist\tat\tall.\nIn\tour\t\u201cwho\tpays?\u201d\tdata\tset\tthere\tsimply\tis\tno\tline\tthat\tperfectly\tseparates\tthe\tpaid\tusers\nfrom\tthe\tunpaid\tusers.\n\nWe\tcan\t(sometimes)\tget\taround\tthis\tby\ttransforming\tthe\tdata\tinto\ta\thigher-dimensional\nspace.\tFor\texample,\tconsider\tthe\tsimple\tone-dimensional\tdata\tset\tshown\tin\tFigure\t16-7.",
    "280": "Figure\t16-6.\tA\tseparating\thyperplane\n\nIt\u2019s\tclear\tthat\tthere\u2019s\tno\thyperplane\tthat\tseparates\tthe\tpositive\texamples\tfrom\tthe\tnegative\nones.\tHowever,\tlook\tat\twhat\thappens\twhen\twe\tmap\tthis\tdata\tset\tto\ttwo\tdimensions\tby\nsending\tthe\tpoint\tx\tto\t(x,\tx**2).\tSuddenly\tit\u2019s\tpossible\tto\tfind\ta\thyperplane\tthat\tsplits\tthe\ndata\t(Figure\t16-8).\n\nThis\tis\tusually\tcalled\tthe\tkernel\ttrick\tbecause\trather\tthan\tactually\tmapping\tthe\tpoints\tinto\nthe\thigher-dimensional\tspace\t(which\tcould\tbe\texpensive\tif\tthere\tare\ta\tlot\tof\tpoints\tand\tthe\nmapping\tis\tcomplicated),\twe\tcan\tuse\ta\t\u201ckernel\u201d\tfunction\tto\tcompute\tdot\tproducts\tin\tthe\nhigher-dimensional\tspace\tand\tuse\tthose\tto\tfind\ta\thyperplane.",
    "281": "Figure\t16-7.\tA\tnonseparable\tone-dimensional\tdata\tset\n\nIt\u2019s\thard\t(and\tprobably\tnot\ta\tgood\tidea)\tto\tuse\tsupport\tvector\tmachines\twithout\trelying\ton\nspecialized\toptimization\tsoftware\twritten\tby\tpeople\twith\tthe\tappropriate\texpertise,\tso\nwe\u2019ll\thave\tto\tleave\tour\ttreatment\there.",
    "282": "Figure\t16-8.\tData\tset\tbecomes\tseparable\tin\thigher\tdimensions",
    "283": "For\tFurther\tInvestigation\n\nscikit-learn\thas\tmodules\tfor\tboth\tLogistic\tRegression\tand\tSupport\tVector\tMachines.\n\nlibsvm\tis\tthe\tsupport\tvector\tmachine\timplementation\tthat\tscikit-learn\tis\tusing\tbehind\nthe\tscenes.\tIts\twebsite\thas\ta\tvariety\tof\tuseful\tdocumentation\tabout\tsupport\tvector\nmachines.",
    "284": "",
    "285": "Chapter\t17.\tDecision\tTrees\n\nA\ttree\tis\tan\tincomprehensible\tmystery.\n\nJim\tWoodring\n\nDataSciencester\u2019s\tVP\tof\tTalent\thas\tinterviewed\ta\tnumber\tof\tjob\tcandidates\tfrom\tthe\tsite,\nwith\tvarying\tdegrees\tof\tsuccess.\tHe\u2019s\tcollected\ta\tdata\tset\tconsisting\tof\tseveral\n(qualitative)\tattributes\tof\teach\tcandidate,\tas\twell\tas\twhether\tthat\tcandidate\tinterviewed\nwell\tor\tpoorly.\tCould\tyou,\the\tasks,\tuse\tthis\tdata\tto\tbuild\ta\tmodel\tidentifying\twhich\ncandidates\twill\tinterview\twell,\tso\tthat\the\tdoesn\u2019t\thave\tto\twaste\ttime\tconducting\ninterviews?\n\nThis\tseems\tlike\ta\tgood\tfit\tfor\ta\tdecision\ttree,\tanother\tpredictive\tmodeling\ttool\tin\tthe\tdata\nscientist\u2019s\tkit.",
    "286": "What\tIs\ta\tDecision\tTree?\n\nA\tdecision\ttree\tuses\ta\ttree\tstructure\tto\trepresent\ta\tnumber\tof\tpossible\tdecision\tpaths\tand\nan\toutcome\tfor\teach\tpath.\n\nIf\tyou\thave\tever\tplayed\tthe\tgame\tTwenty\tQuestions,\tthen\tit\tturns\tout\tyou\tare\tfamiliar\twith\ndecision\ttrees.\tFor\texample:\n\n\u201cI\tam\tthinking\tof\tan\tanimal.\u201d\n\n\u201cDoes\tit\thave\tmore\tthan\tfive\tlegs?\u201d\n\n\u201cNo.\u201d\n\n\u201cIs\tit\tdelicious?\u201d\n\n\u201cNo.\u201d\n\n\u201cDoes\tit\tappear\ton\tthe\tback\tof\tthe\tAustralian\tfive-cent\tcoin?\u201d\n\n\u201cYes.\u201d\n\n\u201cIs\tit\tan\techidna?\u201d\n\n\u201cYes,\tit\tis!\u201d\n\nThis\tcorresponds\tto\tthe\tpath:\n\n\u201cNot\tmore\tthan\t5\tlegs\u201d\t\u2192\t\u201cNot\tdelicious\u201d\t\u2192\t\u201cOn\tthe\t5-cent\tcoin\u201d\t\u2192\t\u201cEchidna!\u201d\n\nin\tan\tidiosyncratic\t(and\tnot\tvery\tcomprehensive)\t\u201cguess\tthe\tanimal\u201d\tdecision\ttree\n(Figure\t17-1).",
    "287": "Figure\t17-1.\tA\t\u201cguess\tthe\tanimal\u201d\tdecision\ttree\n\nDecision\ttrees\thave\ta\tlot\tto\trecommend\tthem.\tThey\u2019re\tvery\teasy\tto\tunderstand\tand\ninterpret,\tand\tthe\tprocess\tby\twhich\tthey\treach\ta\tprediction\tis\tcompletely\ttransparent.\nUnlike\tthe\tother\tmodels\twe\u2019ve\tlooked\tat\tso\tfar,\tdecision\ttrees\tcan\teasily\thandle\ta\tmix\tof\nnumeric\t(e.g.,\tnumber\tof\tlegs)\tand\tcategorical\t(e.g.,\tdelicious/not\tdelicious)\tattributes\tand\ncan\teven\tclassify\tdata\tfor\twhich\tattributes\tare\tmissing.\n\nAt\tthe\tsame\ttime,\tfinding\tan\t\u201coptimal\u201d\tdecision\ttree\tfor\ta\tset\tof\ttraining\tdata\tis\ncomputationally\ta\tvery\thard\tproblem.\t(We\twill\tget\taround\tthis\tby\ttrying\tto\tbuild\ta\tgood-\nenough\ttree\trather\tthan\tan\toptimal\tone,\talthough\tfor\tlarge\tdata\tsets\tthis\tcan\tstill\tbe\ta\tlot\tof\nwork.)\tMore\timportant,\tit\tis\tvery\teasy\t(and\tvery\tbad)\tto\tbuild\tdecision\ttrees\tthat\tare\noverfitted\tto\tthe\ttraining\tdata,\tand\tthat\tdon\u2019t\tgeneralize\twell\tto\tunseen\tdata.\tWe\u2019ll\tlook\tat\nways\tto\taddress\tthis.\n\nMost\tpeople\tdivide\tdecision\ttrees\tinto\tclassification\ttrees\t(which\tproduce\tcategorical\noutputs)\tand\tregression\ttrees\t(which\tproduce\tnumeric\toutputs).\tIn\tthis\tchapter,\twe\u2019ll\tfocus\non\tclassification\ttrees,\tand\twe\u2019ll\twork\tthrough\tthe\tID3\talgorithm\tfor\tlearning\ta\tdecision\ntree\tfrom\ta\tset\tof\tlabeled\tdata,\twhich\tshould\thelp\tus\tunderstand\thow\tdecision\ttrees\nactually\twork.\tTo\tmake\tthings\tsimple,\twe\u2019ll\trestrict\tourselves\tto\tproblems\twith\tbinary\noutputs\tlike\t\u201cshould\tI\thire\tthis\tcandidate?\u201d\tor\t\u201cshould\tI\tshow\tthis\twebsite\tvisitor\nadvertisement\tA\tor\tadvertisement\tB?\u201d\tor\t\u201cwill\teating\tthis\tfood\tI\tfound\tin\tthe\toffice\tfridge\nmake\tme\tsick?\u201d",
    "288": "Entropy\n\nIn\torder\tto\tbuild\ta\tdecision\ttree,\twe\twill\tneed\tto\tdecide\twhat\tquestions\tto\task\tand\tin\twhat\norder.\tAt\teach\tstage\tof\tthe\ttree\tthere\tare\tsome\tpossibilities\twe\u2019ve\teliminated\tand\tsome\tthat\nwe\thaven\u2019t.\tAfter\tlearning\tthat\tan\tanimal\tdoesn\u2019t\thave\tmore\tthan\tfive\tlegs,\twe\u2019ve\neliminated\tthe\tpossibility\tthat\tit\u2019s\ta\tgrasshopper.\tWe\thaven\u2019t\teliminated\tthe\tpossibility\tthat\nit\u2019s\ta\tduck.\tEvery\tpossible\tquestion\tpartitions\tthe\tremaining\tpossibilities\taccording\tto\ttheir\nanswers.\n\nIdeally,\twe\u2019d\tlike\tto\tchoose\tquestions\twhose\tanswers\tgive\ta\tlot\tof\tinformation\tabout\twhat\nour\ttree\tshould\tpredict.\tIf\tthere\u2019s\ta\tsingle\tyes/no\tquestion\tfor\twhich\t\u201cyes\u201d\tanswers\talways\ncorrespond\tto\tTrue\toutputs\tand\t\u201cno\u201d\tanswers\tto\tFalse\toutputs\t(or\tvice\tversa),\tthis\twould\nbe\tan\tawesome\tquestion\tto\tpick.\tConversely,\ta\tyes/no\tquestion\tfor\twhich\tneither\tanswer\ngives\tyou\tmuch\tnew\tinformation\tabout\twhat\tthe\tprediction\tshould\tbe\tis\tprobably\tnot\ta\ngood\tchoice.\n\nWe\tcapture\tthis\tnotion\tof\t\u201chow\tmuch\tinformation\u201d\twith\tentropy.\tYou\thave\tprobably\theard\nthis\tused\tto\tmean\tdisorder.\tWe\tuse\tit\tto\trepresent\tthe\tuncertainty\tassociated\twith\tdata.\n\nImagine\tthat\twe\thave\ta\tset\tS\tof\tdata,\teach\tmember\tof\twhich\tis\tlabeled\tas\tbelonging\tto\tone\nof\ta\tfinite\tnumber\tof\tclasses\t\nthen\tthere\tis\tno\treal\tuncertainty,\twhich\tmeans\twe\u2019d\tlike\tthere\tto\tbe\tlow\tentropy.\tIf\tthe\tdata\npoints\tare\tevenly\tspread\tacross\tthe\tclasses,\tthere\tis\ta\tlot\tof\tuncertainty\tand\twe\u2019d\tlike\tthere\nto\tbe\thigh\tentropy.\n\n.\tIf\tall\tthe\tdata\tpoints\tbelong\tto\ta\tsingle\tclass,\n\nIn\tmath\tterms,\tif\t\n\n\tis\tthe\tproportion\tof\tdata\tlabeled\tas\tclass\t\n\n,\twe\tdefine\tthe\tentropy\tas:\n\nwith\tthe\t(standard)\tconvention\tthat\t\n\n.\n\nWithout\tworrying\ttoo\tmuch\tabout\tthe\tgrisly\tdetails,\teach\tterm\t\nand\tis\tclose\tto\tzero\tprecisely\twhen\t\n\n\tis\tnon-negative\n\tis\teither\tclose\tto\tzero\tor\tclose\tto\tone\t(Figure\t17-2).",
    "289": "Figure\t17-2.\tA\tgraph\tof\t-p\tlog\tp\n\nThis\tmeans\tthe\tentropy\twill\tbe\tsmall\twhen\tevery\t\nthe\tdata\tis\tin\ta\tsingle\tclass),\tand\tit\twill\tbe\tlarger\twhen\tmany\tof\tthe\t\n(i.e.,\twhen\tthe\tdata\tis\tspread\tacross\tmultiple\tclasses).\tThis\tis\texactly\tthe\tbehavior\twe\ndesire.\n\n\tis\tclose\tto\t0\tor\t1\t(i.e.,\twhen\tmost\tof\n\u2019s\tare\tnot\tclose\tto\t0\n\nIt\tis\teasy\tenough\tto\troll\tall\tof\tthis\tinto\ta\tfunction:\n\ndef\tentropy(class_probabilities):\n\t\t\t\t\"\"\"given\ta\tlist\tof\tclass\tprobabilities,\tcompute\tthe\tentropy\"\"\"\n\t\t\t\treturn\tsum(-p\t*\tmath.log(p,\t2)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tp\tin\tclass_probabilities\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tp)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tignore\tzero\tprobabilities\n\nOur\tdata\twill\tconsist\tof\tpairs\t(input,\tlabel),\twhich\tmeans\tthat\twe\u2019ll\tneed\tto\tcompute\nthe\tclass\tprobabilities\tourselves.\tObserve\tthat\twe\tdon\u2019t\tactually\tcare\twhich\tlabel\tis\nassociated\twith\teach\tprobability,\tonly\twhat\tthe\tprobabilities\tare:\n\ndef\tclass_probabilities(labels):\n\t\t\t\ttotal_count\t=\tlen(labels)\n\t\t\t\treturn\t[count\t/\ttotal_count\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tcount\tin\tCounter(labels).values()]\n\ndef\tdata_entropy(labeled_data):\n\t\t\t\tlabels\t=\t[label\tfor\t_,\tlabel\tin\tlabeled_data]\n\t\t\t\tprobabilities\t=\tclass_probabilities(labels)",
    "290": "return\tentropy(probabilities)",
    "291": "The\tEntropy\tof\ta\tPartition\n\nWhat\twe\u2019ve\tdone\tso\tfar\tis\tcompute\tthe\tentropy\t(think\t\u201cuncertainty\u201d)\tof\ta\tsingle\tset\tof\nlabeled\tdata.\tNow,\teach\tstage\tof\ta\tdecision\ttree\tinvolves\tasking\ta\tquestion\twhose\tanswer\npartitions\tdata\tinto\tone\tor\t(hopefully)\tmore\tsubsets.\tFor\tinstance,\tour\t\u201cdoes\tit\thave\tmore\nthan\tfive\tlegs?\u201d\tquestion\tpartitions\tanimals\tinto\tthose\twho\thave\tmore\tthan\tfive\tlegs\t(e.g.,\nspiders)\tand\tthose\tthat\tdon\u2019t\t(e.g.,\techidnas).\n\nCorrespondingly,\twe\u2019d\tlike\tsome\tnotion\tof\tthe\tentropy\tthat\tresults\tfrom\tpartitioning\ta\tset\nof\tdata\tin\ta\tcertain\tway.\tWe\twant\ta\tpartition\tto\thave\tlow\tentropy\tif\tit\tsplits\tthe\tdata\tinto\nsubsets\tthat\tthemselves\thave\tlow\tentropy\t(i.e.,\tare\thighly\tcertain),\tand\thigh\tentropy\tif\tit\ncontains\tsubsets\tthat\t(are\tlarge\tand)\thave\thigh\tentropy\t(i.e.,\tare\thighly\tuncertain).\n\nFor\texample,\tmy\t\u201cAustralian\tfive-cent\tcoin\u201d\tquestion\twas\tpretty\tdumb\t(albeit\tpretty\n\t=\t{echidna}\tand\t\nlucky!),\tas\tit\tpartitioned\tthe\tremaining\tanimals\tat\tthat\tpoint\tinto\t\n{everything\telse},\twhere\t\n\thas\tno\tentropy\tbut\tit\nrepresents\ta\tsmall\tfraction\tof\tthe\tremaining\t\u201cclasses.\u201d)\n\n\tis\tboth\tlarge\tand\thigh-entropy.\t(\n\n\t=\n\nMathematically,\tif\twe\tpartition\tour\tdata\tS\tinto\tsubsets\t\n\n\tcontaining\tproportions\n\n\tof\tthe\tdata,\tthen\twe\tcompute\tthe\tentropy\tof\tthe\tpartition\tas\ta\tweighted\tsum:\n\nwhich\twe\tcan\timplement\tas:\n\ndef\tpartition_entropy(subsets):\n\t\t\t\t\"\"\"find\tthe\tentropy\tfrom\tthis\tpartition\tof\tdata\tinto\tsubsets\n\t\t\t\tsubsets\tis\ta\tlist\tof\tlists\tof\tlabeled\tdata\"\"\"\n\n\t\t\t\ttotal_count\t=\tsum(len(subset)\tfor\tsubset\tin\tsubsets)\n\n\t\t\t\treturn\tsum(\tdata_entropy(subset)\t*\tlen(subset)\t/\ttotal_count\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tsubset\tin\tsubsets\t)\n\nNOTE\n\nOne\tproblem\twith\tthis\tapproach\tis\tthat\tpartitioning\tby\tan\tattribute\twith\tmany\tdifferent\tvalues\twill\tresult\tin\na\tvery\tlow\tentropy\tdue\tto\toverfitting.\tFor\texample,\timagine\tyou\twork\tfor\ta\tbank\tand\tare\ttrying\tto\tbuild\ta\ndecision\ttree\tto\tpredict\twhich\tof\tyour\tcustomers\tare\tlikely\tto\tdefault\ton\ttheir\tmortgages,\tusing\tsome\nhistorical\tdata\tas\tyour\ttraining\tset.\tImagine\tfurther\tthat\tthe\tdata\tset\tcontains\teach\tcustomer\u2019s\tSocial\nSecurity\tnumber.\tPartitioning\ton\tSSN\twill\tproduce\tone-person\tsubsets,\teach\tof\twhich\tnecessarily\thas\tzero\nentropy.\tBut\ta\tmodel\tthat\trelies\ton\tSSN\tis\tcertain\tnot\tto\tgeneralize\tbeyond\tthe\ttraining\tset.\tFor\tthis\treason,\nyou\tshould\tprobably\ttry\tto\tavoid\t(or\tbucket,\tif\tappropriate)\tattributes\twith\tlarge\tnumbers\tof\tpossible\tvalues\nwhen\tcreating\tdecision\ttrees.",
    "292": "Creating\ta\tDecision\tTree\n\nThe\tVP\tprovides\tyou\twith\tthe\tinterviewee\tdata,\tconsisting\tof\t(per\tyour\tspecification)\tpairs\n(input,\tlabel),\twhere\teach\tinput\tis\ta\tdict\tof\tcandidate\tattributes,\tand\teach\tlabel\tis\neither\tTrue\t(the\tcandidate\tinterviewed\twell)\tor\tFalse\t(the\tcandidate\tinterviewed\tpoorly).\nIn\tparticular,\tyou\tare\tprovided\twith\teach\tcandidate\u2019s\tlevel,\ther\tpreferred\tlanguage,\nwhether\tshe\tis\tactive\ton\tTwitter,\tand\twhether\tshe\thas\ta\tPhD:\n\ninputs\t=\t[\n\t\t\t\t({'level':'Senior',\t'lang':'Java',\t'tweets':'no',\t'phd':'no'},\t\t\t\tFalse),\n\t\t\t\t({'level':'Senior',\t'lang':'Java',\t'tweets':'no',\t'phd':'yes'},\t\t\tFalse),\n\t\t\t\t({'level':'Mid',\t'lang':'Python',\t'tweets':'no',\t'phd':'no'},\t\t\t\t\t\tTrue),\n\t\t\t\t({'level':'Junior',\t'lang':'Python',\t'tweets':'no',\t'phd':'no'},\t\t\tTrue),\n\t\t\t\t({'level':'Junior',\t'lang':'R',\t'tweets':'yes',\t'phd':'no'},\t\t\t\t\t\t\tTrue),\n\t\t\t\t({'level':'Junior',\t'lang':'R',\t'tweets':'yes',\t'phd':'yes'},\t\t\t\t\tFalse),\n\t\t\t\t({'level':'Mid',\t'lang':'R',\t'tweets':'yes',\t'phd':'yes'},\t\t\t\t\t\t\t\t\tTrue),\n\t\t\t\t({'level':'Senior',\t'lang':'Python',\t'tweets':'no',\t'phd':'no'},\t\tFalse),\n\t\t\t\t({'level':'Senior',\t'lang':'R',\t'tweets':'yes',\t'phd':'no'},\t\t\t\t\t\t\tTrue),\n\t\t\t\t({'level':'Junior',\t'lang':'Python',\t'tweets':'yes',\t'phd':'no'},\t\tTrue),\n\t\t\t\t({'level':'Senior',\t'lang':'Python',\t'tweets':'yes',\t'phd':'yes'},\tTrue),\n\t\t\t\t({'level':'Mid',\t'lang':'Python',\t'tweets':'no',\t'phd':'yes'},\t\t\t\t\tTrue),\n\t\t\t\t({'level':'Mid',\t'lang':'Java',\t'tweets':'yes',\t'phd':'no'},\t\t\t\t\t\t\tTrue),\n\t\t\t\t({'level':'Junior',\t'lang':'Python',\t'tweets':'no',\t'phd':'yes'},\tFalse)\n]\n\nOur\ttree\twill\tconsist\tof\tdecision\tnodes\t(which\task\ta\tquestion\tand\tdirect\tus\tdifferently\ndepending\ton\tthe\tanswer)\tand\tleaf\tnodes\t(which\tgive\tus\ta\tprediction).\tWe\twill\tbuild\tit\nusing\tthe\trelatively\tsimple\tID3\talgorithm,\twhich\toperates\tin\tthe\tfollowing\tmanner.\tLet\u2019s\nsay\twe\u2019re\tgiven\tsome\tlabeled\tdata,\tand\ta\tlist\tof\tattributes\tto\tconsider\tbranching\ton.\n\nIf\tthe\tdata\tall\thave\tthe\tsame\tlabel,\tthen\tcreate\ta\tleaf\tnode\tthat\tpredicts\tthat\tlabel\tand\nthen\tstop.\n\nIf\tthe\tlist\tof\tattributes\tis\tempty\t(i.e.,\tthere\tare\tno\tmore\tpossible\tquestions\tto\task),\tthen\ncreate\ta\tleaf\tnode\tthat\tpredicts\tthe\tmost\tcommon\tlabel\tand\tthen\tstop.\n\nOtherwise,\ttry\tpartitioning\tthe\tdata\tby\teach\tof\tthe\tattributes\n\nChoose\tthe\tpartition\twith\tthe\tlowest\tpartition\tentropy\n\nAdd\ta\tdecision\tnode\tbased\ton\tthe\tchosen\tattribute\n\nRecur\ton\teach\tpartitioned\tsubset\tusing\tthe\tremaining\tattributes\n\nThis\tis\twhat\u2019s\tknown\tas\ta\t\u201cgreedy\u201d\talgorithm\tbecause,\tat\teach\tstep,\tit\tchooses\tthe\tmost\nimmediately\tbest\toption.\tGiven\ta\tdata\tset,\tthere\tmay\tbe\ta\tbetter\ttree\twith\ta\tworse-looking\nfirst\tmove.\tIf\tso,\tthis\talgorithm\twon\u2019t\tfind\tit.\tNonetheless,\tit\tis\trelatively\teasy\tto\nunderstand\tand\timplement,\twhich\tmakes\tit\ta\tgood\tplace\tto\tbegin\texploring\tdecision\ttrees.\n\nLet\u2019s\tmanually\tgo\tthrough\tthese\tsteps\ton\tthe\tinterviewee\tdata\tset.\tThe\tdata\tset\thas\tboth\nTrue\tand\tFalse\tlabels,\tand\twe\thave\tfour\tattributes\twe\tcan\tsplit\ton.\tSo\tour\tfirst\tstep\twill\nbe\tto\tfind\tthe\tpartition\twith\tthe\tleast\tentropy.\tWe\u2019ll\tstart\tby\twriting\ta\tfunction\tthat\tdoes",
    "293": "the\tpartitioning:\n\ndef\tpartition_by(inputs,\tattribute):\n\t\t\t\t\"\"\"each\tinput\tis\ta\tpair\t(attribute_dict,\tlabel).\n\t\t\t\treturns\ta\tdict\t:\tattribute_value\t->\tinputs\"\"\"\n\t\t\t\tgroups\t=\tdefaultdict(list)\n\t\t\t\tfor\tinput\tin\tinputs:\n\t\t\t\t\t\t\t\tkey\t=\tinput[0][attribute]\t\t\t#\tget\tthe\tvalue\tof\tthe\tspecified\tattribute\n\t\t\t\t\t\t\t\tgroups[key].append(input)\t\t\t#\tthen\tadd\tthis\tinput\tto\tthe\tcorrect\tlist\n\t\t\t\treturn\tgroups\n\nand\tone\tthat\tuses\tit\tto\tcompute\tentropy:\n\ndef\tpartition_entropy_by(inputs,\tattribute):\n\t\t\t\t\"\"\"computes\tthe\tentropy\tcorresponding\tto\tthe\tgiven\tpartition\"\"\"\n\t\t\t\tpartitions\t=\tpartition_by(inputs,\tattribute)\n\t\t\t\treturn\tpartition_entropy(partitions.values())\n\nThen\twe\tjust\tneed\tto\tfind\tthe\tminimum-entropy\tpartition\tfor\tthe\twhole\tdata\tset:\n\nfor\tkey\tin\t['level','lang','tweets','phd']:\n\t\t\t\tprint\tkey,\tpartition_entropy_by(inputs,\tkey)\n\n#\tlevel\t0.693536138896\n#\tlang\t0.860131712855\n#\ttweets\t0.788450457308\n#\tphd\t0.892158928262\n\nThe\tlowest\tentropy\tcomes\tfrom\tsplitting\ton\tlevel,\tso\twe\u2019ll\tneed\tto\tmake\ta\tsubtree\tfor\neach\tpossible\tlevel\tvalue.\tEvery\tMid\tcandidate\tis\tlabeled\tTrue,\twhich\tmeans\tthat\tthe\tMid\nsubtree\tis\tsimply\ta\tleaf\tnode\tpredicting\tTrue.\tFor\tSenior\tcandidates,\twe\thave\ta\tmix\tof\nTrues\tand\tFalses,\tso\twe\tneed\tto\tsplit\tagain:\n\nsenior_inputs\t=\t[(input,\tlabel)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tinput,\tlabel\tin\tinputs\tif\tinput[\"level\"]\t==\t\"Senior\"]\n\nfor\tkey\tin\t['lang',\t'tweets',\t'phd']:\n\t\t\t\tprint\tkey,\tpartition_entropy_by(senior_inputs,\tkey)\n\n#\tlang\t0.4\n#\ttweets\t0.0\n#\tphd\t0.950977500433\n\nThis\tshows\tus\tthat\tour\tnext\tsplit\tshould\tbe\ton\ttweets,\twhich\tresults\tin\ta\tzero-entropy\npartition.\tFor\tthese\tSenior-level\tcandidates,\t\u201cyes\u201d\ttweets\talways\tresult\tin\tTrue\twhile\t\u201cno\u201d\ntweets\talways\tresult\tin\tFalse.\n\nFinally,\tif\twe\tdo\tthe\tsame\tthing\tfor\tthe\tJunior\tcandidates,\twe\tend\tup\tsplitting\ton\tphd,\nafter\twhich\twe\tfind\tthat\tno\tPhD\talways\tresults\tin\tTrue\tand\tPhD\talways\tresults\tin\tFalse.\n\nFigure\t17-3\tshows\tthe\tcomplete\tdecision\ttree.",
    "294": "Figure\t17-3.\tThe\tdecision\ttree\tfor\thiring",
    "295": "Putting\tIt\tAll\tTogether\n\nNow\tthat\twe\u2019ve\tseen\thow\tthe\talgorithm\tworks,\twe\twould\tlike\tto\timplement\tit\tmore\ngenerally.\tThis\tmeans\twe\tneed\tto\tdecide\thow\twe\twant\tto\trepresent\ttrees.\tWe\u2019ll\tuse\tpretty\nmuch\tthe\tmost\tlightweight\trepresentation\tpossible.\tWe\tdefine\ta\ttree\tto\tbe\tone\tof\tthe\nfollowing:\n\nTrue\n\nFalse\n\na\ttuple\t(attribute,\tsubtree_dict)\n\nHere\tTrue\trepresents\ta\tleaf\tnode\tthat\treturns\tTrue\tfor\tany\tinput,\tFalse\trepresents\ta\tleaf\nnode\tthat\treturns\tFalse\tfor\tany\tinput,\tand\ta\ttuple\trepresents\ta\tdecision\tnode\tthat,\tfor\tany\ninput,\tfinds\tits\tattribute\tvalue,\tand\tclassifies\tthe\tinput\tusing\tthe\tcorresponding\tsubtree.\n\nWith\tthis\trepresentation,\tour\thiring\ttree\twould\tlook\tlike:\n\n('level',\n\t{'Junior':\t('phd',\t{'no':\tTrue,\t'yes':\tFalse}),\n\t\t'Mid':\tTrue,\n\t\t'Senior':\t('tweets',\t{'no':\tFalse,\t'yes':\tTrue})})\n\nThere\u2019s\tstill\tthe\tquestion\tof\twhat\tto\tdo\tif\twe\tencounter\tan\tunexpected\t(or\tmissing)\nattribute\tvalue.\tWhat\tshould\tour\thiring\ttree\tdo\tif\tit\tencounters\ta\tcandidate\twhose\tlevel\tis\n\u201cIntern\u201d?\tWe\u2019ll\thandle\tthis\tcase\tby\tadding\ta\tNone\tkey\tthat\tjust\tpredicts\tthe\tmost\tcommon\nlabel.\t(Although\tthis\twould\tbe\ta\tbad\tidea\tif\tNone\tis\tactually\ta\tvalue\tthat\tappears\tin\tthe\ndata.)\n\nGiven\tsuch\ta\trepresentation,\twe\tcan\tclassify\tan\tinput\twith:\n\ndef\tclassify(tree,\tinput):\n\t\t\t\t\"\"\"classify\tthe\tinput\tusing\tthe\tgiven\tdecision\ttree\"\"\"\n\n\t\t\t\t#\tif\tthis\tis\ta\tleaf\tnode,\treturn\tits\tvalue\n\t\t\t\tif\ttree\tin\t[True,\tFalse]:\n\t\t\t\t\t\t\t\treturn\ttree\n\n\t\t\t\t#\totherwise\tthis\ttree\tconsists\tof\tan\tattribute\tto\tsplit\ton\n\t\t\t\t#\tand\ta\tdictionary\twhose\tkeys\tare\tvalues\tof\tthat\tattribute\n\t\t\t\t#\tand\twhose\tvalues\tof\tare\tsubtrees\tto\tconsider\tnext\n\t\t\t\tattribute,\tsubtree_dict\t=\ttree\n\n\t\t\t\tsubtree_key\t=\tinput.get(attribute)\t\t\t\t#\tNone\tif\tinput\tis\tmissing\tattribute\n\n\t\t\t\tif\tsubtree_key\tnot\tin\tsubtree_dict:\t\t\t#\tif\tno\tsubtree\tfor\tkey,\n\t\t\t\t\t\t\t\tsubtree_key\t=\tNone\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\twe'll\tuse\tthe\tNone\tsubtree\n\n\t\t\t\tsubtree\t=\tsubtree_dict[subtree_key]\t\t\t#\tchoose\tthe\tappropriate\tsubtree\n\t\t\t\treturn\tclassify(subtree,\tinput)\t\t\t\t\t\t\t#\tand\tuse\tit\tto\tclassify\tthe\tinput\n\nAll\tthat\u2019s\tleft\tis\tto\tbuild\tthe\ttree\trepresentation\tfrom\tour\ttraining\tdata:\n\ndef\tbuild_tree_id3(inputs,\tsplit_candidates=None):",
    "296": "#\tif\tthis\tis\tour\tfirst\tpass,\n\t\t\t\t#\tall\tkeys\tof\tthe\tfirst\tinput\tare\tsplit\tcandidates\n\t\t\t\tif\tsplit_candidates\tis\tNone:\n\t\t\t\t\t\t\t\tsplit_candidates\t=\tinputs[0][0].keys()\n\n\t\t\t\t#\tcount\tTrues\tand\tFalses\tin\tthe\tinputs\n\t\t\t\tnum_inputs\t=\tlen(inputs)\n\t\t\t\tnum_trues\t=\tlen([label\tfor\titem,\tlabel\tin\tinputs\tif\tlabel])\n\t\t\t\tnum_falses\t=\tnum_inputs\t-\tnum_trues\n\n\t\t\t\tif\tnum_trues\t==\t0:\treturn\tFalse\t\t\t\t\t#\tno\tTrues?\treturn\ta\t\"False\"\tleaf\n\t\t\t\tif\tnum_falses\t==\t0:\treturn\tTrue\t\t\t\t\t#\tno\tFalses?\treturn\ta\t\"True\"\tleaf\n\n\t\t\t\tif\tnot\tsplit_candidates:\t\t\t\t\t\t\t\t\t\t\t\t#\tif\tno\tsplit\tcandidates\tleft\n\t\t\t\t\t\t\t\treturn\tnum_trues\t>=\tnum_falses\t\t#\treturn\tthe\tmajority\tleaf\n\n\t\t\t\t#\totherwise,\tsplit\ton\tthe\tbest\tattribute\n\t\t\t\tbest_attribute\t=\tmin(split_candidates,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkey=partial(partition_entropy_by,\tinputs))\n\n\t\t\t\tpartitions\t=\tpartition_by(inputs,\tbest_attribute)\n\t\t\t\tnew_candidates\t=\t[a\tfor\ta\tin\tsplit_candidates\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\ta\t!=\tbest_attribute]\n\n\t\t\t\t#\trecursively\tbuild\tthe\tsubtrees\n\t\t\t\tsubtrees\t=\t{\tattribute_value\t:\tbuild_tree_id3(subset,\tnew_candidates)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tattribute_value,\tsubset\tin\tpartitions.iteritems()\t}\n\n\t\t\t\tsubtrees[None]\t=\tnum_trues\t>\tnum_falses\t\t\t\t\t\t#\tdefault\tcase\n\n\t\t\t\treturn\t(best_attribute,\tsubtrees)\n\nIn\tthe\ttree\twe\tbuilt,\tevery\tleaf\tconsisted\tentirely\tof\tTrue\tinputs\tor\tentirely\tof\tFalse\tinputs.\nThis\tmeans\tthat\tthe\ttree\tpredicts\tperfectly\ton\tthe\ttraining\tdata\tset.\tBut\twe\tcan\talso\tapply\tit\nto\tnew\tdata\tthat\twasn\u2019t\tin\tthe\ttraining\tset:\n\ntree\t=\tbuild_tree_id3(inputs)\n\nclassify(tree,\t{\t\"level\"\t:\t\"Junior\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"lang\"\t:\t\"Java\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"tweets\"\t:\t\"yes\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"phd\"\t:\t\"no\"}\t)\t\t\t\t\t\t\t\t#\tTrue\n\nclassify(tree,\t{\t\"level\"\t:\t\"Junior\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"lang\"\t:\t\"Java\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"tweets\"\t:\t\"yes\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"phd\"\t:\t\"yes\"}\t)\t\t\t\t\t\t\t#\tFalse\n\nAnd\talso\tto\tdata\twith\tmissing\tor\tunexpected\tvalues:\n\nclassify(tree,\t{\t\"level\"\t:\t\"Intern\"\t}\t)\t#\tTrue\nclassify(tree,\t{\t\"level\"\t:\t\"Senior\"\t}\t)\t#\tFalse\n\nSince\tour\tgoal\twas\tmainly\tto\tdemonstrate\thow\tto\tbuild\ta\ttree,\twe\tbuilt\tthe\ttree\tusing\tthe\tentire\tdata\tset.\tAs\nalways,\tif\twe\twere\treally\ttrying\tto\tcreate\ta\tgood\tmodel\tfor\tsomething,\twe\twould\thave\t(collected\tmore\tdata\nand)\tsplit\tthe\tdata\tinto\ttrain/validation/test\tsubsets.\n\nNOTE",
    "297": "Random\tForests\n\nGiven\thow\tclosely\tdecision\ttrees\tcan\tfit\tthemselves\tto\ttheir\ttraining\tdata,\tit\u2019s\tnot\nsurprising\tthat\tthey\thave\ta\ttendency\tto\toverfit.\tOne\tway\tof\tavoiding\tthis\tis\ta\ttechnique\ncalled\trandom\tforests,\tin\twhich\twe\tbuild\tmultiple\tdecision\ttrees\tand\tlet\tthem\tvote\ton\thow\nto\tclassify\tinputs:\n\ndef\tforest_classify(trees,\tinput):\n\t\t\t\tvotes\t=\t[classify(tree,\tinput)\tfor\ttree\tin\ttrees]\n\t\t\t\tvote_counts\t=\tCounter(votes)\n\t\t\t\treturn\tvote_counts.most_common(1)[0][0]\n\nOur\ttree-building\tprocess\twas\tdeterministic,\tso\thow\tdo\twe\tget\trandom\ttrees?\n\nOne\tpiece\tinvolves\tbootstrapping\tdata\t(recall\t\u201cDigression:\tThe\tBootstrap\u201d).\tRather\tthan\ntraining\teach\ttree\ton\tall\tthe\tinputs\tin\tthe\ttraining\tset,\twe\ttrain\teach\ttree\ton\tthe\tresult\tof\nbootstrap_sample(inputs).\tSince\teach\ttree\tis\tbuilt\tusing\tdifferent\tdata,\teach\ttree\twill\tbe\ndifferent\tfrom\tevery\tother\ttree.\t(A\tside\tbenefit\tis\tthat\tit\u2019s\ttotally\tfair\tto\tuse\tthe\tnonsampled\ndata\tto\ttest\teach\ttree,\twhich\tmeans\tyou\tcan\tget\taway\twith\tusing\tall\tof\tyour\tdata\tas\tthe\ntraining\tset\tif\tyou\tare\tclever\tin\thow\tyou\tmeasure\tperformance.)\tThis\ttechnique\tis\tknown\nas\tbootstrap\taggregating\tor\tbagging.\n\nA\tsecond\tsource\tof\trandomness\tinvolves\tchanging\tthe\tway\twe\tchose\tthe\tbest_attribute\nto\tsplit\ton.\tRather\tthan\tlooking\tat\tall\tthe\tremaining\tattributes,\twe\tfirst\tchoose\ta\trandom\nsubset\tof\tthem\tand\tthen\tsplit\ton\twhichever\tof\tthose\tis\tbest:\n\n\t\t\t\t#\tif\tthere's\talready\tfew\tenough\tsplit\tcandidates,\tlook\tat\tall\tof\tthem\n\t\t\t\tif\tlen(split_candidates)\t<=\tself.num_split_candidates:\n\t\t\t\t\t\t\t\tsampled_split_candidates\t=\tsplit_candidates\n\t\t\t\t#\totherwise\tpick\ta\trandom\tsample\n\t\t\t\telse:\n\t\t\t\t\t\t\t\tsampled_split_candidates\t=\trandom.sample(split_candidates,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.num_split_candidates)\n\n\t\t\t\t#\tnow\tchoose\tthe\tbest\tattribute\tonly\tfrom\tthose\tcandidates\n\t\t\t\tbest_attribute\t=\tmin(sampled_split_candidates,\n\t\t\t\t\t\t\t\tkey=partial(partition_entropy_by,\tinputs))\n\n\t\t\t\tpartitions\t=\tpartition_by(inputs,\tbest_attribute)\n\nThis\tis\tan\texample\tof\ta\tbroader\ttechnique\tcalled\tensemble\tlearning\tin\twhich\twe\tcombine\nseveral\tweak\tlearners\t(typically\thigh-bias,\tlow-variance\tmodels)\tin\torder\tto\tproduce\tan\noverall\tstrong\tmodel.\n\nRandom\tforests\tare\tone\tof\tthe\tmost\tpopular\tand\tversatile\tmodels\taround.",
    "298": "For\tFurther\tExploration\n\nscikit-learn\thas\tmany\tDecision\tTree\tmodels.\tIt\talso\thas\tan\tensemble\tmodule\tthat\nincludes\ta\tRandomForestClassifier\tas\twell\tas\tother\tensemble\tmethods.\n\nWe\tbarely\tscratched\tthe\tsurface\tof\tdecision\ttrees\tand\ttheir\talgorithms.\tWikipedia\tis\ta\ngood\tstarting\tpoint\tfor\tbroader\texploration.",
    "299": "",
    "300": "Chapter\t18.\tNeural\tNetworks\n\nI\tlike\tnonsense;\tit\twakes\tup\tthe\tbrain\tcells.\n\nDr.\tSeuss\n\nAn\tartificial\tneural\tnetwork\t(or\tneural\tnetwork\tfor\tshort)\tis\ta\tpredictive\tmodel\tmotivated\nby\tthe\tway\tthe\tbrain\toperates.\tThink\tof\tthe\tbrain\tas\ta\tcollection\tof\tneurons\twired\ttogether.\nEach\tneuron\tlooks\tat\tthe\toutputs\tof\tthe\tother\tneurons\tthat\tfeed\tinto\tit,\tdoes\ta\tcalculation,\nand\tthen\teither\tfires\t(if\tthe\tcalculation\texceeds\tsome\tthreshhold)\tor\tdoesn\u2019t\t(if\tit\tdoesn\u2019t).\n\nAccordingly,\tartificial\tneural\tnetworks\tconsist\tof\tartificial\tneurons,\twhich\tperform\tsimilar\ncalculations\tover\ttheir\tinputs.\tNeural\tnetworks\tcan\tsolve\ta\twide\tvariety\tof\tproblems\tlike\nhandwriting\trecognition\tand\tface\tdetection,\tand\tthey\tare\tused\theavily\tin\tdeep\tlearning,\none\tof\tthe\ttrendiest\tsubfields\tof\tdata\tscience.\tHowever,\tmost\tneural\tnetworks\tare\t\u201cblack\nboxes\u201d\t\u2014\tinspecting\ttheir\tdetails\tdoesn\u2019t\tgive\tyou\tmuch\tunderstanding\tof\thow\tthey\u2019re\nsolving\ta\tproblem.\tAnd\tlarge\tneural\tnetworks\tcan\tbe\tdifficult\tto\ttrain.\tFor\tmost\tproblems\nyou\u2019ll\tencounter\tas\ta\tbudding\tdata\tscientist,\tthey\u2019re\tprobably\tnot\tthe\tright\tchoice.\nSomeday,\twhen\tyou\u2019re\ttrying\tto\tbuild\tan\tartificial\tintelligence\tto\tbring\tabout\tthe\nSingularity,\tthey\tvery\twell\tmight\tbe.",
    "301": "Perceptrons\n\nPretty\tmuch\tthe\tsimplest\tneural\tnetwork\tis\tthe\tperceptron,\twhich\tapproximates\ta\tsingle\nneuron\twith\tn\tbinary\tinputs.\tIt\tcomputes\ta\tweighted\tsum\tof\tits\tinputs\tand\t\u201cfires\u201d\tif\tthat\nweighted\tsum\tis\tzero\tor\tgreater:\n\ndef\tstep_function(x):\n\t\t\t\treturn\t1\tif\tx\t>=\t0\telse\t0\n\ndef\tperceptron_output(weights,\tbias,\tx):\n\t\t\t\t\"\"\"returns\t1\tif\tthe\tperceptron\t'fires',\t0\tif\tnot\"\"\"\n\t\t\t\tcalculation\t=\tdot(weights,\tx)\t+\tbias\n\t\t\t\treturn\tstep_function(calculation)\n\nThe\tperceptron\tis\tsimply\tdistinguishing\tbetween\tthe\thalf\tspaces\tseparated\tby\tthe\nhyperplane\tof\tpoints\tx\tfor\twhich:\n\ndot(weights,x)\t+\tbias\t==\t0\n\nWith\tproperly\tchosen\tweights,\tperceptrons\tcan\tsolve\ta\tnumber\tof\tsimple\tproblems\n(Figure\t18-1).\tFor\texample,\twe\tcan\tcreate\tan\tAND\tgate\t(which\treturns\t1\tif\tboth\tits\tinputs\nare\t1\tbut\treturns\t0\tif\tone\tof\tits\tinputs\tis\t0)\twith:\n\nweights\t=\t[2,\t2]\nbias\t=\t-3\n\nIf\tboth\tinputs\tare\t1,\tthe\tcalculation\tequals\t2\t+\t2\t-\t3\t=\t1,\tand\tthe\toutput\tis\t1.\tIf\tonly\tone\nof\tthe\tinputs\tis\t1,\tthe\tcalculation\tequals\t2\t+\t0\t-\t3\t=\t-1,\tand\tthe\toutput\tis\t0.\tAnd\tif\tboth\tof\nthe\tinputs\tare\t0,\tthe\tcalculation\tequals\t-3,\tand\tthe\toutput\tis\t0.\n\nSimilarly,\twe\tcould\tbuild\tan\tOR\tgate\twith:\n\nweights\t=\t[2,\t2]\nbias\t=\t-1",
    "302": "Figure\t18-1.\tDecision\tspace\tfor\ta\ttwo-input\tperceptron\n\nAnd\twe\tcould\tbuild\ta\tNOT\tgate\t(which\thas\tone\tinput\tand\tconverts\t1\tto\t0\tand\t0\tto\t1)\twith:\n\nweights\t=\t[-2]\nbias\t=\t1\n\nHowever,\tthere\tare\tsome\tproblems\tthat\tsimply\tcan\u2019t\tbe\tsolved\tby\ta\tsingle\tperceptron.\tFor\nexample,\tno\tmatter\thow\thard\tyou\ttry,\tyou\tcannot\tuse\ta\tperceptron\tto\tbuild\tan\tXOR\tgate\nthat\toutputs\t1\tif\texactly\tone\tof\tits\tinputs\tis\t1\tand\t0\totherwise.\tThis\tis\twhere\twe\tstart\nneeding\tmore-complicated\tneural\tnetworks.\n\nOf\tcourse,\tyou\tdon\u2019t\tneed\tto\tapproximate\ta\tneuron\tin\torder\tto\tbuild\ta\tlogic\tgate:\n\nand_gate\t=\tmin\nor_gate\t=\tmax\nxor_gate\t=\tlambda\tx,\ty:\t0\tif\tx\t==\ty\telse\t1\n\nLike\treal\tneurons,\tartificial\tneurons\tstart\tgetting\tmore\tinteresting\twhen\tyou\tstart\nconnecting\tthem\ttogether.",
    "303": "Feed-Forward\tNeural\tNetworks\n\nThe\ttopology\tof\tthe\tbrain\tis\tenormously\tcomplicated,\tso\tit\u2019s\tcommon\tto\tapproximate\tit\nwith\tan\tidealized\tfeed-forward\tneural\tnetwork\tthat\tconsists\tof\tdiscrete\tlayers\tof\tneurons,\neach\tconnected\tto\tthe\tnext.\tThis\ttypically\tentails\tan\tinput\tlayer\t(which\treceives\tinputs\tand\nfeeds\tthem\tforward\tunchanged),\tone\tor\tmore\t\u201chidden\tlayers\u201d\t(each\tof\twhich\tconsists\tof\nneurons\tthat\ttake\tthe\toutputs\tof\tthe\tprevious\tlayer,\tperforms\tsome\tcalculation,\tand\tpasses\nthe\tresult\tto\tthe\tnext\tlayer),\tand\tan\toutput\tlayer\t(which\tproduces\tthe\tfinal\toutputs).\n\nJust\tlike\tthe\tperceptron,\teach\t(noninput)\tneuron\thas\ta\tweight\tcorresponding\tto\teach\tof\tits\ninputs\tand\ta\tbias.\tTo\tmake\tour\trepresentation\tsimpler,\twe\u2019ll\tadd\tthe\tbias\tto\tthe\tend\tof\tour\nweights\tvector\tand\tgive\teach\tneuron\ta\tbias\tinput\tthat\talways\tequals\t1.\n\nAs\twith\tthe\tperceptron,\tfor\teach\tneuron\twe\u2019ll\tsum\tup\tthe\tproducts\tof\tits\tinputs\tand\tits\nweights.\tBut\there,\trather\tthan\toutputting\tthe\tstep_function\tapplied\tto\tthat\tproduct,\twe\u2019ll\noutput\ta\tsmooth\tapproximation\tof\tthe\tstep\tfunction.\tIn\tparticular,\twe\u2019ll\tuse\tthe\tsigmoid\nfunction\t(Figure\t18-2):\n\ndef\tsigmoid(t):\n\t\t\t\treturn\t1\t/\t(1\t+\tmath.exp(-t))\n\nFigure\t18-2.\tThe\tsigmoid\tfunction",
    "304": "Why\tuse\tsigmoid\tinstead\tof\tthe\tsimpler\tstep_function?\tIn\torder\tto\ttrain\ta\tneural\nnetwork,\twe\u2019ll\tneed\tto\tuse\tcalculus,\tand\tin\torder\tto\tuse\tcalculus,\twe\tneed\tsmooth\nfunctions.\tThe\tstep\tfunction\tisn\u2019t\teven\tcontinuous,\tand\tsigmoid\tis\ta\tgood\tsmooth\napproximation\tof\tit.\n\nYou\tmay\tremember\tsigmoid\tfrom\tChapter\t16,\twhere\tit\twas\tcalled\tlogistic.\tTechnically\t\u201csigmoid\u201d\trefers\nto\tthe\tshape\tof\tthe\tfunction,\t\u201clogistic\u201d\tto\tthis\tparticular\tfunction\talthough\tpeople\toften\tuse\tthe\tterms\ninterchangeably.\n\nNOTE\n\nWe\tthen\tcalculate\tthe\toutput\tas:\n\ndef\tneuron_output(weights,\tinputs):\n\t\t\t\treturn\tsigmoid(dot(weights,\tinputs))\n\nGiven\tthis\tfunction,\twe\tcan\trepresent\ta\tneuron\tsimply\tas\ta\tlist\tof\tweights\twhose\tlength\tis\none\tmore\tthan\tthe\tnumber\tof\tinputs\tto\tthat\tneuron\t(because\tof\tthe\tbias\tweight).\tThen\twe\ncan\trepresent\ta\tneural\tnetwork\tas\ta\tlist\tof\t(noninput)\tlayers,\twhere\teach\tlayer\tis\tjust\ta\tlist\nof\tthe\tneurons\tin\tthat\tlayer.\n\nThat\tis,\twe\u2019ll\trepresent\ta\tneural\tnetwork\tas\ta\tlist\t(layers)\tof\tlists\t(neurons)\tof\tlists\n(weights).\n\nGiven\tsuch\ta\trepresentation,\tusing\tthe\tneural\tnetwork\tis\tquite\tsimple:\n\ndef\tfeed_forward(neural_network,\tinput_vector):\n\t\t\t\t\"\"\"takes\tin\ta\tneural\tnetwork\n\t\t\t\t(represented\tas\ta\tlist\tof\tlists\tof\tlists\tof\tweights)\n\t\t\t\tand\treturns\tthe\toutput\tfrom\tforward-propagating\tthe\tinput\"\"\"\n\n\t\t\t\toutputs\t=\t[]\n\n\t\t\t\t#\tprocess\tone\tlayer\tat\ta\ttime\n\t\t\t\tfor\tlayer\tin\tneural_network:\n\t\t\t\t\t\t\t\tinput_with_bias\t=\tinput_vector\t+\t[1]\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tadd\ta\tbias\tinput\n\t\t\t\t\t\t\t\toutput\t=\t[neuron_output(neuron,\tinput_with_bias)\t\t#\tcompute\tthe\toutput\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tneuron\tin\tlayer]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tfor\teach\tneuron\n\t\t\t\t\t\t\t\toutputs.append(output)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tand\tremember\tit\n\n\t\t\t\t\t\t\t\t#\tthen\tthe\tinput\tto\tthe\tnext\tlayer\tis\tthe\toutput\tof\tthis\tone\n\t\t\t\t\t\t\t\tinput_vector\t=\toutput\n\n\t\t\t\treturn\toutputs\n\nNow\tit\u2019s\teasy\tto\tbuild\tthe\tXOR\tgate\tthat\twe\tcouldn\u2019t\tbuild\twith\ta\tsingle\tperceptron.\tWe\njust\tneed\tto\tscale\tthe\tweights\tup\tso\tthat\tthe\tneuron_outputs\tare\teither\treally\tclose\tto\t0\tor\nreally\tclose\tto\t1:\n\nxor_network\t=\t[#\thidden\tlayer\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[[20,\t20,\t-30],\t\t\t\t\t\t#\t'and'\tneuron\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[20,\t20,\t-10]],\t\t\t\t\t#\t'or'\t\tneuron\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\toutput\tlayer\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[[-60,\t60,\t-30]]]\t\t\t\t#\t'2nd\tinput\tbut\tnot\t1st\tinput'\tneuron\n\nfor\tx\tin\t[0,\t1]:\n\t\t\t\tfor\ty\tin\t[0,\t1]:\n\t\t\t\t\t\t\t\t#\tfeed_forward\tproduces\tthe\toutputs\tof\tevery\tneuron\n\t\t\t\t\t\t\t\t#\tfeed_forward[-1]\tis\tthe\toutputs\tof\tthe\toutput-layer\tneurons",
    "305": "print\tx,\ty,\tfeed_forward(xor_network,[x,\ty])[-1]\n\n#\t0\t0\t[9.38314668300676e-14]\n#\t0\t1\t[0.9999999999999059]\n#\t1\t0\t[0.9999999999999059]\n#\t1\t1\t[9.383146683006828e-14]\n\nBy\tusing\ta\thidden\tlayer,\twe\tare\table\tto\tfeed\tthe\toutput\tof\tan\t\u201cand\u201d\tneuron\tand\tthe\toutput\nof\tan\t\u201cor\u201d\tneuron\tinto\ta\t\u201csecond\tinput\tbut\tnot\tfirst\tinput\u201d\tneuron.\tThe\tresult\tis\ta\tnetwork\nthat\tperforms\t\u201cor,\tbut\tnot\tand,\u201d\twhich\tis\tprecisely\tXOR\t(Figure\t18-3).\n\nFigure\t18-3.\tA\tneural\tnetwork\tfor\tXOR",
    "306": "Backpropagation\n\nUsually\twe\tdon\u2019t\tbuild\tneural\tnetworks\tby\thand.\tThis\tis\tin\tpart\tbecause\twe\tuse\tthem\tto\nsolve\tmuch\tbigger\tproblems\t\u2014\tan\timage\trecognition\tproblem\tmight\tinvolve\thundreds\tor\nthousands\tof\tneurons.\tAnd\tit\u2019s\tin\tpart\tbecause\twe\tusually\twon\u2019t\tbe\table\tto\t\u201creason\tout\u201d\nwhat\tthe\tneurons\tshould\tbe.\n\nInstead\t(as\tusual)\twe\tuse\tdata\tto\ttrain\tneural\tnetworks.\tOne\tpopular\tapproach\tis\tan\nalgorithm\tcalled\tbackpropagation\tthat\thas\tsimilarities\tto\tthe\tgradient\tdescent\talgorithm\nwe\tlooked\tat\tearlier.\n\nImagine\twe\thave\ta\ttraining\tset\tthat\tconsists\tof\tinput\tvectors\tand\tcorresponding\ttarget\noutput\tvectors.\tFor\texample,\tin\tour\tprevious\txor_network\texample,\tthe\tinput\tvector\t[1,\n0]\tcorresponded\tto\tthe\ttarget\toutput\t[1].\tAnd\timagine\tthat\tour\tnetwork\thas\tsome\tset\tof\nweights.\tWe\tthen\tadjust\tthe\tweights\tusing\tthe\tfollowing\talgorithm:\n\n1.\t Run\tfeed_forward\ton\tan\tinput\tvector\tto\tproduce\tthe\toutputs\tof\tall\tthe\tneurons\tin\tthe\n\nnetwork.\n\n2.\t This\tresults\tin\tan\terror\tfor\teach\toutput\tneuron\t\u2014\tthe\tdifference\tbetween\tits\toutput\n\nand\tits\ttarget.\n\n3.\t Compute\tthe\tgradient\tof\tthis\terror\tas\ta\tfunction\tof\tthe\tneuron\u2019s\tweights,\tand\tadjust\n\nits\tweights\tin\tthe\tdirection\tthat\tmost\tdecreases\tthe\terror.\n\n4.\t \u201cPropagate\u201d\tthese\toutput\terrors\tbackward\tto\tinfer\terrors\tfor\tthe\thidden\tlayer.\n\n5.\t Compute\tthe\tgradients\tof\tthese\terrors\tand\tadjust\tthe\thidden\tlayer\u2019s\tweights\tin\tthe\n\nsame\tmanner.\n\nTypically\twe\trun\tthis\talgorithm\tmany\ttimes\tfor\tour\tentire\ttraining\tset\tuntil\tthe\tnetwork\nconverges:\n\ndef\tbackpropagate(network,\tinput_vector,\ttargets):\n\n\t\t\t\thidden_outputs,\toutputs\t=\tfeed_forward(network,\tinput_vector)\n\n\t\t\t\t#\tthe\toutput\t*\t(1\t-\toutput)\tis\tfrom\tthe\tderivative\tof\tsigmoid\n\t\t\t\toutput_deltas\t=\t[output\t*\t(1\t-\toutput)\t*\t(output\t-\ttarget)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\toutput,\ttarget\tin\tzip(outputs,\ttargets)]\n\n\t\t\t\t#\tadjust\tweights\tfor\toutput\tlayer,\tone\tneuron\tat\ta\ttime\n\t\t\t\tfor\ti,\toutput_neuron\tin\tenumerate(network[-1]):\n\t\t\t\t\t\t\t\t#\tfocus\ton\tthe\tith\toutput\tlayer\tneuron\n\t\t\t\t\t\t\t\tfor\tj,\thidden_output\tin\tenumerate(hidden_outputs\t+\t[1]):\n\t\t\t\t\t\t\t\t\t\t\t\t#\tadjust\tthe\tjth\tweight\tbased\ton\tboth\n\t\t\t\t\t\t\t\t\t\t\t\t#\tthis\tneuron's\tdelta\tand\tits\tjth\tinput\n\t\t\t\t\t\t\t\t\t\t\t\toutput_neuron[j]\t-=\toutput_deltas[i]\t*\thidden_output\n\n\t\t\t\t#\tback-propagate\terrors\tto\thidden\tlayer\n\t\t\t\thidden_deltas\t=\t[hidden_output\t*\t(1\t-\thidden_output)\t*\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdot(output_deltas,\t[n[i]\tfor\tn\tin\toutput_layer])\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\ti,\thidden_output\tin\tenumerate(hidden_outputs)]\n\n\t\t\t\t#\tadjust\tweights\tfor\thidden\tlayer,\tone\tneuron\tat\ta\ttime\n\t\t\t\tfor\ti,\thidden_neuron\tin\tenumerate(network[0]):\n\t\t\t\t\t\t\t\tfor\tj,\tinput\tin\tenumerate(input_vector\t+\t[1]):",
    "307": "hidden_neuron[j]\t-=\thidden_deltas[i]\t*\tinput\n\nThis\tis\tpretty\tmuch\tdoing\tthe\tsame\tthing\tas\tif\tyou\texplicitly\twrote\tthe\tsquared\terror\tas\ta\nfunction\tof\tthe\tweights\tand\tused\tthe\tminimize_stochastic\tfunction\twe\tbuilt\tin\nChapter\t8.\n\nIn\tthis\tcase,\texplicitly\twriting\tout\tthe\tgradient\tfunction\tturns\tout\tto\tbe\tkind\tof\ta\tpain.\tIf\nyou\tknow\tcalculus\tand\tthe\tchain\trule,\tthe\tmathematical\tdetails\tare\trelatively\nstraightforward,\tbut\tkeeping\tthe\tnotation\tstraight\t(\u201cthe\tpartial\tderivative\tof\tthe\terror\nfunction\twith\trespect\tto\tthe\tweight\tthat\tneuron\ti\tassigns\tto\tthe\tinput\tcoming\tfrom\tneuron\nj\u201d)\tis\tnot\tmuch\tfun.",
    "308": "Example:\tDefeating\ta\tCAPTCHA\n\nTo\tmake\tsure\tthat\tpeople\tregistering\tfor\tyour\tsite\tare\tactually\tpeople,\tthe\tVP\tof\tProduct\nManagement\twants\tto\timplement\ta\tCAPTCHA\tas\tpart\tof\tthe\tregistration\tprocess.\tIn\nparticular,\the\u2019d\tlike\tto\tshow\tusers\ta\tpicture\tof\ta\tdigit\tand\trequire\tthem\tto\tinput\tthat\tdigit\tto\nprove\tthey\u2019re\thuman.\n\nHe\tdoesn\u2019t\tbelieve\tyou\tthat\tcomputers\tcan\teasily\tsolve\tthis\tproblem,\tso\tyou\tdecide\tto\nconvince\thim\tby\tcreating\ta\tprogram\tthat\tcan\teasily\tsolve\tthe\tproblem.\n\nWe\u2019ll\trepresent\teach\tdigit\tas\ta\t5\t\u00d7\t5\timage:\n\n@@@@@\t\t..@..\t\t@@@@@\t\t@@@@@\t\t@...@\t\t@@@@@\t\t@@@@@\t\t@@@@@\t\t@@@@@\t\t@@@@@\n@...@\t\t..@..\t\t....@\t\t....@\t\t@...@\t\t@....\t\t@....\t\t....@\t\t@...@\t\t@...@\n@...@\t\t..@..\t\t@@@@@\t\t@@@@@\t\t@@@@@\t\t@@@@@\t\t@@@@@\t\t....@\t\t@@@@@\t\t@@@@@\n@...@\t\t..@..\t\t@....\t\t....@\t\t....@\t\t....@\t\t@...@\t\t....@\t\t@...@\t\t....@\n@@@@@\t\t..@..\t\t@@@@@\t\t@@@@@\t\t....@\t\t@@@@@\t\t@@@@@\t\t....@\t\t@@@@@\t\t@@@@@\n\nOur\tneural\tnetwork\twants\tan\tinput\tto\tbe\ta\tvector\tof\tnumbers.\tSo\twe\u2019ll\ttransform\teach\nimage\tto\ta\tvector\tof\tlength\t25,\twhose\telements\tare\teither\t1\t(\u201cthis\tpixel\tis\tin\tthe\timage\u201d)\tor\n0\t(\u201cthis\tpixel\tis\tnot\tin\tthe\timage\u201d).\n\nFor\tinstance,\tthe\tzero\tdigit\twould\tbe\trepresented\tas:\n\nzero_digit\t=\t[1,1,1,1,1,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t1,0,0,0,1,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t1,0,0,0,1,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t1,0,0,0,1,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t1,1,1,1,1]\n\nWe\u2019ll\twant\tour\toutput\tto\tindicate\twhich\tdigit\tthe\tneural\tnetwork\tthinks\tit\tis,\tso\twe\u2019ll\tneed\n10\toutputs.\tThe\tcorrect\toutput\tfor\tdigit\t4,\tfor\tinstance,\twill\tbe:\n\n[0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0]\n\nThen,\tassuming\tour\tinputs\tare\tcorrectly\tordered\tfrom\t0\tto\t9,\tour\ttargets\twill\tbe:\n\ntargets\t=\t[[1\tif\ti\t==\tj\telse\t0\tfor\ti\tin\trange(10)]\n\t\t\t\t\t\t\t\t\t\t\tfor\tj\tin\trange(10)]\n\nso\tthat\t(for\texample)\ttargets[4]\tis\tthe\tcorrect\toutput\tfor\tdigit\t4.\n\nAt\twhich\tpoint\twe\u2019re\tready\tto\tbuild\tour\tneural\tnetwork:\n\nrandom.seed(0)\t\t\t\t\t\t#\tto\tget\trepeatable\tresults\ninput_size\t=\t25\t\t\t\t\t#\teach\tinput\tis\ta\tvector\tof\tlength\t25\nnum_hidden\t=\t5\t\t\t\t\t\t#\twe'll\thave\t5\tneurons\tin\tthe\thidden\tlayer\noutput_size\t=\t10\t\t\t\t#\twe\tneed\t10\toutputs\tfor\teach\tinput\n\n#\teach\thidden\tneuron\thas\tone\tweight\tper\tinput,\tplus\ta\tbias\tweight\nhidden_layer\t=\t[[random.random()\tfor\t__\tin\trange(input_size\t+\t1)]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\t__\tin\trange(num_hidden)]\n\n#\teach\toutput\tneuron\thas\tone\tweight\tper\thidden\tneuron,\tplus\ta\tbias\tweight\noutput_layer\t=\t[[random.random()\tfor\t__\tin\trange(num_hidden\t+\t1)]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\t__\tin\trange(output_size)]",
    "309": "#\tthe\tnetwork\tstarts\tout\twith\trandom\tweights\nnetwork\t=\t[hidden_layer,\toutput_layer]\n\nAnd\twe\tcan\ttrain\tit\tusing\tthe\tbackpropagation\talgorithm:\n\n#\t10,000\titerations\tseems\tenough\tto\tconverge\nfor\t__\tin\trange(10000):\n\t\t\t\tfor\tinput_vector,\ttarget_vector\tin\tzip(inputs,\ttargets):\n\t\t\t\t\t\t\t\tbackpropagate(network,\tinput_vector,\ttarget_vector)\n\nIt\tworks\twell\ton\tthe\ttraining\tset,\tobviously:\n\ndef\tpredict(input):\n\t\t\t\treturn\tfeed_forward(network,\tinput)[-1]\n\npredict(inputs[7])\n#\t[0.026,\t0.0,\t0.0,\t0.018,\t0.001,\t0.0,\t0.0,\t0.967,\t0.0,\t0.0]\n\nWhich\tindicates\tthat\tthe\tdigit\t7\toutput\tneuron\tproduces\t0.97,\twhile\tall\tthe\tother\toutput\nneurons\tproduce\tvery\tsmall\tnumbers.\n\nBut\twe\tcan\talso\tapply\tit\tto\tdifferently\tdrawn\tdigits,\tlike\tmy\tstylized\t3:\n\npredict([0,1,1,1,0,\t\t#\t.@@@.\n\t\t\t\t\t\t\t\t\t0,0,0,1,1,\t\t#\t...@@\n\t\t\t\t\t\t\t\t\t0,0,1,1,0,\t\t#\t..@@.\n\t\t\t\t\t\t\t\t\t0,0,0,1,1,\t\t#\t...@@\n\t\t\t\t\t\t\t\t\t0,1,1,1,0])\t#\t.@@@.\n\n#\t[0.0,\t0.0,\t0.0,\t0.92,\t0.0,\t0.0,\t0.0,\t0.01,\t0.0,\t0.12]\n\nThe\tnetwork\tstill\tthinks\tit\tlooks\tlike\ta\t3,\twhereas\tmy\tstylized\t8\tgets\tvotes\tfor\tbeing\ta\t5,\tan\n8,\tand\ta\t9:\n\npredict([0,1,1,1,0,\t\t#\t.@@@.\n\t\t\t\t\t\t\t\t\t1,0,0,1,1,\t\t#\t@..@@\n\t\t\t\t\t\t\t\t\t0,1,1,1,0,\t\t#\t.@@@.\n\t\t\t\t\t\t\t\t\t1,0,0,1,1,\t\t#\t@..@@\n\t\t\t\t\t\t\t\t\t0,1,1,1,0])\t#\t.@@@.\n\n#\t[0.0,\t0.0,\t0.0,\t0.0,\t0.0,\t0.55,\t0.0,\t0.0,\t0.93,\t1.0]\n\nHaving\ta\tlarger\ttraining\tset\twould\tprobably\thelp.\n\nAlthough\tthe\tnetwork\u2019s\toperation\tis\tnot\texactly\ttransparent,\twe\tcan\tinspect\tthe\tweights\tof\nthe\thidden\tlayer\tto\tget\ta\tsense\tof\twhat\tthey\u2019re\trecognizing.\tIn\tparticular,\twe\tcan\tplot\tthe\nweights\tof\teach\tneuron\tas\ta\t5\t\u00d7\t5\tgrid\tcorresponding\tto\tthe\t5\t\u00d7\t5\tinputs.\n\nIn\treal\tlife\tyou\u2019d\tprobably\twant\tto\tplot\tzero\tweights\tas\twhite,\twith\tlarger\tpositive\tweights\nmore\tand\tmore\t(say)\tgreen\tand\tlarger\tnegative\tweights\tmore\tand\tmore\t(say)\tred.\nUnfortunately,\tthat\u2019s\thard\tto\tdo\tin\ta\tblack-and-white\tbook.\n\nInstead,\twe\u2019ll\tplot\tzero\tweights\tas\twhite,\twith\tfar-away-from-zero\tweights\tdarker\tand\ndarker.\tAnd\twe\u2019ll\tuse\tcrosshatching\tto\tindicate\tnegative\tweights.\n\nTo\tdo\tthis\twe\u2019ll\tuse\tpyplot.imshow,\twhich\twe\thaven\u2019t\tseen\tbefore.\tWith\tit\twe\tcan\tplot",
    "310": "images\tpixel\tby\tpixel.\tNormally\tthis\tisn\u2019t\tall\tthat\tuseful\tfor\tdata\tscience,\tbut\there\tit\u2019s\ta\ngood\tchoice:\n\nimport\tmatplotlib\nweights\t=\tnetwork[0][0]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tfirst\tneuron\tin\thidden\tlayer\nabs_weights\t=\tmap(abs,\tweights)\t\t\t\t\t\t\t#\tdarkness\tonly\tdepends\ton\tabsolute\tvalue\n\ngrid\t=\t[abs_weights[row:(row+5)]\t\t\t\t\t\t#\tturn\tthe\tweights\tinto\ta\t5x5\tgrid\n\t\t\t\t\t\t\t\tfor\trow\tin\trange(0,25,5)]\t\t\t\t\t#\t[weights[0:5],\t...,\tweights[20:25]]\n\nax\t=\tplt.gca()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tto\tuse\thatching,\twe'll\tneed\tthe\taxis\n\nax.imshow(grid,\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\there\tsame\tas\tplt.imshow\n\t\t\t\t\t\t\t\t\t\tcmap=matplotlib.cm.binary,\t\t#\tuse\twhite-black\tcolor\tscale\n\t\t\t\t\t\t\t\t\t\tinterpolation='none')\t\t\t\t\t\t\t#\tplot\tblocks\tas\tblocks\n\ndef\tpatch(x,\ty,\thatch,\tcolor):\n\t\t\t\t\"\"\"return\ta\tmatplotlib\t'patch'\tobject\twith\tthe\tspecified\n\t\t\t\tlocation,\tcrosshatch\tpattern,\tand\tcolor\"\"\"\n\t\t\t\treturn\tmatplotlib.patches.Rectangle((x\t-\t0.5,\ty\t-\t0.5),\t1,\t1,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\thatch=hatch,\tfill=False,\tcolor=color)\n\n#\tcross-hatch\tthe\tnegative\tweights\nfor\ti\tin\trange(5):\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\trow\n\t\t\t\tfor\tj\tin\trange(5):\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tcolumn\n\t\t\t\t\t\t\t\tif\tweights[5*i\t+\tj]\t<\t0:\t\t\t\t\t\t#\trow\ti,\tcolumn\tj\t=\tweights[5*i\t+\tj]\n\t\t\t\t\t\t\t\t\t\t\t\t#\tadd\tblack\tand\twhite\thatches,\tso\tvisible\twhether\tdark\tor\tlight\n\t\t\t\t\t\t\t\t\t\t\t\tax.add_patch(patch(j,\ti,\t'/',\t\t\"white\"))\n\t\t\t\t\t\t\t\t\t\t\t\tax.add_patch(patch(j,\ti,\t'\\\\',\t\"black\"))\n\nplt.show()\n\nFigure\t18-4.\tWeights\tfor\tthe\thidden\tlayer\n\nIn\tFigure\t18-4\twe\tsee\tthat\tthe\tfirst\thidden\tneuron\thas\tlarge\tpositive\tweights\tin\tthe\tleft\ncolumn\tand\tin\tthe\tcenter\tof\tthe\tmiddle\trow,\twhile\tit\thas\tlarge\tnegative\tweights\tin\tthe\tright\ncolumn.\t(And\tyou\tcan\tsee\tthat\tit\thas\ta\tpretty\tlarge\tnegative\tbias,\twhich\tmeans\tthat\tit\nwon\u2019t\tfire\tstrongly\tunless\tit\tgets\tprecisely\tthe\tpositive\tinputs\tit\u2019s\t\u201clooking\tfor.\u201d)\n\nIndeed,\ton\tthose\tinputs,\tit\tdoes\twhat\tyou\u2019d\texpect:\n\nleft_column_only\t=\t[1,\t0,\t0,\t0,\t0]\t*\t5\nprint\tfeed_forward(network,\tleft_column_only)[0][0]\t\t#\t1.0\n\ncenter_middle_row\t=\t[0,\t0,\t0,\t0,\t0]\t*\t2\t+\t[0,\t1,\t1,\t1,\t0]\t+\t[0,\t0,\t0,\t0,\t0]\t*\t2\nprint\tfeed_forward(network,\tcenter_middle_row)[0][0]\t\t#\t0.95\n\nright_column_only\t=\t[0,\t0,\t0,\t0,\t1]\t*\t5\nprint\tfeed_forward(network,\tright_column_only)[0][0]\t\t#\t0.0\n\nSimilarly,\tthe\tmiddle\thidden\tneuron\tseems\tto\t\u201clike\u201d\thorizontal\tlines\tbut\tnot\tside\tvertical",
    "311": "lines,\tand\tthe\tlast\thidden\tneuron\tseems\tto\t\u201clike\u201d\tthe\tcenter\trow\tbut\tnot\tthe\tright\tcolumn.\n(The\tother\ttwo\tneurons\tare\tharder\tto\tinterpret.)\n\nWhat\thappens\twhen\twe\trun\tmy\tstylized\t3\tthrough\tthe\tnetwork?\n\nmy_three\t=\t\t[0,1,1,1,0,\t\t#\t.@@@.\n\t\t\t\t\t\t\t\t\t\t\t\t\t0,0,0,1,1,\t\t#\t...@@\n\t\t\t\t\t\t\t\t\t\t\t\t\t0,0,1,1,0,\t\t#\t..@@.\n\t\t\t\t\t\t\t\t\t\t\t\t\t0,0,0,1,1,\t\t#\t...@@\n\t\t\t\t\t\t\t\t\t\t\t\t\t0,1,1,1,0]\t\t#\t.@@@.\n\nhidden,\toutput\t=\tfeed_forward(network,\tmy_three)\n\nThe\thidden\toutputs\tare:\n\n0.121080\t\t#\tfrom\tnetwork[0][0],\tprobably\tdinged\tby\t(1,\t4)\n0.999979\t\t#\tfrom\tnetwork[0][1],\tbig\tcontributions\tfrom\t(0,\t2)\tand\t(2,\t2)\n0.999999\t\t#\tfrom\tnetwork[0][2],\tpositive\teverywhere\texcept\t(3,\t4)\n0.999992\t\t#\tfrom\tnetwork[0][3],\tagain\tbig\tcontributions\tfrom\t(0,\t2)\tand\t(2,\t2)\n0.000000\t\t#\tfrom\tnetwork[0][4],\tnegative\tor\tzero\teverywhere\texcept\tcenter\trow\n\nwhich\tenter\tinto\tthe\t\u201cthree\u201d\toutput\tneuron\twith\tweights\tnetwork[-1][3]:\n\n-11.61\t\t#\tweight\tfor\thidden[0]\n\t-2.17\t\t#\tweight\tfor\thidden[1]\n\t\t9.31\t\t#\tweight\tfor\thidden[2]\n\t-1.38\t\t#\tweight\tfor\thidden[3]\n-11.47\t\t#\tweight\tfor\thidden[4]\n-\t1.92\t\t#\tweight\tfor\tbias\tinput\n\nSo\tthat\tthe\tneuron\tcomputes:\n\nsigmoid(.121\t*\t-11.61\t+\t1\t*\t-2.17\t+\t1\t*\t9.31\t-\t1.38\t*\t1\t-\t0\t*\t11.47\t-\t1.92)\n\nwhich\tis\t0.92,\tas\twe\tsaw.\tIn\tessence,\tthe\thidden\tlayer\tis\tcomputing\tfive\tdifferent\npartitions\tof\t25-dimensional\tspace,\tmapping\teach\t25-dimensional\tinput\tdown\tto\tfive\nnumbers.\tAnd\tthen\teach\toutput\tneuron\tlooks\tonly\tat\tthe\tresults\tof\tthose\tfive\tpartitions.\n\nAs\twe\tsaw,\tmy_three\tfalls\tslightly\ton\tthe\t\u201clow\u201d\tside\tof\tpartition\t0\t(i.e.,\tonly\tslightly\nactivates\thidden\tneuron\t0),\tfar\ton\tthe\t\u201chigh\u201d\tside\tof\tpartitions\t1,\t2,\tand\t3,\t(i.e.,\tstrongly\nactivates\tthose\thidden\tneurons),\tand\tfar\ton\tthe\tlow\tside\tof\tpartition\t4\t(i.e.,\tdoesn\u2019t\tactive\nthat\tneuron\tat\tall).\n\nAnd\tthen\teach\tof\tthe\t10\toutput\tneurons\tuses\tonly\tthose\tfive\tactivations\tto\tdecide\twhether\nmy_three\tis\ttheir\tdigit\tor\tnot.",
    "312": "For\tFurther\tExploration\n\nCoursera\thas\ta\tfree\tcourse\ton\tNeural\tNetworks\tfor\tMachine\tLearning.\tAs\tI\twrite\tthis\tit\nwas\tlast\trun\tin\t2012,\tbut\tthe\tcourse\tmaterials\tare\tstill\tavailable.\n\nMichael\tNielsen\tis\twriting\ta\tfree\tonline\tbook\ton\tNeural\tNetworks\tand\tDeep\tLearning.\nBy\tthe\ttime\tyou\tread\tthis\tit\tmight\tbe\tfinished.\n\nPyBrain\tis\ta\tpretty\tsimple\tPython\tneural\tnetwork\tlibrary.\n\nPylearn2\tis\ta\tmuch\tmore\tadvanced\t(and\tmuch\tharder\tto\tuse)\tneural\tnetwork\tlibrary.",
    "313": "",
    "314": "Chapter\t19.\tClustering\n\nWhere\twe\tsuch\tclusters\thad\n\nAs\tmade\tus\tnobly\twild,\tnot\tmad\n\nRobert\tHerrick\n\nMost\tof\tthe\talgorithms\tin\tthis\tbook\tare\twhat\u2019s\tknown\tas\tsupervised\tlearning,\tin\tthat\tthey\nstart\twith\ta\tset\tof\tlabeled\tdata\tand\tuse\tthat\tas\tthe\tbasis\tfor\tmaking\tpredictions\tabout\tnew,\nunlabeled\tdata.\tClustering,\thowever,\tis\tan\texample\tof\tunsupervised\tlearning,\tin\twhich\twe\nwork\twith\tcompletely\tunlabeled\tdata\t(or\tin\twhich\tour\tdata\thas\tlabels\tbut\twe\tignore\tthem).",
    "315": "The\tIdea\n\nWhenever\tyou\tlook\tat\tsome\tsource\tof\tdata,\tit\u2019s\tlikely\tthat\tthe\tdata\twill\tsomehow\tform\nclusters.\tA\tdata\tset\tshowing\twhere\tmillionaires\tlive\tprobably\thas\tclusters\tin\tplaces\tlike\nBeverly\tHills\tand\tManhattan.\tA\tdata\tset\tshowing\thow\tmany\thours\tpeople\twork\teach\tweek\nprobably\thas\ta\tcluster\taround\t40\t(and\tif\tit\u2019s\ttaken\tfrom\ta\tstate\twith\tlaws\tmandating\nspecial\tbenefits\tfor\tpeople\twho\twork\tat\tleast\t20\thours\ta\tweek,\tit\tprobably\thas\tanother\ncluster\tright\taround\t19).\tA\tdata\tset\tof\tdemographics\tof\tregistered\tvoters\tlikely\tforms\ta\nvariety\tof\tclusters\t(e.g.,\t\u201csoccer\tmoms,\u201d\t\u201cbored\tretirees,\u201d\t\u201cunemployed\tmillennials\u201d)\tthat\npollsters\tand\tpolitical\tconsultants\tlikely\tconsider\trelevant.\n\nUnlike\tsome\tof\tthe\tproblems\twe\u2019ve\tlooked\tat,\tthere\tis\tgenerally\tno\t\u201ccorrect\u201d\tclustering.\nAn\talternative\tclustering\tscheme\tmight\tgroup\tsome\tof\tthe\t\u201cunemployed\tmillenials\u201d\twith\n\u201cgrad\tstudents,\u201d\tothers\twith\t\u201cparents\u2019\tbasement\tdwellers.\u201d\tNeither\tscheme\tis\tnecessarily\nmore\tcorrect\t\u2014\tinstead,\teach\tis\tlikely\tmore\toptimal\twith\trespect\tto\tits\town\t\u201chow\tgood\tare\nthe\tclusters?\u201d\tmetric.\n\nFurthermore,\tthe\tclusters\twon\u2019t\tlabel\tthemselves.\tYou\u2019ll\thave\tto\tdo\tthat\tby\tlooking\tat\tthe\ndata\tunderlying\teach\tone.",
    "316": "The\tModel\n\nFor\tus,\teach\tinput\twill\tbe\ta\tvector\tin\td-dimensional\tspace\t(which,\tas\tusual,\twe\twill\nrepresent\tas\ta\tlist\tof\tnumbers).\tOur\tgoal\twill\tbe\tto\tidentify\tclusters\tof\tsimilar\tinputs\tand\n(sometimes)\tto\tfind\ta\trepresentative\tvalue\tfor\teach\tcluster.\n\nFor\texample,\teach\tinput\tcould\tbe\t(a\tnumeric\tvector\tthat\tsomehow\trepresents)\tthe\ttitle\tof\ta\nblog\tpost,\tin\twhich\tcase\tthe\tgoal\tmight\tbe\tto\tfind\tclusters\tof\tsimilar\tposts,\tperhaps\tin\norder\tto\tunderstand\twhat\tour\tusers\tare\tblogging\tabout.\tOr\timagine\tthat\twe\thave\ta\tpicture\ncontaining\tthousands\tof\t(red,\tgreen,\tblue)\tcolors\tand\tthat\twe\tneed\tto\tscreen-print\ta\n10-color\tversion\tof\tit.\tClustering\tcan\thelp\tus\tchoose\t10\tcolors\tthat\twill\tminimize\tthe\ttotal\n\u201ccolor\terror.\u201d\n\nOne\tof\tthe\tsimplest\tclustering\tmethods\tis\tk-means,\tin\twhich\tthe\tnumber\tof\tclusters\tk\tis\n\tin\ta\nchosen\tin\tadvance,\tafter\twhich\tthe\tgoal\tis\tto\tpartition\tthe\tinputs\tinto\tsets\t\nway\tthat\tminimizes\tthe\ttotal\tsum\tof\tsquared\tdistances\tfrom\teach\tpoint\tto\tthe\tmean\tof\tits\nassigned\tcluster.\n\nThere\tare\ta\tlot\tof\tways\tto\tassign\tn\tpoints\tto\tk\tclusters,\twhich\tmeans\tthat\tfinding\tan\noptimal\tclustering\tis\ta\tvery\thard\tproblem.\tWe\u2019ll\tsettle\tfor\tan\titerative\talgorithm\tthat\nusually\tfinds\ta\tgood\tclustering:\n\n1.\t Start\twith\ta\tset\tof\tk-means,\twhich\tare\tpoints\tin\td-dimensional\tspace.\n\n2.\t Assign\teach\tpoint\tto\tthe\tmean\tto\twhich\tit\tis\tclosest.\n\n3.\t If\tno\tpoint\u2019s\tassignment\thas\tchanged,\tstop\tand\tkeep\tthe\tclusters.\n\n4.\t If\tsome\tpoint\u2019s\tassignment\thas\tchanged,\trecompute\tthe\tmeans\tand\treturn\tto\tstep\t2.\n\nUsing\tthe\tvector_mean\tfunction\tfrom\tChapter\t4,\tit\u2019s\tpretty\tsimple\tto\tcreate\ta\tclass\tthat\ndoes\tthis:\n\nclass\tKMeans:\n\t\t\t\t\"\"\"performs\tk-means\tclustering\"\"\"\n\n\t\t\t\tdef\t__init__(self,\tk):\n\t\t\t\t\t\t\t\tself.k\t=\tk\t\t\t\t\t\t\t\t\t\t#\tnumber\tof\tclusters\n\t\t\t\t\t\t\t\tself.means\t=\tNone\t\t\t#\tmeans\tof\tclusters\n\n\t\t\t\tdef\tclassify(self,\tinput):\n\t\t\t\t\t\t\t\t\"\"\"return\tthe\tindex\tof\tthe\tcluster\tclosest\tto\tthe\tinput\"\"\"\n\t\t\t\t\t\t\t\treturn\tmin(range(self.k),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkey=lambda\ti:\tsquared_distance(input,\tself.means[i]))\n\n\t\t\t\tdef\ttrain(self,\tinputs):\n\t\t\t\t\t\t\t\t#\tchoose\tk\trandom\tpoints\tas\tthe\tinitial\tmeans\n\t\t\t\t\t\t\t\tself.means\t=\trandom.sample(inputs,\tself.k)\n\t\t\t\t\t\t\t\tassignments\t=\tNone\n\n\t\t\t\t\t\t\t\twhile\tTrue:\n\t\t\t\t\t\t\t\t\t\t\t\t#\tFind\tnew\tassignments\n\t\t\t\t\t\t\t\t\t\t\t\tnew_assignments\t=\tmap(self.classify,\tinputs)\n\n\t\t\t\t\t\t\t\t\t\t\t\t#\tIf\tno\tassignments\thave\tchanged,\twe're\tdone.\n\t\t\t\t\t\t\t\t\t\t\t\tif\tassignments\t==\tnew_assignments:",
    "317": "return\n\n\t\t\t\t\t\t\t\t\t\t\t\t#\tOtherwise\tkeep\tthe\tnew\tassignments,\n\t\t\t\t\t\t\t\t\t\t\t\tassignments\t=\tnew_assignments\n\n\t\t\t\t\t\t\t\t\t\t\t\t#\tAnd\tcompute\tnew\tmeans\tbased\ton\tthe\tnew\tassignments\n\t\t\t\t\t\t\t\t\t\t\t\tfor\ti\tin\trange(self.k):\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tfind\tall\tthe\tpoints\tassigned\tto\tcluster\ti\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ti_points\t=\t[p\tfor\tp,\ta\tin\tzip(inputs,\tassignments)\tif\ta\t==\ti]\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tmake\tsure\ti_points\tis\tnot\tempty\tso\tdon't\tdivide\tby\t0\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\ti_points:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.means[i]\t=\tvector_mean(i_points)\n\nLet\u2019s\ttake\ta\tlook\tat\thow\tthis\tworks.",
    "318": "Example:\tMeetups\n\nTo\tcelebrate\tDataSciencester\u2019s\tgrowth,\tyour\tVP\tof\tUser\tRewards\twants\tto\torganize\nseveral\tin-person\tmeetups\tfor\tyour\thometown\tusers,\tcomplete\twith\tbeer,\tpizza,\tand\nDataSciencester\tt-shirts.\tYou\tknow\tthe\tlocations\tof\tall\tyour\tlocal\tusers\t(Figure\t19-1),\tand\nshe\u2019d\tlike\tyou\tto\tchoose\tmeetup\tlocations\tthat\tmake\tit\tconvenient\tfor\teveryone\tto\tattend.\n\nDepending\ton\thow\tyou\tlook\tat\tit,\tyou\tprobably\tsee\ttwo\tor\tthree\tclusters.\t(It\u2019s\teasy\tto\tdo\nvisually\tbecause\tthe\tdata\tis\tonly\ttwo-dimensional.\tWith\tmore\tdimensions,\tit\twould\tbe\ta\nlot\tharder\tto\teyeball.)\n\nImagine\tfirst\tthat\tshe\thas\tenough\tbudget\tfor\tthree\tmeetups.\tYou\tgo\tto\tyour\tcomputer\tand\ntry\tthis:\n\nrandom.seed(0)\t\t\t\t\t\t\t\t\t\t#\tso\tyou\tget\tthe\tsame\tresults\tas\tme\nclusterer\t=\tKMeans(3)\nclusterer.train(inputs)\nprint\tclusterer.means\n\nFigure\t19-1.\tThe\tlocations\tof\tyour\thometown\tusers\n\nYou\tfind\tthree\tclusters\tcentered\tat\t[-44,5],\t[-16,-10],\tand\t[18,\t20],\tand\tyou\tlook\tfor\nmeetup\tvenues\tnear\tthose\tlocations\t(Figure\t19-2).\n\nYou\tshow\tit\tto\tthe\tVP,\twho\tinforms\tyou\tthat\tnow\tshe\tonly\thas\tenough\tbudget\tfor\ttwo",
    "319": "meetups.\n\n\u201cNo\tproblem,\u201d\tyou\tsay:\n\nrandom.seed(0)\nclusterer\t=\tKMeans(2)\nclusterer.train(inputs)\nprint\tclusterer.means\n\nFigure\t19-2.\tUser\tlocations\tgrouped\tinto\tthree\tclusters\n\nAs\tshown\tin\tFigure\t19-3,\tone\tmeetup\tshould\tstill\tbe\tnear\t[18,\t20],\tbut\tnow\tthe\tother\nshould\tbe\tnear\t[-26,\t-5].",
    "320": "Figure\t19-3.\tUser\tlocations\tgrouped\tinto\ttwo\tclusters",
    "321": "Choosing\tk\n\nIn\tthe\tprevious\texample,\tthe\tchoice\tof\tk\twas\tdriven\tby\tfactors\toutside\tof\tour\tcontrol.\tIn\ngeneral,\tthis\twon\u2019t\tbe\tthe\tcase.\tThere\tis\ta\twide\tvariety\tof\tways\tto\tchoose\ta\tk.\tOne\tthat\u2019s\nreasonably\teasy\tto\tunderstand\tinvolves\tplotting\tthe\tsum\tof\tsquared\terrors\t(between\teach\npoint\tand\tthe\tmean\tof\tits\tcluster)\tas\ta\tfunction\tof\tk\tand\tlooking\tat\twhere\tthe\tgraph\n\u201cbends\u201d:\n\ndef\tsquared_clustering_errors(inputs,\tk):\n\t\t\t\t\"\"\"finds\tthe\ttotal\tsquared\terror\tfrom\tk-means\tclustering\tthe\tinputs\"\"\"\n\t\t\t\tclusterer\t=\tKMeans(k)\n\t\t\t\tclusterer.train(inputs)\n\t\t\t\tmeans\t=\tclusterer.means\n\t\t\t\tassignments\t=\tmap(clusterer.classify,\tinputs)\n\n\t\t\t\treturn\tsum(squared_distance(input,\tmeans[cluster])\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tinput,\tcluster\tin\tzip(inputs,\tassignments))\n\n#\tnow\tplot\tfrom\t1\tup\tto\tlen(inputs)\tclusters\n\nks\t=\trange(1,\tlen(inputs)\t+\t1)\nerrors\t=\t[squared_clustering_errors(inputs,\tk)\tfor\tk\tin\tks]\n\nplt.plot(ks,\terrors)\nplt.xticks(ks)\nplt.xlabel(\"k\")\nplt.ylabel(\"total\tsquared\terror\")\nplt.title(\"Total\tError\tvs.\t#\tof\tClusters\")\nplt.show()",
    "322": "Figure\t19-4.\tChoosing\ta\tk\n\nLooking\tat\tFigure\t19-4,\tthis\tmethod\tagrees\twith\tour\toriginal\teyeballing\tthat\t3\tis\tthe\n\u201cright\u201d\tnumber\tof\tclusters.",
    "323": "Example:\tClustering\tColors\n\nThe\tVP\tof\tSwag\thas\tdesigned\tattractive\tDataSciencester\tstickers\tthat\the\u2019d\tlike\tyou\tto\nhand\tout\tat\tmeetups.\tUnfortunately,\tyour\tsticker\tprinter\tcan\tprint\tat\tmost\tfive\tcolors\tper\nsticker.\tAnd\tsince\tthe\tVP\tof\tArt\tis\ton\tsabbatical,\tthe\tVP\tof\tSwag\tasks\tif\tthere\u2019s\tsome\tway\nyou\tcan\ttake\this\tdesign\tand\tmodify\tit\tso\tthat\tit\tonly\tcontains\tfive\tcolors.\n\nComputer\timages\tcan\tbe\trepresented\tas\ttwo-dimensional\tarray\tof\tpixels,\twhere\teach\tpixel\nis\titself\ta\tthree-dimensional\tvector\t(red,\tgreen,\tblue)\tindicating\tits\tcolor.\n\nCreating\ta\tfive-color\tversion\tof\tthe\timage\tthen\tentails:\n\n1.\t Choosing\tfive\tcolors\n\n2.\t Assigning\tone\tof\tthose\tcolors\tto\teach\tpixel\n\nIt\tturns\tout\tthis\tis\ta\tgreat\ttask\tfor\tk-means\tclustering,\twhich\tcan\tpartition\tthe\tpixels\tinto\nfive\tclusters\tin\tred-green-blue\tspace.\tIf\twe\tthen\trecolor\tthe\tpixels\tin\teach\tcluster\tto\tthe\nmean\tcolor,\twe\u2019re\tdone.\n\nTo\tstart\twith,\twe\u2019ll\tneed\ta\tway\tto\tload\tan\timage\tinto\tPython.\tIt\tturns\tout\twe\tcan\tdo\tthis\nwith\tmatplotlib:\n\npath_to_png_file\t=\tr\"C:\\images\\image.png\"\t\t\t#\twherever\tyour\timage\tis\nimport\tmatplotlib.image\tas\tmpimg\nimg\t=\tmpimg.imread(path_to_png_file)\n\nBehind\tthe\tscenes\timg\tis\ta\tNumPy\tarray,\tbut\tfor\tour\tpurposes,\twe\tcan\ttreat\tit\tas\ta\tlist\tof\nlists\tof\tlists.\n\nimg[i][j]\tis\tthe\tpixel\tin\tthe\tith\trow\tand\tjth\tcolumn,\tand\teach\tpixel\tis\ta\tlist\t[red,\tgreen,\nblue]\tof\tnumbers\tbetween\t0\tand\t1\tindicating\tthe\tcolor\tof\tthat\tpixel:\n\ntop_row\t=\timg[0]\ntop_left_pixel\t=\ttop_row[0]\nred,\tgreen,\tblue\t=\ttop_left_pixel\n\nIn\tparticular,\twe\tcan\tget\ta\tflattened\tlist\tof\tall\tthe\tpixels\tas:\n\npixels\t=\t[pixel\tfor\trow\tin\timg\tfor\tpixel\tin\trow]\n\nand\tthen\tfeed\tthem\tto\tour\tclusterer:\n\nclusterer\t=\tKMeans(5)\nclusterer.train(pixels)\t\t\t#\tthis\tmight\ttake\ta\twhile\n\nOnce\tit\tfinishes,\twe\tjust\tconstruct\ta\tnew\timage\twith\tthe\tsame\tformat:\n\ndef\trecolor(pixel):\n\t\t\t\tcluster\t=\tclusterer.classify(pixel)\t\t\t\t\t\t\t\t#\tindex\tof\tthe\tclosest\tcluster\n\t\t\t\treturn\tclusterer.means[cluster]\t\t\t\t\t\t\t\t\t\t\t\t#\tmean\tof\tthe\tclosest\tcluster",
    "324": "new_img\t=\t[[recolor(pixel)\tfor\tpixel\tin\trow]\t\t\t#\trecolor\tthis\trow\tof\tpixels\n\t\t\t\t\t\t\t\t\t\t\tfor\trow\tin\timg]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tfor\teach\trow\tin\tthe\timage\n\nand\tdisplay\tit,\tusing\tplt.imshow():\n\nplt.imshow(new_img)\nplt.axis('off')\nplt.show()\n\nIt\tis\tdifficult\tto\tshow\tcolor\tresults\tin\ta\tblack-and-white\tbook,\tbut\tFigure\t19-5\tshows\ngrayscale\tversions\tof\ta\tfull-color\tpicture\tand\tthe\toutput\tof\tusing\tthis\tprocess\tto\treduce\tit\nto\tfive\tcolors:\n\nFigure\t19-5.\tOriginal\tpicture\tand\tits\t5-means\tdecoloring",
    "325": "Bottom-up\tHierarchical\tClustering\n\nAn\talternative\tapproach\tto\tclustering\tis\tto\t\u201cgrow\u201d\tclusters\tfrom\tthe\tbottom\tup.\tWe\tcan\tdo\nthis\tin\tthe\tfollowing\tway:\n\n1.\t Make\teach\tinput\tits\town\tcluster\tof\tone.\n\n2.\t As\tlong\tas\tthere\tare\tmultiple\tclusters\tremaining,\tfind\tthe\ttwo\tclosest\tclusters\tand\n\nmerge\tthem.\n\nAt\tthe\tend,\twe\u2019ll\thave\tone\tgiant\tcluster\tcontaining\tall\tthe\tinputs.\tIf\twe\tkeep\ttrack\tof\tthe\nmerge\torder,\twe\tcan\trecreate\tany\tnumber\tof\tclusters\tby\tunmerging.\tFor\texample,\tif\twe\nwant\tthree\tclusters,\twe\tcan\tjust\tundo\tthe\tlast\ttwo\tmerges.\n\nWe\u2019ll\tuse\ta\treally\tsimple\trepresentation\tof\tclusters.\tOur\tvalues\twill\tlive\tin\tleaf\tclusters,\nwhich\twe\twill\trepresent\tas\t1-tuples:\n\nleaf1\t=\t([10,\t20],)\t\t\t#\tto\tmake\ta\t1-tuple\tyou\tneed\tthe\ttrailing\tcomma\nleaf2\t=\t([30,\t-15],)\t\t#\totherwise\tPython\ttreats\tthe\tparentheses\tas\tparentheses\n\nWe\u2019ll\tuse\tthese\tto\tgrow\tmerged\tclusters,\twhich\twe\twill\trepresent\tas\t2-tuples\t(merge\torder,\nchildren):\n\nmerged\t=\t(1,\t[leaf1,\tleaf2])\n\nWe\u2019ll\ttalk\tabout\tmerge\torder\tin\ta\tbit,\tbut\tfirst\tlet\u2019s\tcreate\ta\tfew\thelper\tfunctions:\n\ndef\tis_leaf(cluster):\n\t\t\t\t\"\"\"a\tcluster\tis\ta\tleaf\tif\tit\thas\tlength\t1\"\"\"\n\t\t\t\treturn\tlen(cluster)\t==\t1\n\ndef\tget_children(cluster):\n\t\t\t\t\"\"\"returns\tthe\ttwo\tchildren\tof\tthis\tcluster\tif\tit's\ta\tmerged\tcluster;\n\t\t\t\traises\tan\texception\tif\tthis\tis\ta\tleaf\tcluster\"\"\"\n\t\t\t\tif\tis_leaf(cluster):\n\t\t\t\t\t\t\t\traise\tTypeError(\"a\tleaf\tcluster\thas\tno\tchildren\")\n\t\t\t\telse:\n\t\t\t\t\t\t\t\treturn\tcluster[1]\n\ndef\tget_values(cluster):\n\t\t\t\t\"\"\"returns\tthe\tvalue\tin\tthis\tcluster\t(if\tit's\ta\tleaf\tcluster)\n\t\t\t\tor\tall\tthe\tvalues\tin\tthe\tleaf\tclusters\tbelow\tit\t(if\tit's\tnot)\"\"\"\n\t\t\t\tif\tis_leaf(cluster):\n\t\t\t\t\t\t\t\treturn\tcluster\t\t\t\t\t\t#\tis\talready\ta\t1-tuple\tcontaining\tvalue\n\t\t\t\telse:\n\t\t\t\t\t\t\t\treturn\t[value\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tchild\tin\tget_children(cluster)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tvalue\tin\tget_values(child)]\n\nIn\torder\tto\tmerge\tthe\tclosest\tclusters,\twe\tneed\tsome\tnotion\tof\tthe\tdistance\tbetween\nclusters.\tWe\u2019ll\tuse\tthe\tminimum\tdistance\tbetween\telements\tof\tthe\ttwo\tclusters,\twhich\nmerges\tthe\ttwo\tclusters\tthat\tare\tclosest\tto\ttouching\t(but\twill\tsometimes\tproduce\tlarge\nchain-like\tclusters\tthat\taren\u2019t\tvery\ttight).\tIf\twe\twanted\ttight\tspherical\tclusters,\twe\tmight\nuse\tthe\tmaximum\tdistance\tinstead,\tas\tit\tmerges\tthe\ttwo\tclusters\tthat\tfit\tin\tthe\tsmallest\tball.\nBoth\tare\tcommon\tchoices,\tas\tis\tthe\taverage\tdistance:",
    "326": "def\tcluster_distance(cluster1,\tcluster2,\tdistance_agg=min):\n\t\t\t\t\"\"\"compute\tall\tthe\tpairwise\tdistances\tbetween\tcluster1\tand\tcluster2\n\t\t\t\tand\tapply\t_distance_agg_\tto\tthe\tresulting\tlist\"\"\"\n\t\t\t\treturn\tdistance_agg([distance(input1,\tinput2)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tinput1\tin\tget_values(cluster1)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tinput2\tin\tget_values(cluster2)])\n\nWe\u2019ll\tuse\tthe\tmerge\torder\tslot\tto\ttrack\tthe\torder\tin\twhich\twe\tdid\tthe\tmerging.\tSmaller\nnumbers\twill\trepresent\tlater\tmerges.\tThis\tmeans\twhen\twe\twant\tto\tunmerge\tclusters,\twe\ndo\tso\tfrom\tlowest\tmerge\torder\tto\thighest.\tSince\tleaf\tclusters\twere\tnever\tmerged\t(which\nmeans\twe\tnever\twant\tto\tunmerge\tthem),\twe\u2019ll\tassign\tthem\tinfinity:\n\ndef\tget_merge_order(cluster):\n\t\t\t\tif\tis_leaf(cluster):\n\t\t\t\t\t\t\t\treturn\tfloat('inf')\n\t\t\t\telse:\n\t\t\t\t\t\t\t\treturn\tcluster[0]\t\t#\tmerge_order\tis\tfirst\telement\tof\t2-tuple\n\nNow\twe\u2019re\tready\tto\tcreate\tthe\tclustering\talgorithm:\n\ndef\tbottom_up_cluster(inputs,\tdistance_agg=min):\n\t\t\t\t#\tstart\twith\tevery\tinput\ta\tleaf\tcluster\t/\t1-tuple\n\t\t\t\tclusters\t=\t[(input,)\tfor\tinput\tin\tinputs]\n\n\t\t\t\t#\tas\tlong\tas\twe\thave\tmore\tthan\tone\tcluster\tleft\u2026\n\t\t\t\twhile\tlen(clusters)\t>\t1:\n\t\t\t\t\t\t\t\t#\tfind\tthe\ttwo\tclosest\tclusters\n\t\t\t\t\t\t\t\tc1,\tc2\t=\tmin([(cluster1,\tcluster2)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\ti,\tcluster1\tin\tenumerate(clusters)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tcluster2\tin\tclusters[:i]],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkey=lambda\t(x,\ty):\tcluster_distance(x,\ty,\tdistance_agg))\n\n\t\t\t\t\t\t\t\t#\tremove\tthem\tfrom\tthe\tlist\tof\tclusters\n\t\t\t\t\t\t\t\tclusters\t=\t[c\tfor\tc\tin\tclusters\tif\tc\t!=\tc1\tand\tc\t!=\tc2]\n\n\t\t\t\t\t\t\t\t#\tmerge\tthem,\tusing\tmerge_order\t=\t#\tof\tclusters\tleft\n\t\t\t\t\t\t\t\tmerged_cluster\t=\t(len(clusters),\t[c1,\tc2])\n\n\t\t\t\t\t\t\t\t#\tand\tadd\ttheir\tmerge\n\t\t\t\t\t\t\t\tclusters.append(merged_cluster)\n\n\t\t\t\t#\twhen\tthere's\tonly\tone\tcluster\tleft,\treturn\tit\n\t\t\t\treturn\tclusters[0]\n\nIts\tuse\tis\tvery\tsimple:\n\nbase_cluster\t=\tbottom_up_cluster(inputs)\n\nThis\tproduces\ta\tcluster\twhose\tugly\trepresentation\tis:\n\n(0,\t[(1,\t[(3,\t[(14,\t[(18,\t[([19,\t28],),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([21,\t27],)]),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([20,\t23],)]),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([26,\t13],)]),\n\t\t\t\t\t\t\t\t\t\t(16,\t[([11,\t15],),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([13,\t13],)])]),\n\t\t\t\t\t(2,\t[(4,\t[(5,\t[(9,\t[(11,\t[([-49,\t0],),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([-46,\t5],)]),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([-41,\t8],)]),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([-49,\t15],)]),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([-34,\t-1],)]),\n\t\t\t\t\t\t\t\t\t\t(6,\t[(7,\t[(8,\t[(10,\t[([-22,\t-16],),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([-19,\t-11],)]),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([-25,\t-9],)]),",
    "327": "(13,\t[(15,\t[(17,\t[([-11,\t-6],),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([-12,\t-8],)]),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([-14,\t-5],)]),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([-18,\t-3],)])]),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(12,\t[([-13,\t-19],),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t([-9,\t-16],)])])])])\n\nFor\tevery\tmerged\tcluster,\tI\tlined\tup\tits\tchildren\tvertically.\tIf\twe\tsay\t\u201ccluster\t0\u201d\tfor\tthe\ncluster\twith\tmerge\torder\t0,\tyou\tcan\tinterpret\tthis\tas:\n\nCluster\t0\tis\tthe\tmerger\tof\tcluster\t1\tand\tcluster\t2.\n\nCluster\t1\tis\tthe\tmerger\tof\tcluster\t3\tand\tcluster\t16.\n\nCluster\t16\tis\tthe\tmerger\tof\tthe\tleaf\t[11,\t15]\tand\tthe\tleaf\t[13,\t13].\n\nAnd\tso\ton\u2026\n\nSince\twe\thad\t20\tinputs,\tit\ttook\t19\tmerges\tto\tget\tto\tthis\tone\tcluster.\tThe\tfirst\tmerge\tcreated\ncluster\t18\tby\tcombining\tthe\tleaves\t[19,\t28]\tand\t[21,\t27].\tAnd\tthe\tlast\tmerge\tcreated\ncluster\t0.\n\nGenerally,\tthough,\twe\tdon\u2019t\twant\tto\tbe\tsquinting\tat\tnasty\ttext\trepresentations\tlike\tthis.\n(Although\tit\tcould\tbe\tan\tinteresting\texercise\tto\tcreate\ta\tuser-friendlier\tvisualization\tof\tthe\ncluster\thierarchy.)\tInstead\tlet\u2019s\twrite\ta\tfunction\tthat\tgenerates\tany\tnumber\tof\tclusters\tby\nperforming\tthe\tappropriate\tnumber\tof\tunmerges:\n\ndef\tgenerate_clusters(base_cluster,\tnum_clusters):\n\t\t\t\t#\tstart\twith\ta\tlist\twith\tjust\tthe\tbase\tcluster\n\t\t\t\tclusters\t=\t[base_cluster]\n\n\t\t\t\t#\tas\tlong\tas\twe\tdon't\thave\tenough\tclusters\tyet\u2026\n\t\t\t\twhile\tlen(clusters)\t<\tnum_clusters:\n\t\t\t\t\t\t\t\t#\tchoose\tthe\tlast-merged\tof\tour\tclusters\n\t\t\t\t\t\t\t\tnext_cluster\t=\tmin(clusters,\tkey=get_merge_order)\n\t\t\t\t\t\t\t\t#\tremove\tit\tfrom\tthe\tlist\n\t\t\t\t\t\t\t\tclusters\t=\t[c\tfor\tc\tin\tclusters\tif\tc\t!=\tnext_cluster]\n\t\t\t\t\t\t\t\t#\tand\tadd\tits\tchildren\tto\tthe\tlist\t(i.e.,\tunmerge\tit)\n\t\t\t\t\t\t\t\tclusters.extend(get_children(next_cluster))\n\n\t\t\t\t#\tonce\twe\thave\tenough\tclusters\u2026\n\t\t\t\treturn\tclusters\n\nSo,\tfor\texample,\tif\twe\twant\tto\tgenerate\tthree\tclusters,\twe\tcan\tjust\tdo:\n\nthree_clusters\t=\t[get_values(cluster)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tcluster\tin\tgenerate_clusters(base_cluster,\t3)]\n\nwhich\twe\tcan\teasily\tplot:\n\nfor\ti,\tcluster,\tmarker,\tcolor\tin\tzip([1,\t2,\t3],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tthree_clusters,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t['D','o','*'],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t['r','g','b']):\n\t\t\t\txs,\tys\t=\tzip(*cluster)\t\t#\tmagic\tunzipping\ttrick\n\t\t\t\tplt.scatter(xs,\tys,\tcolor=color,\tmarker=marker)\n\n\t\t\t\t#\tput\ta\tnumber\tat\tthe\tmean\tof\tthe\tcluster\n\t\t\t\tx,\ty\t=\tvector_mean(cluster)",
    "328": "plt.plot(x,\ty,\tmarker='$'\t+\tstr(i)\t+\t'$',\tcolor='black')\n\nplt.title(\"User\tLocations\u20143\tBottom-Up\tClusters,\tMin\")\nplt.xlabel(\"blocks\teast\tof\tcity\tcenter\")\nplt.ylabel(\"blocks\tnorth\tof\tcity\tcenter\")\nplt.show()\n\nThis\tgives\tvery\tdifferent\tresults\tthan\tk-means\tdid,\tas\tshown\tin\tFigure\t19-6.\n\nFigure\t19-6.\tThree\tbottom-up\tclusters\tusing\tmin\tdistance\n\nAs\twe\tmentioned\tabove,\tthis\tis\tbecause\tusing\tmin\tin\tcluster_distance\ttends\tto\tgive\nchain-like\tclusters.\tIf\twe\tinstead\tuse\tmax\t(which\tgives\ttight\tclusters)\tit\tlooks\tthe\tsame\tas\nthe\t3-means\tresult\t(Figure\t19-7).\n\nNOTE\n\nThe\tbottom_up_clustering\timplementation\tabove\tis\trelatively\tsimple,\tbut\tit\u2019s\talso\tshockingly\tinefficient.\nIn\tparticular,\tit\trecomputes\tthe\tdistance\tbetween\teach\tpair\tof\tinputs\tat\tevery\tstep.\tA\tmore\tefficient\nimplementation\tmight\tprecompute\tthe\tdistances\tbetween\teach\tpair\tof\tinputs\tand\tthen\tperform\ta\tlookup\ninside\tcluster_distance.\tA\treally\tefficient\timplementation\twould\tlikely\talso\tremember\tthe\ncluster_distances\tfrom\tthe\tprevious\tstep.",
    "329": "Figure\t19-7.\tThree\tbottom-up\tclusters\tusing\tmax\tdistance",
    "330": "For\tFurther\tExploration\n\nscikit-learn\thas\tan\tentire\tmodule\tsklearn.cluster\tthat\tcontains\tseveral\tclustering\nalgorithms\tincluding\tKMeans\tand\tthe\tWard\thierarchical\tclustering\talgorithm\t(which\tuses\na\tdifferent\tcriterion\tfor\tmerging\tclusters\tthan\tours\tdid).\n\nSciPy\thas\ttwo\tclustering\tmodels\tscipy.cluster.vq\t(which\tdoes\tk-means)\tand\nscipy.cluster.hierarchy\t(which\thas\ta\tvariety\tof\thierarchical\tclustering\talgorithms).",
    "331": "",
    "332": "Chapter\t20.\tNatural\tLanguage\tProcessing\n\nThey\thave\tbeen\tat\ta\tgreat\tfeast\tof\tlanguages,\tand\tstolen\tthe\tscraps.\n\nWilliam\tShakespeare\n\nNatural\tlanguage\tprocessing\t(NLP)\trefers\tto\tcomputational\ttechniques\tinvolving\nlanguage.\tIt\u2019s\ta\tbroad\tfield,\tbut\twe\u2019ll\tlook\tat\ta\tfew\ttechniques\tboth\tsimple\tand\tnot\tsimple.",
    "333": "Word\tClouds\n\nIn\tChapter\t1,\twe\tcomputed\tword\tcounts\tof\tusers\u2019\tinterests.\tOne\tapproach\tto\tvisualizing\nwords\tand\tcounts\tis\tword\tclouds,\twhich\tartistically\tlay\tout\tthe\twords\twith\tsizes\nproportional\tto\ttheir\tcounts.\n\nGenerally,\tthough,\tdata\tscientists\tdon\u2019t\tthink\tmuch\tof\tword\tclouds,\tin\tlarge\tpart\tbecause\nthe\tplacement\tof\tthe\twords\tdoesn\u2019t\tmean\tanything\tother\tthan\t\u201chere\u2019s\tsome\tspace\twhere\tI\nwas\table\tto\tfit\ta\tword.\u201d\n\nIf\tyou\tever\tare\tforced\tto\tcreate\ta\tword\tcloud,\tthink\tabout\twhether\tyou\tcan\tmake\tthe\taxes\nconvey\tsomething.\tFor\texample,\timagine\tthat,\tfor\teach\tof\tsome\tcollection\tof\tdata\nscience\u2013related\tbuzzwords,\tyou\thave\ttwo\tnumbers\tbetween\t0\tand\t100\t\u2014\tthe\tfirst\nrepresenting\thow\tfrequently\tit\tappears\tin\tjob\tpostings,\tthe\tsecond\thow\tfrequently\tit\nappears\ton\tresumes:\n\ndata\t=\t[\t(\"big\tdata\",\t100,\t15),\t(\"Hadoop\",\t95,\t25),\t(\"Python\",\t75,\t50),\n\t\t\t\t\t\t\t\t\t(\"R\",\t50,\t40),\t(\"machine\tlearning\",\t80,\t20),\t(\"statistics\",\t20,\t60),\n\t\t\t\t\t\t\t\t\t(\"data\tscience\",\t60,\t70),\t(\"analytics\",\t90,\t3),\n\t\t\t\t\t\t\t\t\t(\"team\tplayer\",\t85,\t85),\t(\"dynamic\",\t2,\t90),\t(\"synergies\",\t70,\t0),\n\t\t\t\t\t\t\t\t\t(\"actionable\tinsights\",\t40,\t30),\t(\"think\tout\tof\tthe\tbox\",\t45,\t10),\n\t\t\t\t\t\t\t\t\t(\"self-starter\",\t30,\t50),\t(\"customer\tfocus\",\t65,\t15),\n\t\t\t\t\t\t\t\t\t(\"thought\tleadership\",\t35,\t35)]\n\nThe\tword\tcloud\tapproach\tis\tjust\tto\tarrange\tthe\twords\ton\ta\tpage\tin\ta\tcool-looking\tfont\n(Figure\t20-1).",
    "334": "Figure\t20-1.\tBuzzword\tcloud\n\nThis\tlooks\tneat\tbut\tdoesn\u2019t\treally\ttell\tus\tanything.\tA\tmore\tinteresting\tapproach\tmight\tbe\nto\tscatter\tthem\tso\tthat\thorizontal\tposition\tindicates\tposting\tpopularity\tand\tvertical\tposition\nindicates\tresume\tpopularity,\twhich\tproduces\ta\tvisualization\tthat\tconveys\ta\tfew\tinsights\n(Figure\t20-2):\n\ndef\ttext_size(total):\n\t\t\t\t\"\"\"equals\t8\tif\ttotal\tis\t0,\t28\tif\ttotal\tis\t200\"\"\"\n\t\t\t\treturn\t8\t+\ttotal\t/\t200\t*\t20\n\nfor\tword,\tjob_popularity,\tresume_popularity\tin\tdata:\n\t\t\t\tplt.text(job_popularity,\tresume_popularity,\tword,\n\t\t\t\t\t\t\t\t\t\t\t\t\tha='center',\tva='center',\n\t\t\t\t\t\t\t\t\t\t\t\t\tsize=text_size(job_popularity\t+\tresume_popularity))\nplt.xlabel(\"Popularity\ton\tJob\tPostings\")\nplt.ylabel(\"Popularity\ton\tResumes\")\nplt.axis([0,\t100,\t0,\t100])\nplt.xticks([])\nplt.yticks([])\nplt.show()",
    "335": "Figure\t20-2.\tA\tmore\tmeaningful\t(if\tless\tattractive)\tword\tcloud",
    "336": "n-gram\tModels\n\nThe\tDataSciencester\tVP\tof\tSearch\tEngine\tMarketing\twants\tto\tcreate\tthousands\tof\tweb\npages\tabout\tdata\tscience\tso\tthat\tyour\tsite\twill\trank\thigher\tin\tsearch\tresults\tfor\tdata\nscience\u2013related\tterms.\t(You\tattempt\tto\texplain\tto\ther\tthat\tsearch\tengine\talgorithms\tare\nclever\tenough\tthat\tthis\twon\u2019t\tactually\twork,\tbut\tshe\trefuses\tto\tlisten.)\n\nOf\tcourse,\tshe\tdoesn\u2019t\twant\tto\twrite\tthousands\tof\tweb\tpages,\tnor\tdoes\tshe\twant\tto\tpay\ta\nhorde\tof\t\u201ccontent\tstrategists\u201d\tto\tdo\tso.\tInstead\tshe\tasks\tyou\twhether\tyou\tcan\tsomehow\nprogramatically\tgenerate\tthese\tweb\tpages.\tTo\tdo\tthis,\twe\u2019ll\tneed\tsome\tway\tof\tmodeling\nlanguage.\n\nOne\tapproach\tis\tto\tstart\twith\ta\tcorpus\tof\tdocuments\tand\tlearn\ta\tstatistical\tmodel\tof\nlanguage.\tIn\tour\tcase,\twe\u2019ll\tstart\twith\tMike\tLoukides\u2019s\tessay\t\u201cWhat\tis\tdata\tscience?\u201d\n\nAs\tin\tChapter\t9,\twe\u2019ll\tuse\trequests\tand\tBeautifulSoup\tto\tretrieve\tthe\tdata.\tThere\tare\ta\ncouple\tof\tissues\tworth\tcalling\tattention\tto.\n\nThe\tfirst\tis\tthat\tthe\tapostrophes\tin\tthe\ttext\tare\tactually\tthe\tUnicode\tcharacter\tu\"\\u2019\".\nWe\u2019ll\tcreate\ta\thelper\tfunction\tto\treplace\tthem\twith\tnormal\tapostrophes:\n\ndef\tfix_unicode(text):\n\t\t\t\treturn\ttext.replace(u\"\\u2019\",\t\"'\")\n\nThe\tsecond\tissue\tis\tthat\tonce\twe\tget\tthe\ttext\tof\tthe\tweb\tpage,\twe\u2019ll\twant\tto\tsplit\tit\tinto\ta\nsequence\tof\twords\tand\tperiods\t(so\tthat\twe\tcan\ttell\twhere\tsentences\tend).\tWe\tcan\tdo\tthis\nusing\tre.findall():\n\nfrom\tbs4\timport\tBeautifulSoup\nimport\trequests\nurl\t=\t\"http://radar.oreilly.com/2010/06/what-is-data-science.html\"\nhtml\t=\trequests.get(url).text\nsoup\t=\tBeautifulSoup(html,\t'html5lib')\n\ncontent\t=\tsoup.find(\"div\",\t\"entry-content\")\t\t\t#\tfind\tentry-content\tdiv\nregex\t=\tr\"[\\w']+|[\\.]\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tmatches\ta\tword\tor\ta\tperiod\n\ndocument\t=\t[]\n\nfor\tparagraph\tin\tcontent(\"p\"):\n\t\t\t\twords\t=\tre.findall(regex,\tfix_unicode(paragraph.text))\n\t\t\t\tdocument.extend(words)\n\nWe\tcertainly\tcould\t(and\tlikely\tshould)\tclean\tthis\tdata\tfurther.\tThere\tis\tstill\tsome\tamount\nof\textraneous\ttext\tin\tthe\tdocument\t(for\texample,\tthe\tfirst\tword\tis\t\u201cSection\u201d),\tand\twe\u2019ve\nsplit\ton\tmidsentence\tperiods\t(for\texample,\tin\t\u201cWeb\t2.0\u201d),\tand\tthere\tare\ta\thandful\tof\ncaptions\tand\tlists\tsprinkled\tthroughout.\tHaving\tsaid\tthat,\twe\u2019ll\twork\twith\tthe\tdocument\tas\nit\tis.\n\nNow\tthat\twe\thave\tthe\ttext\tas\ta\tsequence\tof\twords,\twe\tcan\tmodel\tlanguage\tin\tthe\nfollowing\tway:\tgiven\tsome\tstarting\tword\t(say\t\u201cbook\u201d)\twe\tlook\tat\tall\tthe\twords\tthat\tfollow\nit\tin\tthe\tsource\tdocuments\t(here\t\u201cisn\u2019t,\u201d\t\u201ca,\u201d\t\u201cshows,\u201d\t\u201cdemonstrates,\u201d\tand\t\u201cteaches\u201d).\tWe",
    "337": "randomly\tchoose\tone\tof\tthese\tto\tbe\tthe\tnext\tword,\tand\twe\trepeat\tthe\tprocess\tuntil\twe\tget\nto\ta\tperiod,\twhich\tsignifies\tthe\tend\tof\tthe\tsentence.\tWe\tcall\tthis\ta\tbigram\tmodel,\tas\tit\tis\ndetermined\tcompletely\tby\tthe\tfrequencies\tof\tthe\tbigrams\t(word\tpairs)\tin\tthe\toriginal\tdata.\n\nWhat\tabout\ta\tstarting\tword?\tWe\tcan\tjust\tpick\trandomly\tfrom\twords\tthat\tfollow\ta\tperiod.\nTo\tstart,\tlet\u2019s\tprecompute\tthe\tpossible\tword\ttransitions.\tRecall\tthat\tzip\tstops\twhen\tany\tof\nits\tinputs\tis\tdone,\tso\tthat\tzip(document,\tdocument[1:])\tgives\tus\tprecisely\tthe\tpairs\tof\nconsecutive\telements\tof\tdocument:\n\nbigrams\t=\tzip(document,\tdocument[1:])\ntransitions\t=\tdefaultdict(list)\nfor\tprev,\tcurrent\tin\tbigrams:\n\t\t\t\ttransitions[prev].append(current)\n\nNow\twe\u2019re\tready\tto\tgenerate\tsentences:\n\ndef\tgenerate_using_bigrams():\n\t\t\t\tcurrent\t=\t\".\"\t\t\t#\tthis\tmeans\tthe\tnext\tword\twill\tstart\ta\tsentence\n\t\t\t\tresult\t=\t[]\n\t\t\t\twhile\tTrue:\n\t\t\t\t\t\t\t\tnext_word_candidates\t=\ttransitions[current]\t\t\t\t#\tbigrams\t(current,\t_)\n\t\t\t\t\t\t\t\tcurrent\t=\trandom.choice(next_word_candidates)\t\t#\tchoose\tone\tat\trandom\n\t\t\t\t\t\t\t\tresult.append(current)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tappend\tit\tto\tresults\n\t\t\t\t\t\t\t\tif\tcurrent\t==\t\".\":\treturn\t\"\t\".join(result)\t\t\t\t\t#\tif\t\".\"\twe're\tdone\n\nThe\tsentences\tit\tproduces\tare\tgibberish,\tbut\tthey\u2019re\tthe\tkind\tof\tgibberish\tyou\tmight\tput\ton\nyour\twebsite\tif\tyou\twere\ttrying\tto\tsound\tdata-sciencey.\tFor\texample:\n\nIf\tyou\tmay\tknow\twhich\tare\tyou\twant\tto\tdata\tsort\tthe\tdata\tfeeds\tweb\tfriend\tsomeone\ton\ntrending\ttopics\tas\tthe\tdata\tin\tHadoop\tis\tthe\tdata\tscience\trequires\ta\tbook\tdemonstrates\nwhy\tvisualizations\tare\tbut\twe\tdo\tmassive\tcorrelations\tacross\tmany\tcommercial\tdisk\ndrives\tin\tPython\tlanguage\tand\tcreates\tmore\ttractable\tform\tmaking\tconnections\tthen\tuse\nand\tuses\tit\tto\tsolve\ta\tdata.\n\nBigram\tModel\n\nWe\tcan\tmake\tthe\tsentences\tless\tgibberishy\tby\tlooking\tat\ttrigrams,\ttriplets\tof\tconsecutive\nwords.\t(More\tgenerally,\tyou\tmight\tlook\tat\tn-grams\tconsisting\tof\tn\tconsecutive\twords,\tbut\nthree\twill\tbe\tplenty\tfor\tus.)\tNow\tthe\ttransitions\twill\tdepend\ton\tthe\tprevious\ttwo\twords:\n\ntrigrams\t=\tzip(document,\tdocument[1:],\tdocument[2:])\ntrigram_transitions\t=\tdefaultdict(list)\nstarts\t=\t[]\n\nfor\tprev,\tcurrent,\tnext\tin\ttrigrams:\n\n\t\t\t\tif\tprev\t==\t\".\":\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tif\tthe\tprevious\t\"word\"\twas\ta\tperiod\n\t\t\t\t\t\t\t\tstarts.append(current)\t\t\t#\tthen\tthis\tis\ta\tstart\tword\n\n\t\t\t\ttrigram_transitions[(prev,\tcurrent)].append(next)\n\nNotice\tthat\tnow\twe\thave\tto\ttrack\tthe\tstarting\twords\tseparately.\tWe\tcan\tgenerate\tsentences\nin\tpretty\tmuch\tthe\tsame\tway:\n\ndef\tgenerate_using_trigrams():\n\t\t\t\tcurrent\t=\trandom.choice(starts)\t\t\t#\tchoose\ta\trandom\tstarting\tword",
    "338": "prev\t=\t\".\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tand\tprecede\tit\twith\ta\t'.'\n\t\t\t\tresult\t=\t[current]\n\t\t\t\twhile\tTrue:\n\t\t\t\t\t\t\t\tnext_word_candidates\t=\ttrigram_transitions[(prev,\tcurrent)]\n\t\t\t\t\t\t\t\tnext_word\t=\trandom.choice(next_word_candidates)\n\n\t\t\t\t\t\t\t\tprev,\tcurrent\t=\tcurrent,\tnext_word\n\t\t\t\t\t\t\t\tresult.append(current)\n\n\t\t\t\t\t\t\t\tif\tcurrent\t==\t\".\":\n\t\t\t\t\t\t\t\t\t\t\t\treturn\t\"\t\".join(result)\n\nThis\tproduces\tbetter\tsentences\tlike:\n\nIn\thindsight\tMapReduce\tseems\tlike\tan\tepidemic\tand\tif\tso\tdoes\tthat\tgive\tus\tnew\tinsights\ninto\thow\teconomies\twork\tThat\u2019s\tnot\ta\tquestion\twe\tcould\teven\thave\tasked\ta\tfew\tyears\nthere\thas\tbeen\tinstrumented.\n\nTrigram\tModel\n\nOf\tcourse,\tthey\tsound\tbetter\tbecause\tat\teach\tstep\tthe\tgeneration\tprocess\thas\tfewer\nchoices,\tand\tat\tmany\tsteps\tonly\ta\tsingle\tchoice.\tThis\tmeans\tthat\tyou\tfrequently\tgenerate\nsentences\t(or\tat\tleast\tlong\tphrases)\tthat\twere\tseen\tverbatim\tin\tthe\toriginal\tdata.\tHaving\nmore\tdata\twould\thelp;\tit\twould\talso\twork\tbetter\tif\tyou\tcollected\tn-grams\tfrom\tmultiple\nessays\tabout\tdata\tscience.",
    "339": "Grammars\n\nA\tdifferent\tapproach\tto\tmodeling\tlanguage\tis\twith\tgrammars,\trules\tfor\tgenerating\nacceptable\tsentences.\tIn\telementary\tschool,\tyou\tprobably\tlearned\tabout\tparts\tof\tspeech\nand\thow\tto\tcombine\tthem.\tFor\texample,\tif\tyou\thad\ta\treally\tbad\tEnglish\tteacher,\tyou\nmight\tsay\tthat\ta\tsentence\tnecessarily\tconsists\tof\ta\tnoun\tfollowed\tby\ta\tverb.\tIf\tyou\tthen\nhave\ta\tlist\tof\tnouns\tand\tverbs,\tyou\tcan\tgenerate\tsentences\taccording\tto\tthe\trule.\n\nWe\u2019ll\tdefine\ta\tslightly\tmore\tcomplicated\tgrammar:\n\ngrammar\t=\t{\n\t\t\t\t\"_S\"\t\t:\t[\"_NP\t_VP\"],\n\t\t\t\t\"_NP\"\t:\t[\"_N\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"_A\t_NP\t_P\t_A\t_N\"],\n\t\t\t\t\"_VP\"\t:\t[\"_V\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"_V\t_NP\"],\n\t\t\t\t\"_N\"\t\t:\t[\"data\tscience\",\t\"Python\",\t\"regression\"],\n\t\t\t\t\"_A\"\t\t:\t[\"big\",\t\"linear\",\t\"logistic\"],\n\t\t\t\t\"_P\"\t\t:\t[\"about\",\t\"near\"],\n\t\t\t\t\"_V\"\t\t:\t[\"learns\",\t\"trains\",\t\"tests\",\t\"is\"]\n}\n\nI\tmade\tup\tthe\tconvention\tthat\tnames\tstarting\twith\tunderscores\trefer\tto\trules\tthat\tneed\nfurther\texpanding,\tand\tthat\tother\tnames\tare\tterminals\tthat\tdon\u2019t\tneed\tfurther\tprocessing.\n\nSo,\tfor\texample,\t\"_S\"\tis\tthe\t\u201csentence\u201d\trule,\twhich\tproduces\ta\t\"_NP\"\t(\u201cnoun\tphrase\u201d)\trule\nfollowed\tby\ta\t\"_VP\"\t(\u201cverb\tphrase\u201d)\trule.\n\nThe\tverb\tphrase\trule\tcan\tproduce\teither\tthe\t\"_V\"\t(\u201cverb\u201d)\trule,\tor\tthe\tverb\trule\tfollowed\nby\tthe\tnoun\tphrase\trule.\n\nNotice\tthat\tthe\t\"_NP\"\trule\tcontains\titself\tin\tone\tof\tits\tproductions.\tGrammars\tcan\tbe\nrecursive,\twhich\tallows\teven\tfinite\tgrammars\tlike\tthis\tto\tgenerate\tinfinitely\tmany\ndifferent\tsentences.\n\nHow\tdo\twe\tgenerate\tsentences\tfrom\tthis\tgrammar?\tWe\u2019ll\tstart\twith\ta\tlist\tcontaining\tthe\nsentence\trule\t[\"_S\"].\tAnd\tthen\twe\u2019ll\trepeatedly\texpand\teach\trule\tby\treplacing\tit\twith\ta\nrandomly\tchosen\tone\tof\tits\tproductions.\tWe\tstop\twhen\twe\thave\ta\tlist\tconsisting\tsolely\tof\nterminals.\n\nFor\texample,\tone\tsuch\tprogression\tmight\tlook\tlike:\n\n['_S']\n['_NP','_VP']\n['_N','_VP']\n['Python','_VP']\n['Python','_V','_NP']\n['Python','trains','_NP']\n['Python','trains','_A','_NP','_P','_A','_N']\n['Python','trains','logistic','_NP','_P','_A','_N']\n['Python','trains','logistic','_N','_P','_A','_N']\n['Python','trains','logistic','data\tscience','_P','_A','_N']\n['Python','trains','logistic','data\tscience','about','_A',\t'_N']\n['Python','trains','logistic','data\tscience','about','logistic','_N']\n['Python','trains','logistic','data\tscience','about','logistic','Python']",
    "340": "How\tdo\twe\timplement\tthis?\tWell,\tto\tstart,\twe\u2019ll\tcreate\ta\tsimple\thelper\tfunction\tto\tidentify\nterminals:\n\ndef\tis_terminal(token):\n\t\t\t\treturn\ttoken[0]\t!=\t\"_\"\n\nNext\twe\tneed\tto\twrite\ta\tfunction\tto\tturn\ta\tlist\tof\ttokens\tinto\ta\tsentence.\tWe\u2019ll\tlook\tfor\tthe\nfirst\tnonterminal\ttoken.\tIf\twe\tcan\u2019t\tfind\tone,\tthat\tmeans\twe\thave\ta\tcompleted\tsentence\nand\twe\u2019re\tdone.\n\nIf\twe\tdo\tfind\ta\tnonterminal,\tthen\twe\trandomly\tchoose\tone\tof\tits\tproductions.\tIf\tthat\nproduction\tis\ta\tterminal\t(i.e.,\ta\tword),\twe\tsimply\treplace\tthe\ttoken\twith\tit.\tOtherwise\tit\u2019s\ta\nsequence\tof\tspace-separated\tnonterminal\ttokens\tthat\twe\tneed\tto\tsplit\tand\tthen\tsplice\tinto\nthe\tcurrent\ttokens.\tEither\tway,\twe\trepeat\tthe\tprocess\ton\tthe\tnew\tset\tof\ttokens.\n\nPutting\tit\tall\ttogether\twe\tget:\n\ndef\texpand(grammar,\ttokens):\n\t\t\t\tfor\ti,\ttoken\tin\tenumerate(tokens):\n\n\t\t\t\t\t\t\t\t#\tskip\tover\tterminals\n\t\t\t\t\t\t\t\tif\tis_terminal(token):\tcontinue\n\n\t\t\t\t\t\t\t\t#\tif\twe\tget\there,\twe\tfound\ta\tnon-terminal\ttoken\n\t\t\t\t\t\t\t\t#\tso\twe\tneed\tto\tchoose\ta\treplacement\tat\trandom\n\t\t\t\t\t\t\t\treplacement\t=\trandom.choice(grammar[token])\n\n\t\t\t\t\t\t\t\tif\tis_terminal(replacement):\n\t\t\t\t\t\t\t\t\t\t\t\ttokens[i]\t=\treplacement\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t\t\ttokens\t=\ttokens[:i]\t+\treplacement.split()\t+\ttokens[(i+1):]\n\n\t\t\t\t\t\t\t\t#\tnow\tcall\texpand\ton\tthe\tnew\tlist\tof\ttokens\n\t\t\t\t\t\t\t\treturn\texpand(grammar,\ttokens)\n\n\t\t\t\t#\tif\twe\tget\there\twe\thad\tall\tterminals\tand\tare\tdone\n\t\t\t\treturn\ttokens\n\nAnd\tnow\twe\tcan\tstart\tgenerating\tsentences:\n\ndef\tgenerate_sentence(grammar):\n\t\t\t\treturn\texpand(grammar,\t[\"_S\"])\n\nTry\tchanging\tthe\tgrammar\t\u2014\tadd\tmore\twords,\tadd\tmore\trules,\tadd\tyour\town\tparts\tof\nspeech\t\u2014\tuntil\tyou\u2019re\tready\tto\tgenerate\tas\tmany\tweb\tpages\tas\tyour\tcompany\tneeds.\n\nGrammars\tare\tactually\tmore\tinteresting\twhen\tthey\u2019re\tused\tin\tthe\tother\tdirection.\tGiven\ta\nsentence\twe\tcan\tuse\ta\tgrammar\tto\tparse\tthe\tsentence.\tThis\tthen\tallows\tus\tto\tidentify\nsubjects\tand\tverbs\tand\thelps\tus\tmake\tsense\tof\tthe\tsentence.\n\nUsing\tdata\tscience\tto\tgenerate\ttext\tis\ta\tneat\ttrick;\tusing\tit\tto\tunderstand\ttext\tis\tmore\nmagical.\t(See\t\u201cFor\tFurther\tInvestigation\u201d\tfor\tlibraries\tthat\tyou\tcould\tuse\tfor\tthis.)",
    "341": "An\tAside:\tGibbs\tSampling\n\nGenerating\tsamples\tfrom\tsome\tdistributions\tis\teasy.\tWe\tcan\tget\tuniform\trandom\tvariables\nwith:\n\nrandom.random()\n\nand\tnormal\trandom\tvariables\twith:\n\ninverse_normal_cdf(random.random())\n\nBut\tsome\tdistributions\tare\tharder\tto\tsample\tfrom.\tGibbs\tsampling\tis\ta\ttechnique\tfor\ngenerating\tsamples\tfrom\tmultidimensional\tdistributions\twhen\twe\tonly\tknow\tsome\tof\tthe\nconditional\tdistributions.\n\nFor\texample,\timagine\trolling\ttwo\tdice.\tLet\tx\tbe\tthe\tvalue\tof\tthe\tfirst\tdie\tand\ty\tbe\tthe\tsum\nof\tthe\tdice,\tand\timagine\tyou\twanted\tto\tgenerate\tlots\tof\t(x,\ty)\tpairs.\tIn\tthis\tcase\tit\u2019s\teasy\tto\ngenerate\tthe\tsamples\tdirectly:\n\ndef\troll_a_die():\n\t\t\t\treturn\trandom.choice([1,2,3,4,5,6])\n\ndef\tdirect_sample():\n\t\t\t\td1\t=\troll_a_die()\n\t\t\t\td2\t=\troll_a_die()\n\t\t\t\treturn\td1,\td1\t+\td2\n\nBut\timagine\tthat\tyou\tonly\tknew\tthe\tconditional\tdistributions.\tThe\tdistribution\tof\ty\nconditional\ton\tx\tis\teasy\t\u2014\tif\tyou\tknow\tthe\tvalue\tof\tx,\ty\tis\tequally\tlikely\tto\tbe\tx\t+\t1,\tx\t+\t2,\nx\t+\t3,\tx\t+\t4,\tx\t+\t5,\tor\tx\t+\t6:\n\ndef\trandom_y_given_x(x):\n\t\t\t\t\"\"\"equally\tlikely\tto\tbe\tx\t+\t1,\tx\t+\t2,\t...\t,\tx\t+\t6\"\"\"\n\t\t\t\treturn\tx\t+\troll_a_die()\n\nThe\tother\tdirection\tis\tmore\tcomplicated.\tFor\texample,\tif\tyou\tknow\tthat\ty\tis\t2,\tthen\nnecessarily\tx\tis\t1\t(since\tthe\tonly\tway\ttwo\tdice\tcan\tsum\tto\t2\tis\tif\tboth\tof\tthem\tare\t1).\tIf\nyou\tknow\ty\tis\t3,\tthen\tx\tis\tequally\tlikely\tto\tbe\t1\tor\t2.\tSimilarly,\tif\ty\tis\t11,\tthen\tx\thas\tto\tbe\neither\t5\tor\t6:\n\ndef\trandom_x_given_y(y):\n\t\t\t\tif\ty\t<=\t7:\n\t\t\t\t\t\t\t\t#\tif\tthe\ttotal\tis\t7\tor\tless,\tthe\tfirst\tdie\tis\tequally\tlikely\tto\tbe\n\t\t\t\t\t\t\t\t#\t1,\t2,\t...,\t(total\t-\t1)\n\t\t\t\t\t\t\t\treturn\trandom.randrange(1,\ty)\n\t\t\t\telse:\n\t\t\t\t\t\t\t\t#\tif\tthe\ttotal\tis\t7\tor\tmore,\tthe\tfirst\tdie\tis\tequally\tlikely\tto\tbe\n\t\t\t\t\t\t\t\t#\t(total\t-\t6),\t(total\t-\t5),\t...,\t6\n\t\t\t\t\t\t\t\treturn\trandom.randrange(y\t-\t6,\t7)\n\nThe\tway\tGibbs\tsampling\tworks\tis\tthat\twe\tstart\twith\tany\t(valid)\tvalue\tfor\tx\tand\ty\tand\tthen\nrepeatedly\talternate\treplacing\tx\twith\ta\trandom\tvalue\tpicked\tconditional\ton\ty\tand\treplacing",
    "342": "y\twith\ta\trandom\tvalue\tpicked\tconditional\ton\tx.\tAfter\ta\tnumber\tof\titerations,\tthe\tresulting\nvalues\tof\tx\tand\ty\twill\trepresent\ta\tsample\tfrom\tthe\tunconditional\tjoint\tdistribution:\n\ndef\tgibbs_sample(num_iters=100):\n\t\t\t\tx,\ty\t=\t1,\t2\t#\tdoesn't\treally\tmatter\n\t\t\t\tfor\t_\tin\trange(num_iters):\n\t\t\t\t\t\t\t\tx\t=\trandom_x_given_y(y)\n\t\t\t\t\t\t\t\ty\t=\trandom_y_given_x(x)\n\t\t\t\treturn\tx,\ty\n\nYou\tcan\tcheck\tthat\tthis\tgives\tsimilar\tresults\tto\tthe\tdirect\tsample:\n\ndef\tcompare_distributions(num_samples=1000):\n\t\t\t\tcounts\t=\tdefaultdict(lambda:\t[0,\t0])\n\t\t\t\tfor\t_\tin\trange(num_samples):\n\t\t\t\t\t\t\t\tcounts[gibbs_sample()][0]\t+=\t1\n\t\t\t\t\t\t\t\tcounts[direct_sample()][1]\t+=\t1\n\t\t\t\treturn\tcounts\n\nWe\u2019ll\tuse\tthis\ttechnique\tin\tthe\tnext\tsection.",
    "343": "Topic\tModeling\n\nWhen\twe\tbuilt\tour\tData\tScientists\tYou\tShould\tKnow\trecommender\tin\tChapter\t1,\twe\nsimply\tlooked\tfor\texact\tmatches\tin\tpeople\u2019s\tstated\tinterests.\n\nA\tmore\tsophisticated\tapproach\tto\tunderstanding\tour\tusers\u2019\tinterests\tmight\ttry\tto\tidentify\nthe\ttopics\tthat\tunderlie\tthose\tinterests.\tA\ttechnique\tcalled\tLatent\tDirichlet\tAnalysis\t(LDA)\nis\tcommonly\tused\tto\tidentify\tcommon\ttopics\tin\ta\tset\tof\tdocuments.\tWe\u2019ll\tapply\tit\tto\ndocuments\tthat\tconsist\tof\teach\tuser\u2019s\tinterests.\n\nLDA\thas\tsome\tsimilarities\tto\tthe\tNaive\tBayes\tClassifier\twe\tbuilt\tin\tChapter\t13,\tin\tthat\tit\nassumes\ta\tprobabilistic\tmodel\tfor\tdocuments.\tWe\u2019ll\tgloss\tover\tthe\thairier\tmathematical\ndetails,\tbut\tfor\tour\tpurposes\tthe\tmodel\tassumes\tthat:\n\nThere\tis\tsome\tfixed\tnumber\tK\tof\ttopics.\n\nThere\tis\ta\trandom\tvariable\tthat\tassigns\teach\ttopic\tan\tassociated\tprobability\tdistribution\nover\twords.\tYou\tshould\tthink\tof\tthis\tdistribution\tas\tthe\tprobability\tof\tseeing\tword\tw\ngiven\ttopic\tk.\n\nThere\tis\tanother\trandom\tvariable\tthat\tassigns\teach\tdocument\ta\tprobability\tdistribution\nover\ttopics.\tYou\tshould\tthink\tof\tthis\tdistribution\tas\tthe\tmixture\tof\ttopics\tin\tdocument\nd.\n\nEach\tword\tin\ta\tdocument\twas\tgenerated\tby\tfirst\trandomly\tpicking\ta\ttopic\t(from\tthe\ndocument\u2019s\tdistribution\tof\ttopics)\tand\tthen\trandomly\tpicking\ta\tword\t(from\tthe\ttopic\u2019s\ndistribution\tof\twords).\n\nIn\tparticular,\twe\thave\ta\tcollection\tof\tdocuments\teach\tof\twhich\tis\ta\tlist\tof\twords.\tAnd\twe\nhave\ta\tcorresponding\tcollection\tof\tdocument_topics\tthat\tassigns\ta\ttopic\t(here\ta\tnumber\nbetween\t0\tand\tK\t\u2013\t1)\tto\teach\tword\tin\teach\tdocument.\n\nSo\tthat\tthe\tfifth\tword\tin\tthe\tfourth\tdocument\tis:\n\ndocuments[3][4]\n\nand\tthe\ttopic\tfrom\twhich\tthat\tword\twas\tchosen\tis:\n\ndocument_topics[3][4]\n\nThis\tvery\texplicitly\tdefines\teach\tdocument\u2019s\tdistribution\tover\ttopics,\tand\tit\timplicitly\ndefines\teach\ttopic\u2019s\tdistribution\tover\twords.\n\nWe\tcan\testimate\tthe\tlikelihood\tthat\ttopic\t1\tproduces\ta\tcertain\tword\tby\tcomparing\thow\nmany\ttimes\ttopic\t1\tproduces\tthat\tword\twith\thow\tmany\ttimes\ttopic\t1\tproduces\tany\tword.\n(Similarly,\twhen\twe\tbuilt\ta\tspam\tfilter\tin\tChapter\t13,\twe\tcompared\thow\tmany\ttimes\teach\nword\tappeared\tin\tspams\twith\tthe\ttotal\tnumber\tof\twords\tappearing\tin\tspams.)\n\nAlthough\tthese\ttopics\tare\tjust\tnumbers,\twe\tcan\tgive\tthem\tdescriptive\tnames\tby\tlooking\tat",
    "344": "the\twords\ton\twhich\tthey\tput\tthe\theaviest\tweight.\tWe\tjust\thave\tto\tsomehow\tgenerate\tthe\ndocument_topics.\tThis\tis\twhere\tGibbs\tsampling\tcomes\tinto\tplay.\n\nWe\tstart\tby\tassigning\tevery\tword\tin\tevery\tdocument\ta\ttopic\tcompletely\tat\trandom.\tNow\nwe\tgo\tthrough\teach\tdocument\tone\tword\tat\ta\ttime.\tFor\tthat\tword\tand\tdocument,\twe\nconstruct\tweights\tfor\teach\ttopic\tthat\tdepend\ton\tthe\t(current)\tdistribution\tof\ttopics\tin\tthat\ndocument\tand\tthe\t(current)\tdistribution\tof\twords\tfor\tthat\ttopic.\tWe\tthen\tuse\tthose\tweights\nto\tsample\ta\tnew\ttopic\tfor\tthat\tword.\tIf\twe\titerate\tthis\tprocess\tmany\ttimes,\twe\twill\tend\tup\nwith\ta\tjoint\tsample\tfrom\tthe\ttopic-word\tdistribution\tand\tthe\tdocument-topic\tdistribution.\n\nTo\tstart\twith,\twe\u2019ll\tneed\ta\tfunction\tto\trandomly\tchoose\tan\tindex\tbased\ton\tan\tarbitrary\tset\nof\tweights:\n\ndef\tsample_from(weights):\n\t\t\t\t\"\"\"returns\ti\twith\tprobability\tweights[i]\t/\tsum(weights)\"\"\"\n\t\t\t\ttotal\t=\tsum(weights)\n\t\t\t\trnd\t=\ttotal\t*\trandom.random()\t\t\t\t\t\t#\tuniform\tbetween\t0\tand\ttotal\n\t\t\t\tfor\ti,\tw\tin\tenumerate(weights):\n\t\t\t\t\t\t\t\trnd\t-=\tw\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\treturn\tthe\tsmallest\ti\tsuch\tthat\n\t\t\t\t\t\t\t\tif\trnd\t<=\t0:\treturn\ti\t\t\t\t\t\t\t\t\t\t#\tweights[0]\t+\t...\t+\tweights[i]\t>=\trnd\n\nFor\tinstance,\tif\tyou\tgive\tit\tweights\t[1,\t1,\t3]\tthen\tone-fifth\tof\tthe\ttime\tit\twill\treturn\t0,\none-fifth\tof\tthe\ttime\tit\twill\treturn\t1,\tand\tthree-fifths\tof\tthe\ttime\tit\twill\treturn\t2.\n\nOur\tdocuments\tare\tour\tusers\u2019\tinterests,\twhich\tlook\tlike:\n\ndocuments\t=\t[\n\t\t\t\t[\"Hadoop\",\t\"Big\tData\",\t\"HBase\",\t\"Java\",\t\"Spark\",\t\"Storm\",\t\"Cassandra\"],\n\t\t\t\t[\"NoSQL\",\t\"MongoDB\",\t\"Cassandra\",\t\"HBase\",\t\"Postgres\"],\n\t\t\t\t[\"Python\",\t\"scikit-learn\",\t\"scipy\",\t\"numpy\",\t\"statsmodels\",\t\"pandas\"],\n\t\t\t\t[\"R\",\t\"Python\",\t\"statistics\",\t\"regression\",\t\"probability\"],\n\t\t\t\t[\"machine\tlearning\",\t\"regression\",\t\"decision\ttrees\",\t\"libsvm\"],\n\t\t\t\t[\"Python\",\t\"R\",\t\"Java\",\t\"C++\",\t\"Haskell\",\t\"programming\tlanguages\"],\n\t\t\t\t[\"statistics\",\t\"probability\",\t\"mathematics\",\t\"theory\"],\n\t\t\t\t[\"machine\tlearning\",\t\"scikit-learn\",\t\"Mahout\",\t\"neural\tnetworks\"],\n\t\t\t\t[\"neural\tnetworks\",\t\"deep\tlearning\",\t\"Big\tData\",\t\"artificial\tintelligence\"],\n\t\t\t\t[\"Hadoop\",\t\"Java\",\t\"MapReduce\",\t\"Big\tData\"],\n\t\t\t\t[\"statistics\",\t\"R\",\t\"statsmodels\"],\n\t\t\t\t[\"C++\",\t\"deep\tlearning\",\t\"artificial\tintelligence\",\t\"probability\"],\n\t\t\t\t[\"pandas\",\t\"R\",\t\"Python\"],\n\t\t\t\t[\"databases\",\t\"HBase\",\t\"Postgres\",\t\"MySQL\",\t\"MongoDB\"],\n\t\t\t\t[\"libsvm\",\t\"regression\",\t\"support\tvector\tmachines\"]\n]\n\nAnd\twe\u2019ll\ttry\tto\tfind\tK\t=\t4\ttopics.\n\nIn\torder\tto\tcalculate\tthe\tsampling\tweights,\twe\u2019ll\tneed\tto\tkeep\ttrack\tof\tseveral\tcounts.\nLet\u2019s\tfirst\tcreate\tthe\tdata\tstructures\tfor\tthem.\n\nHow\tmany\ttimes\teach\ttopic\tis\tassigned\tto\teach\tdocument:\n\n#\ta\tlist\tof\tCounters,\tone\tfor\teach\tdocument\ndocument_topic_counts\t=\t[Counter()\tfor\t_\tin\tdocuments]\n\nHow\tmany\ttimes\teach\tword\tis\tassigned\tto\teach\ttopic:\n\n#\ta\tlist\tof\tCounters,\tone\tfor\teach\ttopic",
    "345": "topic_word_counts\t=\t[Counter()\tfor\t_\tin\trange(K)]\n\nThe\ttotal\tnumber\tof\twords\tassigned\tto\teach\ttopic:\n\n#\ta\tlist\tof\tnumbers,\tone\tfor\teach\ttopic\ntopic_counts\t=\t[0\tfor\t_\tin\trange(K)]\n\nThe\ttotal\tnumber\tof\twords\tcontained\tin\teach\tdocument:\n\n#\ta\tlist\tof\tnumbers,\tone\tfor\teach\tdocument\ndocument_lengths\t=\tmap(len,\tdocuments)\n\nThe\tnumber\tof\tdistinct\twords:\n\ndistinct_words\t=\tset(word\tfor\tdocument\tin\tdocuments\tfor\tword\tin\tdocument)\nW\t=\tlen(distinct_words)\n\nAnd\tthe\tnumber\tof\tdocuments:\n\nD\t=\tlen(documents)\n\nFor\texample,\tonce\twe\tpopulate\tthese,\twe\tcan\tfind,\tfor\texample,\tthe\tnumber\tof\twords\tin\ndocuments[3]\tassociated\twith\ttopic\t1\tas:\n\ndocument_topic_counts[3][1]\n\nAnd\twe\tcan\tfind\tthe\tnumber\tof\ttimes\tnlp\tis\tassociated\twith\ttopic\t2\tas:\n\ntopic_word_counts[2][\"nlp\"]\n\nNow\twe\u2019re\tready\tto\tdefine\tour\tconditional\tprobability\tfunctions.\tAs\tin\tChapter\t13,\teach\nhas\ta\tsmoothing\tterm\tthat\tensures\tevery\ttopic\thas\ta\tnonzero\tchance\tof\tbeing\tchosen\tin\nany\tdocument\tand\tthat\tevery\tword\thas\ta\tnonzero\tchance\tof\tbeing\tchosen\tfor\tany\ttopic:\n\ndef\tp_topic_given_document(topic,\td,\talpha=0.1):\n\t\t\t\t\"\"\"the\tfraction\tof\twords\tin\tdocument\t_d_\n\t\t\t\tthat\tare\tassigned\tto\t_topic_\t(plus\tsome\tsmoothing)\"\"\"\n\n\t\t\t\treturn\t((document_topic_counts[d][topic]\t+\talpha)\t/\n\t\t\t\t\t\t\t\t\t\t\t\t(document_lengths[d]\t+\tK\t*\talpha))\n\ndef\tp_word_given_topic(word,\ttopic,\tbeta=0.1):\n\t\t\t\t\"\"\"the\tfraction\tof\twords\tassigned\tto\t_topic_\n\t\t\t\tthat\tequal\t_word_\t(plus\tsome\tsmoothing)\"\"\"\n\n\t\t\t\treturn\t((topic_word_counts[topic][word]\t+\tbeta)\t/\n\t\t\t\t\t\t\t\t\t\t\t\t(topic_counts[topic]\t+\tW\t*\tbeta))\n\nWe\u2019ll\tuse\tthese\tto\tcreate\tthe\tweights\tfor\tupdating\ttopics:\n\ndef\ttopic_weight(d,\tword,\tk):\n\t\t\t\t\"\"\"given\ta\tdocument\tand\ta\tword\tin\tthat\tdocument,\n\t\t\t\treturn\tthe\tweight\tfor\tthe\tkth\ttopic\"\"\"\n\n\t\t\t\treturn\tp_word_given_topic(word,\tk)\t*\tp_topic_given_document(k,\td)",
    "346": "def\tchoose_new_topic(d,\tword):\n\t\t\t\treturn\tsample_from([topic_weight(d,\tword,\tk)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tk\tin\trange(K)])\n\nThere\tare\tsolid\tmathematical\treasons\twhy\ttopic_weight\tis\tdefined\tthe\tway\tit\tis,\tbut\ttheir\ndetails\twould\tlead\tus\ttoo\tfar\tafield.\tHopefully\tit\tmakes\tat\tleast\tintuitive\tsense\tthat\t\u2014\ngiven\ta\tword\tand\tits\tdocument\t\u2014\tthe\tlikelihood\tof\tany\ttopic\tchoice\tdepends\ton\tboth\thow\nlikely\tthat\ttopic\tis\tfor\tthe\tdocument\tand\thow\tlikely\tthat\tword\tis\tfor\tthe\ttopic.\n\nThis\tis\tall\tthe\tmachinery\twe\tneed.\tWe\tstart\tby\tassigning\tevery\tword\tto\ta\trandom\ttopic,\nand\tpopulating\tour\tcounters\tappropriately:\n\nrandom.seed(0)\ndocument_topics\t=\t[[random.randrange(K)\tfor\tword\tin\tdocument]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tdocument\tin\tdocuments]\n\nfor\td\tin\trange(D):\n\t\t\t\tfor\tword,\ttopic\tin\tzip(documents[d],\tdocument_topics[d]):\n\t\t\t\t\t\t\t\tdocument_topic_counts[d][topic]\t+=\t1\n\t\t\t\t\t\t\t\ttopic_word_counts[topic][word]\t+=\t1\n\t\t\t\t\t\t\t\ttopic_counts[topic]\t+=\t1\n\nOur\tgoal\tis\tto\tget\ta\tjoint\tsample\tof\tthe\ttopics-words\tdistribution\tand\tthe\tdocuments-topics\ndistribution.\tWe\tdo\tthis\tusing\ta\tform\tof\tGibbs\tsampling\tthat\tuses\tthe\tconditional\nprobabilities\tdefined\tpreviously:\n\nfor\titer\tin\trange(1000):\n\t\t\t\tfor\td\tin\trange(D):\n\t\t\t\t\t\t\t\tfor\ti,\t(word,\ttopic)\tin\tenumerate(zip(documents[d],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdocument_topics[d])):\n\n\t\t\t\t\t\t\t\t\t\t\t\t#\tremove\tthis\tword\t/\ttopic\tfrom\tthe\tcounts\n\t\t\t\t\t\t\t\t\t\t\t\t#\tso\tthat\tit\tdoesn't\tinfluence\tthe\tweights\n\t\t\t\t\t\t\t\t\t\t\t\tdocument_topic_counts[d][topic]\t-=\t1\n\t\t\t\t\t\t\t\t\t\t\t\ttopic_word_counts[topic][word]\t-=\t1\n\t\t\t\t\t\t\t\t\t\t\t\ttopic_counts[topic]\t-=\t1\n\t\t\t\t\t\t\t\t\t\t\t\tdocument_lengths[d]\t-=\t1\n\n\t\t\t\t\t\t\t\t\t\t\t\t#\tchoose\ta\tnew\ttopic\tbased\ton\tthe\tweights\n\t\t\t\t\t\t\t\t\t\t\t\tnew_topic\t=\tchoose_new_topic(d,\tword)\n\t\t\t\t\t\t\t\t\t\t\t\tdocument_topics[d][i]\t=\tnew_topic\n\n\t\t\t\t\t\t\t\t\t\t\t\t#\tand\tnow\tadd\tit\tback\tto\tthe\tcounts\n\t\t\t\t\t\t\t\t\t\t\t\tdocument_topic_counts[d][new_topic]\t+=\t1\n\t\t\t\t\t\t\t\t\t\t\t\ttopic_word_counts[new_topic][word]\t+=\t1\n\t\t\t\t\t\t\t\t\t\t\t\ttopic_counts[new_topic]\t+=\t1\n\t\t\t\t\t\t\t\t\t\t\t\tdocument_lengths[d]\t+=\t1\n\nWhat\tare\tthe\ttopics?\tThey\u2019re\tjust\tnumbers\t0,\t1,\t2,\tand\t3.\tIf\twe\twant\tnames\tfor\tthem\twe\nhave\tto\tdo\tthat\tourselves.\tLet\u2019s\tlook\tat\tthe\tfive\tmost\theavily\tweighted\twords\tfor\teach\n(Table\t20-1):\n\nfor\tk,\tword_counts\tin\tenumerate(topic_word_counts):\n\t\t\t\tfor\tword,\tcount\tin\tword_counts.most_common():\n\t\t\t\t\t\t\t\tif\tcount\t>\t0:\tprint\tk,\tword,\tcount\n\nTable\t20-1.\tMost\tcommon\twords\tper\ttopic\n\nTopic\t0\n\nTopic\t1\n\nTopic\t2\n\nTopic\t3",
    "347": "Java\n\nR\n\nHBase\n\nregression\n\nBig\tData\n\nstatistics\n\nPostgres\n\nlibsvm\n\nHadoop\n\nPython\n\nMongoDB scikit-learn\n\ndeep\tlearning\n\nprobability Cassandra machine\tlearning\n\nartificial\tintelligence pandas\n\nNoSQL\n\nneural\tnetworks\n\nBased\ton\tthese\tI\u2019d\tprobably\tassign\ttopic\tnames:\n\ntopic_names\t=\t[\"Big\tData\tand\tprogramming\tlanguages\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Python\tand\tstatistics\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"databases\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"machine\tlearning\"]\n\nat\twhich\tpoint\twe\tcan\tsee\thow\tthe\tmodel\tassigns\ttopics\tto\teach\tuser\u2019s\tinterests:\n\nfor\tdocument,\ttopic_counts\tin\tzip(documents,\tdocument_topic_counts):\n\t\t\t\tprint\tdocument\n\t\t\t\tfor\ttopic,\tcount\tin\ttopic_counts.most_common():\n\t\t\t\t\t\t\t\tif\tcount\t>\t0:\n\t\t\t\t\t\t\t\t\t\t\t\tprint\ttopic_names[topic],\tcount,\n\t\t\t\tprint\n\nwhich\tgives:\n\n['Hadoop',\t'Big\tData',\t'HBase',\t'Java',\t'Spark',\t'Storm',\t'Cassandra']\nBig\tData\tand\tprogramming\tlanguages\t4\tdatabases\t3\n['NoSQL',\t'MongoDB',\t'Cassandra',\t'HBase',\t'Postgres']\ndatabases\t5\n['Python',\t'scikit-learn',\t'scipy',\t'numpy',\t'statsmodels',\t'pandas']\nPython\tand\tstatistics\t5\tmachine\tlearning\t1\n\nand\tso\ton.\tGiven\tthe\t\u201cands\u201d\twe\tneeded\tin\tsome\tof\tour\ttopic\tnames,\tit\u2019s\tpossible\twe\nshould\tuse\tmore\ttopics,\talthough\tmost\tlikely\twe\tdon\u2019t\thave\tenough\tdata\tto\tsuccessfully\nlearn\tthem.",
    "348": "For\tFurther\tExploration\n\nNatural\tLanguage\tToolkit\tis\ta\tpopular\t(and\tpretty\tcomprehensive)\tlibrary\tof\tNLP\ttools\nfor\tPython.\tIt\thas\tits\town\tentire\tbook,\twhich\tis\tavailable\tto\tread\tonline.\n\ngensim\tis\ta\tPython\tlibrary\tfor\ttopic\tmodeling,\twhich\tis\ta\tbetter\tbet\tthan\tour\tfrom-\nscratch\tmodel.",
    "349": "",
    "350": "Chapter\t21.\tNetwork\tAnalysis\n\nYour\tconnections\tto\tall\tthe\tthings\taround\tyou\tliterally\tdefine\twho\tyou\tare.\n\nAaron\tO\u2019Connell\n\nMany\tinteresting\tdata\tproblems\tcan\tbe\tfruitfully\tthought\tof\tin\tterms\tof\tnetworks,\nconsisting\tof\tnodes\tof\tsome\ttype\tand\tthe\tedges\tthat\tjoin\tthem.\n\nFor\tinstance,\tyour\tFacebook\tfriends\tform\tthe\tnodes\tof\ta\tnetwork\twhose\tedges\tare\nfriendship\trelations.\tA\tless\tobvious\texample\tis\tthe\tWorld\tWide\tWeb\titself,\twith\teach\tweb\npage\ta\tnode,\tand\teach\thyperlink\tfrom\tone\tpage\tto\tanother\tan\tedge.\n\nFacebook\tfriendship\tis\tmutual\t\u2014\tif\tI\tam\tFacebook\tfriends\twith\tyou\tthan\tnecessarily\tyou\nare\tfriends\twith\tme.\tIn\tthis\tcase,\twe\tsay\tthat\tthe\tedges\tare\tundirected.\tHyperlinks\tare\tnot\n\u2014\tmy\twebsite\tlinks\tto\twhitehouse.gov,\tbut\t(for\treasons\tinexplicable\tto\tme)\nwhitehouse.gov\trefuses\tto\tlink\tto\tmy\twebsite.\tWe\tcall\tthese\ttypes\tof\tedges\tdirected.\tWe\u2019ll\nlook\tat\tboth\tkinds\tof\tnetworks.",
    "351": "Betweenness\tCentrality\n\nIn\tChapter\t1,\twe\tcomputed\tthe\tkey\tconnectors\tin\tthe\tDataSciencester\tnetwork\tby\tcounting\nthe\tnumber\tof\tfriends\teach\tuser\thad.\tNow\twe\thave\tenough\tmachinery\tto\tlook\tat\tother\napproaches.\tRecall\tthat\tthe\tnetwork\t(Figure\t21-1)\tcomprised\tusers:\n\nusers\t=\t[\n\t\t\t\t{\t\"id\":\t0,\t\"name\":\t\"Hero\"\t},\n\t\t\t\t{\t\"id\":\t1,\t\"name\":\t\"Dunn\"\t},\n\t\t\t\t{\t\"id\":\t2,\t\"name\":\t\"Sue\"\t},\n\t\t\t\t{\t\"id\":\t3,\t\"name\":\t\"Chi\"\t},\n\t\t\t\t{\t\"id\":\t4,\t\"name\":\t\"Thor\"\t},\n\t\t\t\t{\t\"id\":\t5,\t\"name\":\t\"Clive\"\t},\n\t\t\t\t{\t\"id\":\t6,\t\"name\":\t\"Hicks\"\t},\n\t\t\t\t{\t\"id\":\t7,\t\"name\":\t\"Devin\"\t},\n\t\t\t\t{\t\"id\":\t8,\t\"name\":\t\"Kate\"\t},\n\t\t\t\t{\t\"id\":\t9,\t\"name\":\t\"Klein\"\t}\n]\n\nand\tfriendships:\n\nfriendships\t=\t[(0,\t1),\t(0,\t2),\t(1,\t2),\t(1,\t3),\t(2,\t3),\t(3,\t4),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(4,\t5),\t(5,\t6),\t(5,\t7),\t(6,\t8),\t(7,\t8),\t(8,\t9)]\n\nFigure\t21-1.\tThe\tDataSciencester\tnetwork\n\nWe\talso\tadded\tfriend\tlists\tto\teach\tuser\tdict:\n\nfor\tuser\tin\tusers:\n\t\t\t\tuser[\"friends\"]\t=\t[]\n\nfor\ti,\tj\tin\tfriendships:\n\t\t\t\t#\tthis\tworks\tbecause\tusers[i]\tis\tthe\tuser\twhose\tid\tis\ti\n\t\t\t\tusers[i][\"friends\"].append(users[j])\t#\tadd\ti\tas\ta\tfriend\tof\tj\n\t\t\t\tusers[j][\"friends\"].append(users[i])\t#\tadd\tj\tas\ta\tfriend\tof\ti\n\nWhen\twe\tleft\toff\twe\twere\tdissatisfied\twith\tour\tnotion\tof\tdegree\tcentrality,\twhich\tdidn\u2019t\nreally\tagree\twith\tour\tintuition\tabout\twho\twere\tthe\tkey\tconnectors\tof\tthe\tnetwork.\n\nAn\talternative\tmetric\tis\tbetweenness\tcentrality,\twhich\tidentifies\tpeople\twho\tfrequently\nare\ton\tthe\tshortest\tpaths\tbetween\tpairs\tof\tother\tpeople.\tIn\tparticular,\tthe\tbetweenness\ncentrality\tof\tnode\ti\tis\tcomputed\tby\tadding\tup,\tfor\tevery\tother\tpair\tof\tnodes\tj\tand\tk,\tthe\nproportion\tof\tshortest\tpaths\tbetween\tnode\tj\tand\tnode\tk\tthat\tpass\tthrough\ti.",
    "352": "That\tis,\tto\tfigure\tout\tThor\u2019s\tbetweenness\tcentrality,\twe\u2019ll\tneed\tto\tcompute\tall\tthe\tshortest\npaths\tbetween\tall\tpairs\tof\tpeople\twho\taren\u2019t\tThor.\tAnd\tthen\twe\u2019ll\tneed\tto\tcount\thow\nmany\tof\tthose\tshortest\tpaths\tpass\tthrough\tThor.\tFor\tinstance,\tthe\tonly\tshortest\tpath\nbetween\tChi\t(id\t3)\tand\tClive\t(id\t5)\tpasses\tthrough\tThor,\twhile\tneither\tof\tthe\ttwo\tshortest\npaths\tbetween\tHero\t(id\t0)\tand\tChi\t(id\t3)\tdoes.\n\nSo,\tas\ta\tfirst\tstep,\twe\u2019ll\tneed\tto\tfigure\tout\tthe\tshortest\tpaths\tbetween\tall\tpairs\tof\tpeople.\nThere\tare\tsome\tpretty\tsophisticated\talgorithms\tfor\tdoing\tso\tefficiently,\tbut\t(as\tis\talmost\nalways\tthe\tcase)\twe\twill\tuse\ta\tless\tefficient,\teasier-to-understand\talgorithm.\n\nThis\talgorithm\t(an\timplementation\tof\tbreadth-first\tsearch)\tis\tone\tof\tthe\tmore\tcomplicated\nones\tin\tthe\tbook,\tso\tlet\u2019s\ttalk\tthrough\tit\tcarefully:\n\n1.\t Our\tgoal\tis\ta\tfunction\tthat\ttakes\ta\tfrom_user\tand\tfinds\tall\tshortest\tpaths\tto\tevery\n\nother\tuser.\n\n2.\t We\u2019ll\trepresent\ta\tpath\tas\tlist\tof\tuser\tIDs.\tSince\tevery\tpath\tstarts\tat\tfrom_user,\twe\n\nwon\u2019t\tinclude\ther\tID\tin\tthe\tlist.\tThis\tmeans\tthat\tthe\tlength\tof\tthe\tlist\trepresenting\tthe\npath\twill\tbe\tthe\tlength\tof\tthe\tpath\titself.\n\n3.\t We\u2019ll\tmaintain\ta\tdictionary\tshortest_paths_to\twhere\tthe\tkeys\tare\tuser\tIDs\tand\tthe\nvalues\tare\tlists\tof\tpaths\tthat\tend\tat\tthe\tuser\twith\tthe\tspecified\tID.\tIf\tthere\tis\ta\tunique\nshortest\tpath,\tthe\tlist\twill\tjust\tcontain\tthat\tone\tpath.\tIf\tthere\tare\tmultiple\tshortest\npaths,\tthe\tlist\twill\tcontain\tall\tof\tthem.\n\n4.\t We\u2019ll\talso\tmaintain\ta\tqueue\tfrontier\tthat\tcontains\tthe\tusers\twe\twant\tto\texplore\tin\nthe\torder\twe\twant\tto\texplore\tthem.\tWe\u2019ll\tstore\tthem\tas\tpairs\t(prev_user,\tuser)\tso\nthat\twe\tknow\thow\twe\tgot\tto\teach\tone.\tWe\tinitialize\tthe\tqueue\twith\tall\tthe\tneighbors\nof\tfrom_user.\t(We\thaven\u2019t\tever\ttalked\tabout\tqueues,\twhich\tare\tdata\tstructures\noptimized\tfor\t\u201cadd\tto\tthe\tend\u201d\tand\t\u201cremove\tfrom\tthe\tfront\u201d\toperations.\tIn\tPython,\nthey\tare\timplemented\tas\tcollections.deque\twhich\tis\tactually\ta\tdouble-ended\nqueue.)\n\n5.\t As\twe\texplore\tthe\tgraph,\twhenever\twe\tfind\tnew\tneighbors\tthat\twe\tdon\u2019t\talready\n\nknow\tshortest\tpaths\tto,\twe\tadd\tthem\tto\tthe\tend\tof\tthe\tqueue\tto\texplore\tlater,\twith\tthe\ncurrent\tuser\tas\tprev_user.\n\n6.\t When\twe\ttake\ta\tuser\toff\tthe\tqueue,\tand\twe\u2019ve\tnever\tencountered\tthat\tuser\tbefore,\nwe\u2019ve\tdefinitely\tfound\tone\tor\tmore\tshortest\tpaths\tto\thim\t\u2014\teach\tof\tthe\tshortest\npaths\tto\tprev_user\twith\tone\textra\tstep\tadded.\n\n7.\t When\twe\ttake\ta\tuser\toff\tthe\tqueue\tand\twe\thave\tencountered\tthat\tuser\tbefore,\tthen\neither\twe\u2019ve\tfound\tanother\tshortest\tpath\t(in\twhich\tcase\twe\tshould\tadd\tit)\tor\twe\u2019ve\nfound\ta\tlonger\tpath\t(in\twhich\tcase\twe\tshouldn\u2019t).\n\n8.\t When\tno\tmore\tusers\tare\tleft\ton\tthe\tqueue,\twe\u2019ve\texplored\tthe\twhole\tgraph\t(or,\tat\nleast,\tthe\tparts\tof\tit\tthat\tare\treachable\tfrom\tthe\tstarting\tuser)\tand\twe\u2019re\tdone.",
    "353": "We\tcan\tput\tthis\tall\ttogether\tinto\ta\t(large)\tfunction:\n\nfrom\tcollections\timport\tdeque\n\ndef\tshortest_paths_from(from_user):\n\n\t\t\t\t#\ta\tdictionary\tfrom\t\"user_id\"\tto\t*all*\tshortest\tpaths\tto\tthat\tuser\n\t\t\t\tshortest_paths_to\t=\t{\tfrom_user[\"id\"]\t:\t[[]]\t}\n\n\t\t\t\t#\ta\tqueue\tof\t(previous\tuser,\tnext\tuser)\tthat\twe\tneed\tto\tcheck.\n\t\t\t\t#\tstarts\tout\twith\tall\tpairs\t(from_user,\tfriend_of_from_user)\n\t\t\t\tfrontier\t=\tdeque((from_user,\tfriend)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tfriend\tin\tfrom_user[\"friends\"])\n\n\t\t\t\t#\tkeep\tgoing\tuntil\twe\tempty\tthe\tqueue\n\t\t\t\twhile\tfrontier:\n\n\t\t\t\t\t\t\t\tprev_user,\tuser\t=\tfrontier.popleft()\t\t\t#\tremove\tthe\tuser\twho's\n\t\t\t\t\t\t\t\tuser_id\t=\tuser[\"id\"]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tfirst\tin\tthe\tqueue\n\n\t\t\t\t\t\t\t\t#\tbecause\tof\tthe\tway\twe're\tadding\tto\tthe\tqueue,\n\t\t\t\t\t\t\t\t#\tnecessarily\twe\talready\tknow\tsome\tshortest\tpaths\tto\tprev_user\n\t\t\t\t\t\t\t\tpaths_to_prev_user\t=\tshortest_paths_to[prev_user[\"id\"]]\n\t\t\t\t\t\t\t\tnew_paths_to_user\t=\t[path\t+\t[user_id]\tfor\tpath\tin\tpaths_to_prev_user]\n\n\t\t\t\t\t\t\t\t#\tit's\tpossible\twe\talready\tknow\ta\tshortest\tpath\n\t\t\t\t\t\t\t\told_paths_to_user\t=\tshortest_paths_to.get(user_id,\t[])\n\n\t\t\t\t\t\t\t\t#\twhat's\tthe\tshortest\tpath\tto\there\tthat\twe've\tseen\tso\tfar?\n\t\t\t\t\t\t\t\tif\told_paths_to_user:\n\t\t\t\t\t\t\t\t\t\t\t\tmin_path_length\t=\tlen(old_paths_to_user[0])\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t\t\tmin_path_length\t=\tfloat('inf')\n\n\t\t\t\t\t\t\t\t#\tonly\tkeep\tpaths\tthat\taren't\ttoo\tlong\tand\tare\tactually\tnew\n\t\t\t\t\t\t\t\tnew_paths_to_user\t=\t[path\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tpath\tin\tnew_paths_to_user\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tlen(path)\t<=\tmin_path_length\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tand\tpath\tnot\tin\told_paths_to_user]\n\n\t\t\t\t\t\t\t\tshortest_paths_to[user_id]\t=\told_paths_to_user\t+\tnew_paths_to_user\n\n\t\t\t\t\t\t\t\t#\tadd\tnever-seen\tneighbors\tto\tthe\tfrontier\n\t\t\t\t\t\t\t\tfrontier.extend((user,\tfriend)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tfriend\tin\tuser[\"friends\"]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tfriend[\"id\"]\tnot\tin\tshortest_paths_to)\n\n\t\t\t\treturn\tshortest_paths_to\n\nNow\twe\tcan\tstore\tthese\tdicts\twith\teach\tnode:\n\nfor\tuser\tin\tusers:\n\t\t\t\tuser[\"shortest_paths\"]\t=\tshortest_paths_from(user)\n\nAnd\twe\u2019re\tfinally\tready\tto\tcompute\tbetweenness\tcentrality.\tFor\tevery\tpair\tof\tnodes\ti\tand\nj,\twe\tknow\tthe\tn\tshortest\tpaths\tfrom\ti\tto\tj.\tThen,\tfor\teach\tof\tthose\tpaths,\twe\tjust\tadd\t1/n\tto\nthe\tcentrality\tof\teach\tnode\ton\tthat\tpath:\n\nfor\tuser\tin\tusers:\n\t\t\t\tuser[\"betweenness_centrality\"]\t=\t0.0\n\nfor\tsource\tin\tusers:\n\t\t\t\tsource_id\t=\tsource[\"id\"]\n\t\t\t\tfor\ttarget_id,\tpaths\tin\tsource[\"shortest_paths\"].iteritems():\n\t\t\t\t\t\t\t\tif\tsource_id\t<\ttarget_id:\t\t\t\t\t\t#\tdon't\tdouble\tcount\n\t\t\t\t\t\t\t\t\t\t\t\tnum_paths\t=\tlen(paths)\t\t\t\t\t#\thow\tmany\tshortest\tpaths?\n\t\t\t\t\t\t\t\t\t\t\t\tcontrib\t=\t1\t/\tnum_paths\t\t\t\t#\tcontribution\tto\tcentrality\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tpath\tin\tpaths:",
    "354": "for\tid\tin\tpath:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tid\tnot\tin\t[source_id,\ttarget_id]:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tusers[id][\"betweenness_centrality\"]\t+=\tcontrib\n\nFigure\t21-2.\tThe\tDataSciencester\tnetwork\tsized\tby\tbetweenness\tcentrality\n\nAs\tshown\tin\tFigure\t21-2,\tusers\t0\tand\t9\thave\tcentrality\t0\t(as\tneither\tis\ton\tany\tshortest\tpath\nbetween\tother\tusers),\twhereas\t3,\t4,\tand\t5\tall\thave\thigh\tcentralities\t(as\tall\tthree\tlie\ton\nmany\tshortest\tpaths).\n\nGenerally\tthe\tcentrality\tnumbers\taren\u2019t\tthat\tmeaningful\tthemselves.\tWhat\twe\tcare\tabout\tis\thow\tthe\nnumbers\tfor\teach\tnode\tcompare\tto\tthe\tnumbers\tfor\tother\tnodes.\n\nNOTE\n\nAnother\tmeasure\twe\tcan\tlook\tat\tis\tcloseness\tcentrality.\tFirst,\tfor\teach\tuser\twe\tcompute\nher\tfarness,\twhich\tis\tthe\tsum\tof\tthe\tlengths\tof\ther\tshortest\tpaths\tto\teach\tother\tuser.\tSince\nwe\u2019ve\talready\tcomputed\tthe\tshortest\tpaths\tbetween\teach\tpair\tof\tnodes,\tit\u2019s\teasy\tto\tadd\ntheir\tlengths.\t(If\tthere\tare\tmultiple\tshortest\tpaths,\tthey\tall\thave\tthe\tsame\tlength,\tso\twe\tcan\njust\tlook\tat\tthe\tfirst\tone.)\n\ndef\tfarness(user):\n\t\t\t\t\"\"\"the\tsum\tof\tthe\tlengths\tof\tthe\tshortest\tpaths\tto\teach\tother\tuser\"\"\"\n\t\t\t\treturn\tsum(len(paths[0])\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tpaths\tin\tuser[\"shortest_paths\"].values())\n\nafter\twhich\tit\u2019s\tvery\tlittle\twork\tto\tcompute\tcloseness\tcentrality\t(Figure\t21-3):\n\nfor\tuser\tin\tusers:\n\t\t\t\tuser[\"closeness_centrality\"]\t=\t1\t/\tfarness(user)",
    "355": "Figure\t21-3.\tThe\tDataSciencester\tnetwork\tsized\tby\tcloseness\tcentrality\n\nThere\tis\tmuch\tless\tvariation\there\t\u2014\teven\tthe\tvery\tcentral\tnodes\tare\tstill\tpretty\tfar\tfrom\nthe\tnodes\tout\ton\tthe\tperiphery.\n\nAs\twe\tsaw,\tcomputing\tshortest\tpaths\tis\tkind\tof\ta\tpain.\tFor\tthis\treason,\tbetweenness\tand\ncloseness\tcentrality\taren\u2019t\toften\tused\ton\tlarge\tnetworks.\tThe\tless\tintuitive\t(but\tgenerally\neasier\tto\tcompute)\teigenvector\tcentrality\tis\tmore\tfrequently\tused.",
    "356": "Eigenvector\tCentrality\n\nIn\torder\tto\ttalk\tabout\teigenvector\tcentrality,\twe\thave\tto\ttalk\tabout\teigenvectors,\tand\tin\norder\tto\ttalk\tabout\teigenvectors,\twe\thave\tto\ttalk\tabout\tmatrix\tmultiplication.",
    "357": "Matrix\tMultiplication\n\nIf\tA\tis\ta\t\n\nAB\tis\tthe\t\n\n\tmatrix\tand\tB\tis\ta\t\n\n\tmatrix,\tand\tif\t\n\n,\tthen\ttheir\tproduct\n\n\tmatrix\twhose\t(i,j)th\tentry\tis:\n\nWhich\tis\tjust\tthe\tdot\tproduct\tof\tthe\tith\trow\tof\tA\t(thought\tof\tas\ta\tvector)\twith\tthe\tjth\ncolumn\tof\tB\t(also\tthought\tof\tas\ta\tvector):\n\ndef\tmatrix_product_entry(A,\tB,\ti,\tj):\n\t\t\t\treturn\tdot(get_row(A,\ti),\tget_column(B,\tj))\n\nafter\twhich\twe\thave:\n\ndef\tmatrix_multiply(A,\tB):\n\t\t\t\tn1,\tk1\t=\tshape(A)\n\t\t\t\tn2,\tk2\t=\tshape(B)\n\t\t\t\tif\tk1\t!=\tn2:\n\t\t\t\t\t\t\t\traise\tArithmeticError(\"incompatible\tshapes!\")\n\n\t\t\t\treturn\tmake_matrix(n1,\tk2,\tpartial(matrix_product_entry,\tA,\tB))\n\n\tmatrix\tand\tB\tis\ta\t\n\nNotice\tthat\tif\tA\tis\ta\t\nIf\twe\ttreat\ta\tvector\tas\ta\tone-column\tmatrix,\twe\tcan\tthink\tof\tA\tas\ta\tfunction\tthat\tmaps\tk-\ndimensional\tvectors\tto\tn-dimensional\tvectors,\twhere\tthe\tfunction\tis\tjust\tmatrix\nmultiplication.\n\n\tmatrix,\tthen\tAB\tis\ta\t\n\n\tmatrix.\n\nPreviously\twe\trepresented\tvectors\tsimply\tas\tlists,\twhich\tisn\u2019t\tquite\tthe\tsame:\n\nv\t=\t[1,\t2,\t3]\nv_as_matrix\t=\t[[1],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[2],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[3]]\n\nSo\twe\u2019ll\tneed\tsome\thelper\tfunctions\tto\tconvert\tback\tand\tforth\tbetween\tthe\ttwo\nrepresentations:\n\ndef\tvector_as_matrix(v):\n\t\t\t\t\"\"\"returns\tthe\tvector\tv\t(represented\tas\ta\tlist)\tas\ta\tn\tx\t1\tmatrix\"\"\"\n\t\t\t\treturn\t[[v_i]\tfor\tv_i\tin\tv]\n\ndef\tvector_from_matrix(v_as_matrix):\n\t\t\t\t\"\"\"returns\tthe\tn\tx\t1\tmatrix\tas\ta\tlist\tof\tvalues\"\"\"\n\t\t\t\treturn\t[row[0]\tfor\trow\tin\tv_as_matrix]\n\nafter\twhich\twe\tcan\tdefine\tthe\tmatrix\toperation\tusing\tmatrix_multiply:\n\ndef\tmatrix_operate(A,\tv):\n\t\t\t\tv_as_matrix\t=\tvector_as_matrix(v)\n\t\t\t\tproduct\t=\tmatrix_multiply(A,\tv_as_matrix)\n\t\t\t\treturn\tvector_from_matrix(product)",
    "358": "When\tA\tis\ta\tsquare\tmatrix,\tthis\toperation\tmaps\tn-dimensional\tvectors\tto\tother\tn-\ndimensional\tvectors.\tIt\u2019s\tpossible\tthat,\tfor\tsome\tmatrix\tA\tand\tvector\tv,\twhen\tA\toperates\ton\nv\twe\tget\tback\ta\tscalar\tmultiple\tof\tv.\tThat\tis,\tthat\tthe\tresult\tis\ta\tvector\tthat\tpoints\tin\tthe\nsame\tdirection\tas\tv.\tWhen\tthis\thappens\t(and\twhen,\tin\taddition,\tv\tis\tnot\ta\tvector\tof\tall\nzeroes),\twe\tcall\tv\tan\teigenvector\tof\tA.\tAnd\twe\tcall\tthe\tmultiplier\tan\teigenvalue.\n\nOne\tpossible\tway\tto\tfind\tan\teigenvector\tof\tA\tis\tby\tpicking\ta\tstarting\tvector\tv,\tapplying\nmatrix_operate,\trescaling\tthe\tresult\tto\thave\tmagnitude\t1,\tand\trepeating\tuntil\tthe\tprocess\nconverges:\n\ndef\tfind_eigenvector(A,\ttolerance=0.00001):\n\t\t\t\tguess\t=\t[random.random()\tfor\t__\tin\tA]\n\n\t\t\t\twhile\tTrue:\n\t\t\t\t\t\t\t\tresult\t=\tmatrix_operate(A,\tguess)\n\t\t\t\t\t\t\t\tlength\t=\tmagnitude(result)\n\t\t\t\t\t\t\t\tnext_guess\t=\tscalar_multiply(1/length,\tresult)\n\n\t\t\t\t\t\t\t\tif\tdistance(guess,\tnext_guess)\t<\ttolerance:\n\t\t\t\t\t\t\t\t\t\t\t\treturn\tnext_guess,\tlength\t\t\t#\teigenvector,\teigenvalue\n\n\t\t\t\t\t\t\t\tguess\t=\tnext_guess\n\nBy\tconstruction,\tthe\treturned\tguess\tis\ta\tvector\tsuch\tthat,\twhen\tyou\tapply\tmatrix_operate\nto\tit\tand\trescale\tit\tto\thave\tlength\t1,\tyou\tget\tback\t(a\tvector\tvery\tclose\tto)\titself.\tWhich\nmeans\tit\u2019s\tan\teigenvector.\n\nNot\tall\tmatrices\tof\treal\tnumbers\thave\teigenvectors\tand\teigenvalues.\tFor\texample\tthe\nmatrix:\n\nrotate\t=\t[[\t0,\t1],\n\t\t\t\t\t\t\t\t\t\t[-1,\t0]]\n\nrotates\tvectors\t90\tdegrees\tclockwise,\twhich\tmeans\tthat\tthe\tonly\tvector\tit\tmaps\tto\ta\tscalar\nmultiple\tof\titself\tis\ta\tvector\tof\tzeroes.\tIf\tyou\ttried\tfind_eigenvector(rotate)\tit\twould\nrun\tforever.\tEven\tmatrices\tthat\thave\teigenvectors\tcan\tsometimes\tget\tstuck\tin\tcycles.\nConsider\tthe\tmatrix:\n\nflip\t=\t[[0,\t1],\n\t\t\t\t\t\t\t\t[1,\t0]]\n\nThis\tmatrix\tmaps\tany\tvector\t[x,\ty]\tto\t[y,\tx].\tThis\tmeans\tthat,\tfor\texample,\t[1,\t1]\tis\tan\neigenvector\twith\teigenvalue\t1.\tHowever,\tif\tyou\tstart\twith\ta\trandom\tvector\twith\tunequal\ncoordinates,\tfind_eigenvector\twill\tjust\trepeatedly\tswap\tthe\tcoordinates\tforever.\t(Not-\nfrom-scratch\tlibraries\tlike\tNumPy\tuse\tdifferent\tmethods\tthat\twould\twork\tin\tthis\tcase.)\nNonetheless,\twhen\tfind_eigenvector\tdoes\treturn\ta\tresult,\tthat\tresult\tis\tindeed\tan\neigenvector.",
    "359": "Centrality\n\nHow\tdoes\tthis\thelp\tus\tunderstand\tthe\tDataSciencester\tnetwork?\n\nTo\tstart\twith,\twe\u2019ll\tneed\tto\trepresent\tthe\tconnections\tin\tour\tnetwork\tas\tan\nadjacency_matrix,\twhose\t(i,j)th\tentry\tis\teither\t1\t(if\tuser\ti\tand\tuser\tj\tare\tfriends)\tor\t0\t(if\nthey\u2019re\tnot):\n\ndef\tentry_fn(i,\tj):\n\t\t\t\treturn\t1\tif\t(i,\tj)\tin\tfriendships\tor\t(j,\ti)\tin\tfriendships\telse\t0\n\nn\t=\tlen(users)\nadjacency_matrix\t=\tmake_matrix(n,\tn,\tentry_fn)\n\nThe\teigenvector\tcentrality\tfor\teach\tuser\tis\tthen\tthe\tentry\tcorresponding\tto\tthat\tuser\tin\tthe\neigenvector\treturned\tby\tfind_eigenvector\t(Figure\t21-4):\n\nFor\ttechnical\treasons\tthat\tare\tway\tbeyond\tthe\tscope\tof\tthis\tbook,\tany\tnonzero\tadjacency\tmatrix\tnecessarily\nhas\tan\teigenvector\tall\tof\twhose\tvalues\tare\tnon-negative.\tAnd\tfortunately\tfor\tus,\tfor\tthis\tadjacency_matrix\nour\tfind_eigenvector\tfunction\tfinds\tit.\n\nNOTE\n\neigenvector_centralities,\t_\t=\tfind_eigenvector(adjacency_matrix)\n\nFigure\t21-4.\tThe\tDataSciencester\tnetwork\tsized\tby\teigenvector\tcentrality\n\nUsers\twith\thigh\teigenvector\tcentrality\tshould\tbe\tthose\twho\thave\ta\tlot\tof\tconnections\tand\nconnections\tto\tpeople\twho\tthemselves\thave\thigh\tcentrality.\n\nHere\tusers\t1\tand\t2\tare\tthe\tmost\tcentral,\tas\tthey\tboth\thave\tthree\tconnections\tto\tpeople\twho\nare\tthemselves\thighly\tcentral.\tAs\twe\tmove\taway\tfrom\tthem,\tpeople\u2019s\tcentralities\tsteadily\ndrop\toff.\n\nOn\ta\tnetwork\tthis\tsmall,\teigenvector\tcentrality\tbehaves\tsomewhat\terratically.\tIf\tyou\ttry\nadding\tor\tsubtracting\tlinks,\tyou\u2019ll\tfind\tthat\tsmall\tchanges\tin\tthe\tnetwork\tcan\tdramatically\nchange\tthe\tcentrality\tnumbers.\tIn\ta\tmuch\tlarger\tnetwork\tthis\twould\tnot\tparticularly\tbe\tthe\ncase.\n\nWe\tstill\thaven\u2019t\tmotivated\twhy\tan\teigenvector\tmight\tlead\tto\ta\treasonable\tnotion\tof",
    "360": "centrality.\tBeing\tan\teigenvector\tmeans\tthat\tif\tyou\tcompute:\n\nmatrix_operate(adjacency_matrix,\teigenvector_centralities)\n\nthe\tresult\tis\ta\tscalar\tmultiple\tof\teigenvector_centralities.\n\nIf\tyou\tlook\tat\thow\tmatrix\tmultiplication\tworks,\tmatrix_operate\tproduces\ta\tvector\twhose\nith\telement\tis:\n\ndot(get_row(adjacency_matrix,\ti),\teigenvector_centralities)\n\nwhich\tis\tprecisely\tthe\tsum\tof\tthe\teigenvector\tcentralities\tof\tthe\tusers\tconnected\tto\tuser\ti.\n\nIn\tother\twords,\teigenvector\tcentralities\tare\tnumbers,\tone\tper\tuser,\tsuch\tthat\teach\tuser\u2019s\nvalue\tis\ta\tconstant\tmultiple\tof\tthe\tsum\tof\this\tneighbors\u2019\tvalues.\tIn\tthis\tcase\tcentrality\nmeans\tbeing\tconnected\tto\tpeople\twho\tthemselves\tare\tcentral.\tThe\tmore\tcentrality\tyou\tare\ndirectly\tconnected\tto,\tthe\tmore\tcentral\tyou\tare.\tThis\tis\tof\tcourse\ta\tcircular\tdefinition\t\u2014\neigenvectors\tare\tthe\tway\tof\tbreaking\tout\tof\tthe\tcircularity.\n\nAnother\tway\tof\tunderstanding\tthis\tis\tby\tthinking\tabout\twhat\tfind_eigenvector\tis\tdoing\nhere.\tIt\tstarts\tby\tassigning\teach\tnode\ta\trandom\tcentrality.\tIt\tthen\trepeats\tthe\tfollowing\ttwo\nsteps\tuntil\tthe\tprocess\tconverges:\n\n1.\t Give\teach\tnode\ta\tnew\tcentrality\tscore\tthat\tequals\tthe\tsum\tof\tits\tneighbors\u2019\t(old)\n\ncentrality\tscores.\n\n2.\t Rescale\tthe\tvector\tof\tcentralities\tto\thave\tmagnitude\t1.\n\nAlthough\tthe\tmathematics\tbehind\tit\tmay\tseem\tsomewhat\topaque\tat\tfirst,\tthe\tcalculation\nitself\tis\trelatively\tstraightforward\t(unlike,\tsay,\tbetweenness\tcentrality)\tand\tis\tpretty\teasy\nto\tperform\ton\teven\tvery\tlarge\tgraphs.",
    "361": "Directed\tGraphs\tand\tPageRank\n\nDataSciencester\tisn\u2019t\tgetting\tmuch\ttraction,\tso\tthe\tVP\tof\tRevenue\tconsiders\tpivoting\tfrom\na\tfriendship\tmodel\tto\tan\tendorsement\tmodel.\tIt\tturns\tout\tthat\tno\tone\tparticularly\tcares\nwhich\tdata\tscientists\tare\tfriends\twith\tone\tanother,\tbut\ttech\trecruiters\tcare\tvery\tmuch\nwhich\tdata\tscientists\tare\trespected\tby\tother\tdata\tscientists.\n\nIn\tthis\tnew\tmodel,\twe\u2019ll\ttrack\tendorsements\t(source,\ttarget)\tthat\tno\tlonger\trepresent\ta\nreciprocal\trelationship,\tbut\trather\tthat\tsource\tendorses\ttarget\tas\tan\tawesome\tdata\nscientist\t(Figure\t21-5).\tWe\u2019ll\tneed\tto\taccount\tfor\tthis\tasymmetry:\n\nendorsements\t=\t[(0,\t1),\t(1,\t0),\t(0,\t2),\t(2,\t0),\t(1,\t2),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(2,\t1),\t(1,\t3),\t(2,\t3),\t(3,\t4),\t(5,\t4),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(5,\t6),\t(7,\t5),\t(6,\t8),\t(8,\t7),\t(8,\t9)]\n\nfor\tuser\tin\tusers:\n\t\t\t\tuser[\"endorses\"]\t=\t[]\t\t\t\t\t\t\t#\tadd\tone\tlist\tto\ttrack\toutgoing\tendorsements\n\t\t\t\tuser[\"endorsed_by\"]\t=\t[]\t\t\t\t#\tand\tanother\tto\ttrack\tendorsements\n\nfor\tsource_id,\ttarget_id\tin\tendorsements:\n\t\t\t\tusers[source_id][\"endorses\"].append(users[target_id])\n\t\t\t\tusers[target_id][\"endorsed_by\"].append(users[source_id])\n\nFigure\t21-5.\tThe\tDataSciencester\tnetwork\tof\tendorsements\n\nafter\twhich\twe\tcan\teasily\tfind\tthe\tmost_endorsed\tdata\tscientists\tand\tsell\tthat\tinformation\nto\trecruiters:\n\nendorsements_by_id\t=\t[(user[\"id\"],\tlen(user[\"endorsed_by\"]))\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tuser\tin\tusers]\n\nsorted(endorsements_by_id,\n\t\t\t\t\t\t\tkey=lambda\t(user_id,\tnum_endorsements):\tnum_endorsements,\n\t\t\t\t\t\t\treverse=True)\n\nHowever,\t\u201cnumber\tof\tendorsements\u201d\tis\tan\teasy\tmetric\tto\tgame.\tAll\tyou\tneed\tto\tdo\tis\ncreate\tphony\taccounts\tand\thave\tthem\tendorse\tyou.\tOr\tarrange\twith\tyour\tfriends\tto\nendorse\teach\tother.\t(As\tusers\t0,\t1,\tand\t2\tseem\tto\thave\tdone.)\n\nA\tbetter\tmetric\twould\ttake\tinto\taccount\twho\tendorses\tyou.\tEndorsements\tfrom\tpeople\nwho\thave\ta\tlot\tof\tendorsements\tshould\tsomehow\tcount\tmore\tthan\tendorsements\tfrom\npeople\twith\tfew\tendorsements.\tThis\tis\tthe\tessence\tof\tthe\tPageRank\talgorithm,\tused\tby",
    "362": "Google\tto\trank\twebsites\tbased\ton\twhich\tother\twebsites\tlink\tto\tthem,\twhich\tother\twebsites\nlink\tto\tthose,\tand\tso\ton.\n\n(If\tthis\tsort\tof\treminds\tyou\tof\tthe\tidea\tbehind\teigenvector\tcentrality,\tit\tshould.)\n\nA\tsimplified\tversion\tlooks\tlike\tthis:\n\n1.\t There\tis\ta\ttotal\tof\t1.0\t(or\t100%)\tPageRank\tin\tthe\tnetwork.\n\n2.\t Initially\tthis\tPageRank\tis\tequally\tdistributed\tamong\tnodes.\n\n3.\t At\teach\tstep,\ta\tlarge\tfraction\tof\teach\tnode\u2019s\tPageRank\tis\tdistributed\tevenly\tamong\n\nits\toutgoing\tlinks.\n\n4.\t At\teach\tstep,\tthe\tremainder\tof\teach\tnode\u2019s\tPageRank\tis\tdistributed\tevenly\tamong\tall\n\nnodes.\n\ndef\tpage_rank(users,\tdamping\t=\t0.85,\tnum_iters\t=\t100):\n\n\t\t\t\t#\tinitially\tdistribute\tPageRank\tevenly\n\t\t\t\tnum_users\t=\tlen(users)\n\t\t\t\tpr\t=\t{\tuser[\"id\"]\t:\t1\t/\tnum_users\tfor\tuser\tin\tusers\t}\n\n\t\t\t\t#\tthis\tis\tthe\tsmall\tfraction\tof\tPageRank\n\t\t\t\t#\tthat\teach\tnode\tgets\teach\titeration\n\t\t\t\tbase_pr\t=\t(1\t-\tdamping)\t/\tnum_users\n\n\t\t\t\tfor\t__\tin\trange(num_iters):\n\t\t\t\t\t\t\t\tnext_pr\t=\t{\tuser[\"id\"]\t:\tbase_pr\tfor\tuser\tin\tusers\t}\n\t\t\t\t\t\t\t\tfor\tuser\tin\tusers:\n\t\t\t\t\t\t\t\t\t\t\t\t#\tdistribute\tPageRank\tto\toutgoing\tlinks\n\t\t\t\t\t\t\t\t\t\t\t\tlinks_pr\t=\tpr[user[\"id\"]]\t*\tdamping\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tendorsee\tin\tuser[\"endorses\"]:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnext_pr[endorsee[\"id\"]]\t+=\tlinks_pr\t/\tlen(user[\"endorses\"])\n\n\t\t\t\t\t\t\t\tpr\t=\tnext_pr\n\n\t\t\t\treturn\tpr\n\nPageRank\t(Figure\t21-6)\tidentifies\tuser\t4\t(Thor)\tas\tthe\thighest\tranked\tdata\tscientist.\n\nFigure\t21-6.\tThe\tDataSciencester\tnetwork\tsized\tby\tPageRank\n\nEven\tthough\the\thas\tfewer\tendorsements\t(2)\tthan\tusers\t0,\t1,\tand\t2,\this\tendorsements\tcarry\nwith\tthem\trank\tfrom\ttheir\tendorsements.\tAdditionally,\tboth\tof\this\tendorsers\tendorsed\tonly\nhim,\twhich\tmeans\tthat\the\tdoesn\u2019t\thave\tto\tdivide\ttheir\trank\twith\tanyone\telse.",
    "363": "For\tFurther\tExploration\n\nThere\tare\tmany\tother\tnotions\tof\tcentrality\tbesides\tthe\tones\twe\tused\t(although\tthe\tones\nwe\tused\tare\tpretty\tmuch\tthe\tmost\tpopular\tones).\n\nNetworkX\tis\ta\tPython\tlibrary\tfor\tnetwork\tanalysis.\tIt\thas\tfunctions\tfor\tcomputing\ncentralities\tand\tfor\tvisualizing\tgraphs.\n\nGephi\tis\ta\tlove-it/hate-it\tGUI-based\tnetwork-visualization\ttool.",
    "364": "",
    "365": "Chapter\t22.\tRecommender\tSystems\n\nO\tnature,\tnature,\twhy\tart\tthou\tso\tdishonest,\tas\tever\tto\tsend\tmen\twith\tthese\tfalse\nrecommendations\tinto\tthe\tworld!\n\nHenry\tFielding\n\nAnother\tcommon\tdata\tproblem\tis\tproducing\trecommendations\tof\tsome\tsort.\tNetflix\nrecommends\tmovies\tyou\tmight\twant\tto\twatch.\tAmazon\trecommends\tproducts\tyou\tmight\nwant\tto\tbuy.\tTwitter\trecommends\tusers\tyou\tmight\twant\tto\tfollow.\tIn\tthis\tchapter,\twe\u2019ll\nlook\tat\tseveral\tways\tto\tuse\tdata\tto\tmake\trecommendations.\n\nIn\tparticular,\twe\u2019ll\tlook\tat\tthe\tdata\tset\tof\tusers_interests\tthat\twe\u2019ve\tused\tbefore:\n\nusers_interests\t=\t[\n\t\t\t\t[\"Hadoop\",\t\"Big\tData\",\t\"HBase\",\t\"Java\",\t\"Spark\",\t\"Storm\",\t\"Cassandra\"],\n\t\t\t\t[\"NoSQL\",\t\"MongoDB\",\t\"Cassandra\",\t\"HBase\",\t\"Postgres\"],\n\t\t\t\t[\"Python\",\t\"scikit-learn\",\t\"scipy\",\t\"numpy\",\t\"statsmodels\",\t\"pandas\"],\n\t\t\t\t[\"R\",\t\"Python\",\t\"statistics\",\t\"regression\",\t\"probability\"],\n\t\t\t\t[\"machine\tlearning\",\t\"regression\",\t\"decision\ttrees\",\t\"libsvm\"],\n\t\t\t\t[\"Python\",\t\"R\",\t\"Java\",\t\"C++\",\t\"Haskell\",\t\"programming\tlanguages\"],\n\t\t\t\t[\"statistics\",\t\"probability\",\t\"mathematics\",\t\"theory\"],\n\t\t\t\t[\"machine\tlearning\",\t\"scikit-learn\",\t\"Mahout\",\t\"neural\tnetworks\"],\n\t\t\t\t[\"neural\tnetworks\",\t\"deep\tlearning\",\t\"Big\tData\",\t\"artificial\tintelligence\"],\n\t\t\t\t[\"Hadoop\",\t\"Java\",\t\"MapReduce\",\t\"Big\tData\"],\n\t\t\t\t[\"statistics\",\t\"R\",\t\"statsmodels\"],\n\t\t\t\t[\"C++\",\t\"deep\tlearning\",\t\"artificial\tintelligence\",\t\"probability\"],\n\t\t\t\t[\"pandas\",\t\"R\",\t\"Python\"],\n\t\t\t\t[\"databases\",\t\"HBase\",\t\"Postgres\",\t\"MySQL\",\t\"MongoDB\"],\n\t\t\t\t[\"libsvm\",\t\"regression\",\t\"support\tvector\tmachines\"]\n]\n\nAnd\twe\u2019ll\tthink\tabout\tthe\tproblem\tof\trecommending\tnew\tinterests\tto\ta\tuser\tbased\ton\ther\ncurrently\tspecified\tinterests.",
    "366": "Manual\tCuration\n\nBefore\tthe\tInternet,\twhen\tyou\tneeded\tbook\trecommendations\tyou\twould\tgo\tto\tthe\tlibrary,\nwhere\ta\tlibrarian\twas\tavailable\tto\tsuggest\tbooks\tthat\twere\trelevant\tto\tyour\tinterests\tor\nsimilar\tto\tbooks\tyou\tliked.\n\nGiven\tDataSciencester\u2019s\tlimited\tnumber\tof\tusers\tand\tinterests,\tit\twould\tbe\teasy\tfor\tyou\tto\nspend\tan\tafternoon\tmanually\trecommending\tinterests\tfor\teach\tuser.\tBut\tthis\tmethod\ndoesn\u2019t\tscale\tparticularly\twell,\tand\tit\u2019s\tlimited\tby\tyour\tpersonal\tknowledge\tand\nimagination.\t(Not\tthat\tI\u2019m\tsuggesting\tthat\tyour\tpersonal\tknowledge\tand\timagination\tare\nlimited.)\tSo\tlet\u2019s\tthink\tabout\twhat\twe\tcan\tdo\twith\tdata.",
    "367": "Recommending\tWhat\u2019s\tPopular\n\nOne\teasy\tapproach\tis\tto\tsimply\trecommend\twhat\u2019s\tpopular:\n\npopular_interests\t=\tCounter(interest\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tuser_interests\tin\tusers_interests\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tinterest\tin\tuser_interests).most_common()\n\nwhich\tlooks\tlike:\n\n[('Python',\t4),\n\t('R',\t4),\n\t('Java',\t3),\n\t('regression',\t3),\n\t('statistics',\t3),\n\t('probability',\t3),\n\t#\t...\n]\n\nHaving\tcomputed\tthis,\twe\tcan\tjust\tsuggest\tto\ta\tuser\tthe\tmost\tpopular\tinterests\tthat\the\u2019s\nnot\talready\tinterested\tin:\n\ndef\tmost_popular_new_interests(user_interests,\tmax_results=5):\n\t\t\t\tsuggestions\t=\t[(interest,\tfrequency)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tinterest,\tfrequency\tin\tpopular_interests\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tinterest\tnot\tin\tuser_interests]\n\t\t\t\treturn\tsuggestions[:max_results]\n\nSo,\tif\tyou\tare\tuser\t1,\twith\tinterests:\n\n[\"NoSQL\",\t\"MongoDB\",\t\"Cassandra\",\t\"HBase\",\t\"Postgres\"]\n\nthen\twe\u2019d\trecommend\tyou:\n\nmost_popular_new_interests(users_interests[1],\t5)\n\n#\t[('Python',\t4),\t('R',\t4),\t('Java',\t3),\t('regression',\t3),\t('statistics',\t3)]\n\nIf\tyou\tare\tuser\t3,\twho\u2019s\talready\tinterested\tin\tmany\tof\tthose\tthings,\tyou\u2019d\tinstead\tget:\n\n[('Java',\t3),\n\t('HBase',\t3),\n\t('Big\tData',\t3),\n\t('neural\tnetworks',\t2),\n\t('Hadoop',\t2)]\n\nOf\tcourse,\t\u201clots\tof\tpeople\tare\tinterested\tin\tPython\tso\tmaybe\tyou\tshould\tbe\ttoo\u201d\tis\tnot\tthe\nmost\tcompelling\tsales\tpitch.\tIf\tsomeone\tis\tbrand\tnew\tto\tour\tsite\tand\twe\tdon\u2019t\tknow\nanything\tabout\tthem,\tthat\u2019s\tpossibly\tthe\tbest\twe\tcan\tdo.\tLet\u2019s\tsee\thow\twe\tcan\tdo\tbetter\tby\nbasing\teach\tuser\u2019s\trecommendations\ton\ther\tinterests.",
    "368": "User-Based\tCollaborative\tFiltering\n\nOne\tway\tof\ttaking\ta\tuser\u2019s\tinterests\tinto\taccount\tis\tto\tlook\tfor\tusers\twho\tare\tsomehow\nsimilar\tto\thim,\tand\tthen\tsuggest\tthe\tthings\tthat\tthose\tusers\tare\tinterested\tin.\n\nIn\torder\tto\tdo\tthat,\twe\u2019ll\tneed\ta\tway\tto\tmeasure\thow\tsimilar\ttwo\tusers\tare.\tHere\twe\u2019ll\tuse\na\tmetric\tcalled\tcosine\tsimilarity.\tGiven\ttwo\tvectors,\tv\tand\tw,\tit\u2019s\tdefined\tas:\n\ndef\tcosine_similarity(v,\tw):\n\t\t\t\treturn\tdot(v,\tw)\t/\tmath.sqrt(dot(v,\tv)\t*\tdot(w,\tw))\n\nIt\tmeasures\tthe\t\u201cangle\u201d\tbetween\tv\tand\tw.\tIf\tv\tand\tw\tpoint\tin\tthe\tsame\tdirection,\tthen\tthe\nnumerator\tand\tdenominator\tare\tequal,\tand\ttheir\tcosine\tsimilarity\tequals\t1.\tIf\tv\tand\tw\tpoint\nin\topposite\tdirections,\tthen\ttheir\tcosine\tsimilarity\tequals\t-1.\tAnd\tif\tv\tis\t0\twhenever\tw\tis\nnot\t(and\tvice\tversa)\tthen\tdot(v,\tw)\tis\t0\tand\tso\tthe\tcosine\tsimilarity\twill\tbe\t0.\n\nWe\u2019ll\tapply\tthis\tto\tvectors\tof\t0s\tand\t1s,\teach\tvector\tv\trepresenting\tone\tuser\u2019s\tinterests.\nv[i]\twill\tbe\t1\tif\tthe\tuser\tis\tspecified\tthe\tith\tinterest,\t0\totherwise.\tAccordingly,\t\u201csimilar\nusers\u201d\twill\tmean\t\u201cusers\twhose\tinterest\tvectors\tmost\tnearly\tpoint\tin\tthe\tsame\tdirection.\u201d\nUsers\twith\tidentical\tinterests\twill\thave\tsimilarity\t1.\tUsers\twith\tno\tidentical\tinterests\twill\nhave\tsimilarity\t0.\tOtherwise\tthe\tsimilarity\twill\tfall\tin\tbetween,\twith\tnumbers\tcloser\tto\t1\nindicating\t\u201cvery\tsimilar\u201d\tand\tnumbers\tcloser\tto\t0\tindicating\t\u201cnot\tvery\tsimilar.\u201d\n\nA\tgood\tplace\tto\tstart\tis\tcollecting\tthe\tknown\tinterests\tand\t(implicitly)\tassigning\tindices\tto\nthem.\tWe\tcan\tdo\tthis\tby\tusing\ta\tset\tcomprehension\tto\tfind\tthe\tunique\tinterests,\tputting\nthem\tin\ta\tlist,\tand\tthen\tsorting\tthem.\tThe\tfirst\tinterest\tin\tthe\tresulting\tlist\twill\tbe\tinterest\n0,\tand\tso\ton:\n\nunique_interests\t=\tsorted(list({\tinterest\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tuser_interests\tin\tusers_interests\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tinterest\tin\tuser_interests\t}))\n\nThis\tgives\tus\ta\tlist\tthat\tstarts:\n\n['Big\tData',\n\t'C++',\n\t'Cassandra',\n\t'HBase',\n\t'Hadoop',\n\t'Haskell',\n\t#\t...\n]\n\nNext\twe\twant\tto\tproduce\tan\t\u201cinterest\u201d\tvector\tof\t0s\tand\t1s\tfor\teach\tuser.\tWe\tjust\tneed\tto\niterate\tover\tthe\tunique_interests\tlist,\tsubstituting\ta\t1\tif\tthe\tuser\thas\teach\tinterest,\ta\t0\tif\nnot:\n\ndef\tmake_user_interest_vector(user_interests):\n\t\t\t\t\"\"\"given\ta\tlist\tof\tinterests,\tproduce\ta\tvector\twhose\tith\telement\tis\t1\n\t\t\t\tif\tunique_interests[i]\tis\tin\tthe\tlist,\t0\totherwise\"\"\"\n\t\t\t\treturn\t[1\tif\tinterest\tin\tuser_interests\telse\t0\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tinterest\tin\tunique_interests]",
    "369": "after\twhich,\twe\tcan\tcreate\ta\tmatrix\tof\tuser\tinterests\tsimply\tby\tmap-ping\tthis\tfunction\nagainst\tthe\tlist\tof\tlists\tof\tinterests:\n\nuser_interest_matrix\t=\tmap(make_user_interest_vector,\tusers_interests)\n\nNow\tuser_interest_matrix[i][j]\tequals\t1\tif\tuser\ti\tspecified\tinterest\tj,\t0\totherwise.\n\nBecause\twe\thave\ta\tsmall\tdata\tset,\tit\u2019s\tno\tproblem\tto\tcompute\tthe\tpairwise\tsimilarities\nbetween\tall\tof\tour\tusers:\n\nuser_similarities\t=\t[[cosine_similarity(interest_vector_i,\tinterest_vector_j)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tinterest_vector_j\tin\tuser_interest_matrix]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tinterest_vector_i\tin\tuser_interest_matrix]\n\nafter\twhich,\tuser_similarities[i][j]\tgives\tus\tthe\tsimilarity\tbetween\tusers\ti\tand\tj.\n\nFor\tinstance,\tuser_similarities[0][9]\tis\t0.57,\tas\tthose\ttwo\tusers\tshare\tinterests\tin\nHadoop,\tJava,\tand\tBig\tData.\tOn\tthe\tother\thand,\tuser_similarities[0][8]\tis\tonly\t0.19,\nas\tusers\t0\tand\t8\tshare\tonly\tone\tinterest,\tBig\tData.\n\nIn\tparticular,\tuser_similarities[i]\tis\tthe\tvector\tof\tuser\ti\u2019s\tsimilarities\tto\tevery\tother\nuser.\tWe\tcan\tuse\tthis\tto\twrite\ta\tfunction\tthat\tfinds\tthe\tmost\tsimilar\tusers\tto\ta\tgiven\tuser.\nWe\u2019ll\tmake\tsure\tnot\tto\tinclude\tthe\tuser\therself,\tnor\tany\tusers\twith\tzero\tsimilarity.\tAnd\nwe\u2019ll\tsort\tthe\tresults\tfrom\tmost\tsimilar\tto\tleast\tsimilar:\n\ndef\tmost_similar_users_to(user_id):\n\t\t\t\tpairs\t=\t[(other_user_id,\tsimilarity)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tfind\tother\n\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tother_user_id,\tsimilarity\tin\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tusers\twith\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tenumerate(user_similarities[user_id])\t\t\t\t\t\t\t\t\t#\tnonzero\n\t\t\t\t\t\t\t\t\t\t\t\t\tif\tuser_id\t!=\tother_user_id\tand\tsimilarity\t>\t0]\t\t#\tsimilarity\n\n\t\t\t\treturn\tsorted(pairs,\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tsort\tthem\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkey=lambda\t(_,\tsimilarity):\tsimilarity,\t\t\t\t\t#\tmost\tsimilar\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treverse=True)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tfirst\n\nFor\tinstance,\tif\twe\tcall\tmost_similar_users_to(0)\twe\tget:\n\n[(9,\t0.5669467095138409),\n\t(1,\t0.3380617018914066),\n\t(8,\t0.1889822365046136),\n\t(13,\t0.1690308509457033),\n\t(5,\t0.1543033499620919)]\n\nHow\tdo\twe\tuse\tthis\tto\tsuggest\tnew\tinterests\tto\ta\tuser?\tFor\teach\tinterest,\twe\tcan\tjust\tadd\nup\tthe\tuser-similarities\tof\tthe\tother\tusers\tinterested\tin\tit:\n\ndef\tuser_based_suggestions(user_id,\tinclude_current_interests=False):\n\t\t\t\t#\tsum\tup\tthe\tsimilarities\n\t\t\t\tsuggestions\t=\tdefaultdict(float)\n\t\t\t\tfor\tother_user_id,\tsimilarity\tin\tmost_similar_users_to(user_id):\n\t\t\t\t\t\t\t\tfor\tinterest\tin\tusers_interests[other_user_id]:\n\t\t\t\t\t\t\t\t\t\t\t\tsuggestions[interest]\t+=\tsimilarity\n\n\t\t\t\t#\tconvert\tthem\tto\ta\tsorted\tlist\n\t\t\t\tsuggestions\t=\tsorted(suggestions.items(),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkey=lambda\t(_,\tweight):\tweight,",
    "370": "reverse=True)\n\n\t\t\t\t#\tand\t(maybe)\texclude\talready-interests\n\t\t\t\tif\tinclude_current_interests:\n\t\t\t\t\t\t\t\treturn\tsuggestions\n\t\t\t\telse:\n\t\t\t\t\t\t\t\treturn\t[(suggestion,\tweight)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tsuggestion,\tweight\tin\tsuggestions\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tsuggestion\tnot\tin\tusers_interests[user_id]]\n\nIf\twe\tcall\tuser_based_suggestions(0),\tthe\tfirst\tseveral\tsuggested\tinterests\tare:\n\n[('MapReduce',\t0.5669467095138409),\n\t('MongoDB',\t0.50709255283711),\n\t('Postgres',\t0.50709255283711),\n\t('NoSQL',\t0.3380617018914066),\n\t('neural\tnetworks',\t0.1889822365046136),\n\t('deep\tlearning',\t0.1889822365046136),\n\t('artificial\tintelligence',\t0.1889822365046136),\n\t#...\n]\n\nThese\tseem\tlike\tpretty\tdecent\tsuggestions\tfor\tsomeone\twhose\tstated\tinterests\tare\t\u201cBig\nData\u201d\tand\tdatabase-related.\t(The\tweights\taren\u2019t\tintrinsically\tmeaningful;\twe\tjust\tuse\tthem\nfor\tordering.)\n\nThis\tapproach\tdoesn\u2019t\twork\tas\twell\twhen\tthe\tnumber\tof\titems\tgets\tvery\tlarge.\tRecall\tthe\ncurse\tof\tdimensionality\tfrom\tChapter\t12\t\u2014\tin\tlarge-dimensional\tvector\tspaces\tmost\nvectors\tare\tvery\tfar\tapart\t(and\ttherefore\tpoint\tin\tvery\tdifferent\tdirections).\tThat\tis,\twhen\nthere\tare\ta\tlarge\tnumber\tof\tinterests\tthe\t\u201cmost\tsimilar\tusers\u201d\tto\ta\tgiven\tuser\tmight\tnot\tbe\nsimilar\tat\tall.\n\nImagine\ta\tsite\tlike\tAmazon.com,\tfrom\twhich\tI\u2019ve\tbought\tthousands\tof\titems\tover\tthe\tlast\ncouple\tof\tdecades.\tYou\tcould\tattempt\tto\tidentify\tsimilar\tusers\tto\tme\tbased\ton\tbuying\npatterns,\tbut\tmost\tlikely\tin\tall\tthe\tworld\tthere\u2019s\tno\tone\twhose\tpurchase\thistory\tlooks\teven\nremotely\tlike\tmine.\tWhoever\tmy\t\u201cmost\tsimilar\u201d\tshopper\tis,\the\u2019s\tprobably\tnot\tsimilar\tto\nme\tat\tall,\tand\this\tpurchases\twould\talmost\tcertainly\tmake\tfor\tlousy\trecommendations.",
    "371": "Item-Based\tCollaborative\tFiltering\n\nAn\talternative\tapproach\tis\tto\tcompute\tsimilarities\tbetween\tinterests\tdirectly.\tWe\tcan\tthen\ngenerate\tsuggestions\tfor\teach\tuser\tby\taggregating\tinterests\tthat\tare\tsimilar\tto\ther\tcurrent\ninterests.\n\nTo\tstart\twith,\twe\u2019ll\twant\tto\ttranspose\tour\tuser-interest\tmatrix\tso\tthat\trows\tcorrespond\tto\ninterests\tand\tcolumns\tcorrespond\tto\tusers:\n\ninterest_user_matrix\t=\t[[user_interest_vector[j]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tuser_interest_vector\tin\tuser_interest_matrix]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tj,\t_\tin\tenumerate(unique_interests)]\n\nWhat\tdoes\tthis\tlook\tlike?\tRow\tj\tof\tinterest_user_matrix\tis\tcolumn\tj\tof\nuser_interest_matrix.\tThat\tis,\tit\thas\t1\tfor\teach\tuser\twith\tthat\tinterest\tand\t0\tfor\teach\nuser\twithout\tthat\tinterest.\n\nFor\texample,\tunique_interests[0]\tis\tBig\tData,\tand\tso\tinterest_user_matrix[0]\tis:\n\n[1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t0]\n\nbecause\tusers\t0,\t8,\tand\t9\tindicated\tinterest\tin\tBig\tData.\n\nWe\tcan\tnow\tuse\tcosine\tsimilarity\tagain.\tIf\tprecisely\tthe\tsame\tusers\tare\tinterested\tin\ttwo\ntopics,\ttheir\tsimilarity\twill\tbe\t1.\tIf\tno\ttwo\tusers\tare\tinterested\tin\tboth\ttopics,\ttheir\nsimilarity\twill\tbe\t0:\n\ninterest_similarities\t=\t[[cosine_similarity(user_vector_i,\tuser_vector_j)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tuser_vector_j\tin\tinterest_user_matrix]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tuser_vector_i\tin\tinterest_user_matrix]\n\nFor\texample,\twe\tcan\tfind\tthe\tinterests\tmost\tsimilar\tto\tBig\tData\t(interest\t0)\tusing:\n\ndef\tmost_similar_interests_to(interest_id):\n\t\t\t\tsimilarities\t=\tinterest_similarities[interest_id]\n\t\t\t\tpairs\t=\t[(unique_interests[other_interest_id],\tsimilarity)\n\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tother_interest_id,\tsimilarity\tin\tenumerate(similarities)\n\t\t\t\t\t\t\t\t\t\t\t\t\tif\tinterest_id\t!=\tother_interest_id\tand\tsimilarity\t>\t0]\n\t\t\t\treturn\tsorted(pairs,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkey=lambda\t(_,\tsimilarity):\tsimilarity,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treverse=True)\n\nwhich\tsuggests\tthe\tfollowing\tsimilar\tinterests:\n\n[('Hadoop',\t0.8164965809277261),\n\t('Java',\t0.6666666666666666),\n\t('MapReduce',\t0.5773502691896258),\n\t('Spark',\t0.5773502691896258),\n\t('Storm',\t0.5773502691896258),\n\t('Cassandra',\t0.4082482904638631),\n\t('artificial\tintelligence',\t0.4082482904638631),\n\t('deep\tlearning',\t0.4082482904638631),\n\t('neural\tnetworks',\t0.4082482904638631),\n\t('HBase',\t0.3333333333333333)]",
    "372": "Now\twe\tcan\tcreate\trecommendations\tfor\ta\tuser\tby\tsumming\tup\tthe\tsimilarities\tof\tthe\ninterests\tsimilar\tto\this:\n\ndef\titem_based_suggestions(user_id,\tinclude_current_interests=False):\n\t\t\t\t#\tadd\tup\tthe\tsimilar\tinterests\n\t\t\t\tsuggestions\t=\tdefaultdict(float)\n\t\t\t\tuser_interest_vector\t=\tuser_interest_matrix[user_id]\n\t\t\t\tfor\tinterest_id,\tis_interested\tin\tenumerate(user_interest_vector):\n\t\t\t\t\t\t\t\tif\tis_interested\t==\t1:\n\t\t\t\t\t\t\t\t\t\t\t\tsimilar_interests\t=\tmost_similar_interests_to(interest_id)\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tinterest,\tsimilarity\tin\tsimilar_interests:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsuggestions[interest]\t+=\tsimilarity\n\n\t\t\t\t#\tsort\tthem\tby\tweight\n\t\t\t\tsuggestions\t=\tsorted(suggestions.items(),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkey=lambda\t(_,\tsimilarity):\tsimilarity,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treverse=True)\n\n\t\t\t\tif\tinclude_current_interests:\n\t\t\t\t\t\t\t\treturn\tsuggestions\n\t\t\t\telse:\n\t\t\t\t\t\t\t\treturn\t[(suggestion,\tweight)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tsuggestion,\tweight\tin\tsuggestions\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tsuggestion\tnot\tin\tusers_interests[user_id]]\n\nFor\tuser\t0,\tthis\tgenerates\tthe\tfollowing\t(seemingly\treasonable)\trecommendations:\n\n[('MapReduce',\t1.861807319565799),\n\t('Postgres',\t1.3164965809277263),\n\t('MongoDB',\t1.3164965809277263),\n\t('NoSQL',\t1.2844570503761732),\n\t('programming\tlanguages',\t0.5773502691896258),\n\t('MySQL',\t0.5773502691896258),\n\t('Haskell',\t0.5773502691896258),\n\t('databases',\t0.5773502691896258),\n\t('neural\tnetworks',\t0.4082482904638631),\n\t('deep\tlearning',\t0.4082482904638631),\n\t('C++',\t0.4082482904638631),\n\t('artificial\tintelligence',\t0.4082482904638631),\n\t('Python',\t0.2886751345948129),\n\t('R',\t0.2886751345948129)]",
    "373": "For\tFurther\tExploration\n\nCrab\tis\ta\tframework\tfor\tbuilding\trecommender\tsystems\tin\tPython.\n\nGraphlab\talso\thas\ta\trecommender\ttoolkit.\n\nThe\tNetflix\tPrize\twas\ta\tsomewhat\tfamous\tcompetition\tto\tbuild\ta\tbetter\tsystem\tto\nrecommend\tmovies\tto\tNetflix\tusers.",
    "374": "",
    "375": "Chapter\t23.\tDatabases\tand\tSQL\n\nMemory\tis\tman\u2019s\tgreatest\tfriend\tand\tworst\tenemy.\n\nGilbert\tParker\n\nThe\tdata\tyou\tneed\twill\toften\tlive\tin\tdatabases,\tsystems\tdesigned\tfor\tefficiently\tstoring\nand\tquerying\tdata.\tThe\tbulk\tof\tthese\tare\trelational\tdatabases,\tsuch\tas\tOracle,\tMySQL,\nand\tSQL\tServer,\twhich\tstore\tdata\tin\ttables\tand\tare\ttypically\tqueried\tusing\tStructured\nQuery\tLanguage\t(SQL),\ta\tdeclarative\tlanguage\tfor\tmanipulating\tdata.\n\nSQL\tis\ta\tpretty\tessential\tpart\tof\tthe\tdata\tscientist\u2019s\ttoolkit.\tIn\tthis\tchapter,\twe\u2019ll\tcreate\nNotQuiteABase,\ta\tPython\timplementation\tof\tsomething\tthat\u2019s\tnot\tquite\ta\tdatabase.\tWe\u2019ll\nalso\tcover\tthe\tbasics\tof\tSQL\twhile\tshowing\thow\tthey\twork\tin\tour\tnot-quite\tdatabase,\nwhich\tis\tthe\tmost\t\u201cfrom\tscratch\u201d\tway\tI\tcould\tthink\tof\tto\thelp\tyou\tunderstand\twhat\tthey\u2019re\ndoing.\tMy\thope\tis\tthat\tsolving\tproblems\tin\tNotQuiteABase\twill\tgive\tyou\ta\tgood\tsense\tof\nhow\tyou\tmight\tsolve\tthe\tsame\tproblems\tusing\tSQL.",
    "376": "CREATE\tTABLE\tand\tINSERT\n\nA\trelational\tdatabase\tis\ta\tcollection\tof\ttables\t(and\tof\trelationships\tamong\tthem).\tA\ttable\tis\nsimply\ta\tcollection\tof\trows,\tnot\tunlike\tthe\tmatrices\twe\u2019ve\tbeen\tworking\twith.\tHowever,\ta\ntable\talso\thas\tassociated\twith\tit\ta\tfixed\tschema\tconsisting\tof\tcolumn\tnames\tand\tcolumn\ntypes.\n\nFor\texample,\timagine\ta\tusers\tdata\tset\tcontaining\tfor\teach\tuser\ther\tuser_id,\tname,\tand\nnum_friends:\n\nusers\t=\t[[0,\t\"Hero\",\t0],\n\t\t\t\t\t\t\t\t\t[1,\t\"Dunn\",\t2],\n\t\t\t\t\t\t\t\t\t[2,\t\"Sue\",\t3],\n\t\t\t\t\t\t\t\t\t[3,\t\"Chi\",\t3]]\n\nIn\tSQL,\twe\tmight\tcreate\tthis\ttable\twith:\n\nCREATE\tTABLE\tusers\t(\n\t\t\t\tuser_id\tINT\tNOT\tNULL,\n\t\t\t\tname\tVARCHAR(200),\n\t\t\t\tnum_friends\tINT);\n\nNotice\tthat\twe\tspecified\tthat\tthe\tuser_id\tand\tnum_friends\tmust\tbe\tintegers\t(and\tthat\nuser_id\tisn\u2019t\tallowed\tto\tbe\tNULL,\twhich\tindicates\ta\tmissing\tvalue\tand\tis\tsort\tof\tlike\tour\nNone)\tand\tthat\tthe\tname\tshould\tbe\ta\tstring\tof\tlength\t200\tor\tless.\tNotQuiteABase\twon\u2019t\ntake\ttypes\tinto\taccount,\tbut\twe\u2019ll\tbehave\tas\tif\tit\tdid.\n\nSQL\tis\talmost\tcompletely\tcase\tand\tindentation\tinsensitive.\tThe\tcapitalization\tand\tindentation\tstyle\there\tis\nmy\tpreferred\tstyle.\tIf\tyou\tstart\tlearning\tSQL,\tyou\twill\tsurely\tencounter\tother\texamples\tstyled\tdifferently.\n\nNOTE\n\nYou\tcan\tinsert\tthe\trows\twith\tINSERT\tstatements:\n\nINSERT\tINTO\tusers\t(user_id,\tname,\tnum_friends)\tVALUES\t(0,\t'Hero',\t0);\n\nNotice\talso\tthat\tSQL\tstatements\tneed\tto\tend\twith\tsemicolons,\tand\tthat\tSQL\trequires\nsingle\tquotes\tfor\tits\tstrings.\n\nIn\tNotQuiteABase,\tyou\u2019ll\tcreate\ta\tTable\tsimply\tby\tspecifying\tthe\tnames\tof\tits\tcolumns.\nAnd\tto\tinsert\ta\trow,\tyou\u2019ll\tuse\tthe\ttable\u2019s\tinsert()\tmethod,\twhich\ttakes\ta\tlist\tof\trow\nvalues\tthat\tneed\tto\tbe\tin\tthe\tsame\torder\tas\tthe\ttable\u2019s\tcolumn\tnames.\n\nBehind\tthe\tscenes,\twe\u2019ll\tstore\teach\trow\tas\ta\tdict\tfrom\tcolumn\tnames\tto\tvalues.\tA\treal\ndatabase\twould\tnever\tuse\tsuch\ta\tspace-wasting\trepresentation,\tbut\tdoing\tso\twill\tmake\nNotQuiteABase\tmuch\teasier\tto\twork\twith:\n\nclass\tTable:\n\t\t\t\tdef\t__init__(self,\tcolumns):\n\t\t\t\t\t\t\t\tself.columns\t=\tcolumns\n\t\t\t\t\t\t\t\tself.rows\t=\t[]",
    "377": "def\t__repr__(self):\n\t\t\t\t\t\t\t\t\"\"\"pretty\trepresentation\tof\tthe\ttable:\tcolumns\tthen\trows\"\"\"\n\t\t\t\t\t\t\t\treturn\tstr(self.columns)\t+\t\"\\n\"\t+\t\"\\n\".join(map(str,\tself.rows))\n\n\t\t\t\tdef\tinsert(self,\trow_values):\n\t\t\t\t\t\t\t\tif\tlen(row_values)\t!=\tlen(self.columns):\n\t\t\t\t\t\t\t\t\t\t\t\traise\tTypeError(\"wrong\tnumber\tof\telements\")\n\t\t\t\t\t\t\t\trow_dict\t=\tdict(zip(self.columns,\trow_values))\n\t\t\t\t\t\t\t\tself.rows.append(row_dict)\n\nFor\texample,\twe\tcould\tset\tup:\n\nusers\t=\tTable([\"user_id\",\t\"name\",\t\"num_friends\"])\nusers.insert([0,\t\"Hero\",\t0])\nusers.insert([1,\t\"Dunn\",\t2])\nusers.insert([2,\t\"Sue\",\t3])\nusers.insert([3,\t\"Chi\",\t3])\nusers.insert([4,\t\"Thor\",\t3])\nusers.insert([5,\t\"Clive\",\t2])\nusers.insert([6,\t\"Hicks\",\t3])\nusers.insert([7,\t\"Devin\",\t2])\nusers.insert([8,\t\"Kate\",\t2])\nusers.insert([9,\t\"Klein\",\t3])\nusers.insert([10,\t\"Jen\",\t1])\n\nIf\tyou\tnow\tprint\tusers,\tyou\u2019ll\tsee:\n\n['user_id',\t'name',\t'num_friends']\n{'user_id':\t0,\t'name':\t'Hero',\t'num_friends':\t0}\n{'user_id':\t1,\t'name':\t'Dunn',\t'num_friends':\t2}\n{'user_id':\t2,\t'name':\t'Sue',\t'num_friends':\t3}\n...",
    "378": "UPDATE\n\nSometimes\tyou\tneed\tto\tupdate\tthe\tdata\tthat\u2019s\talready\tin\tthe\tdatabase.\tFor\tinstance,\tif\nDunn\tacquires\tanother\tfriend,\tyou\tmight\tneed\tto\tdo\tthis:\n\nUPDATE\tusers\nSET\tnum_friends\t=\t3\nWHERE\tuser_id\t=\t1;\n\nThe\tkey\tfeatures\tare:\n\nWhat\ttable\tto\tupdate\n\nWhich\trows\tto\tupdate\n\nWhich\tfields\tto\tupdate\n\nWhat\ttheir\tnew\tvalues\tshould\tbe\n\nWe\u2019ll\tadd\ta\tsimilar\tupdate\tmethod\tto\tNotQuiteABase.\tIts\tfirst\targument\twill\tbe\ta\tdict\nwhose\tkeys\tare\tthe\tcolumns\tto\tupdate\tand\twhose\tvalues\tare\tthe\tnew\tvalues\tfor\tthose\nfields.\tAnd\tits\tsecond\targument\tis\ta\tpredicate\tthat\treturns\tTrue\tfor\trows\tthat\tshould\tbe\nupdated,\tFalse\totherwise:\n\n\t\t\t\tdef\tupdate(self,\tupdates,\tpredicate):\n\t\t\t\t\t\t\t\tfor\trow\tin\tself.rows:\n\t\t\t\t\t\t\t\t\t\t\t\tif\tpredicate(row):\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tcolumn,\tnew_value\tin\tupdates.iteritems():\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trow[column]\t=\tnew_value\n\nafter\twhich\twe\tcan\tsimply\tdo\tthis:\n\nusers.update({'num_friends'\t:\t3},\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tset\tnum_friends\t=\t3\n\t\t\t\t\t\t\t\t\t\t\t\t\tlambda\trow:\trow['user_id']\t==\t1)\t\t\t#\tin\trows\twhere\tuser_id\t==\t1",
    "379": "DELETE\n\nThere\tare\ttwo\tways\tto\tdelete\trows\tfrom\ta\ttable\tin\tSQL.\tThe\tdangerous\tway\tdeletes\tevery\nrow\tfrom\ta\ttable:\n\nDELETE\tFROM\tusers;\n\nThe\tless\tdangerous\tway\tadds\ta\tWHERE\tclause\tand\tonly\tdeletes\trows\tthat\tmatch\ta\tcertain\ncondition:\n\nDELETE\tFROM\tusers\tWHERE\tuser_id\t=\t1;\n\nIt\u2019s\teasy\tto\tadd\tthis\tfunctionality\tto\tour\tTable:\n\n\t\t\t\tdef\tdelete(self,\tpredicate=lambda\trow:\tTrue):\n\t\t\t\t\t\t\t\t\"\"\"delete\tall\trows\tmatching\tpredicate\n\t\t\t\t\t\t\t\tor\tall\trows\tif\tno\tpredicate\tsupplied\"\"\"\n\t\t\t\t\t\t\t\tself.rows\t=\t[row\tfor\trow\tin\tself.rows\tif\tnot(predicate(row))]\n\nIf\tyou\tsupply\ta\tpredicate\tfunction\t(i.e.,\ta\tWHERE\tclause),\tthis\tdeletes\tonly\tthe\trows\tthat\nsatisfy\tit.\tIf\tyou\tdon\u2019t\tsupply\tone,\tthe\tdefault\tpredicate\talways\treturns\tTrue,\tand\tyou\twill\ndelete\tevery\trow.\n\nFor\texample:\n\nusers.delete(lambda\trow:\trow[\"user_id\"]\t==\t1)\t\t#\tdeletes\trows\twith\tuser_id\t==\t1\nusers.delete()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tdeletes\tevery\trow",
    "380": "SELECT\n\nTypically\tyou\tdon\u2019t\tinspect\tSQL\ttables\tdirectly.\tInstead\tyou\tquery\tthem\twith\ta\tSELECT\nstatement:\n\nSELECT\t*\tFROM\tusers;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t--\tget\tthe\tentire\tcontents\nSELECT\t*\tFROM\tusers\tLIMIT\t2;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t--\tget\tthe\tfirst\ttwo\trows\nSELECT\tuser_id\tFROM\tusers;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t--\tonly\tget\tspecific\tcolumns\nSELECT\tuser_id\tFROM\tusers\tWHERE\tname\t=\t'Dunn';\t\t--\tonly\tget\tspecific\trows\n\nYou\tcan\talso\tuse\tSELECT\tstatements\tto\tcalculate\tfields:\n\nSELECT\tLENGTH(name)\tAS\tname_length\tFROM\tusers;\n\nWe\u2019ll\tgive\tour\tTable\tclass\ta\tselect()\tmethod\tthat\treturns\ta\tnew\tTable.\tThe\tmethod\naccepts\ttwo\toptional\targuments:\n\nkeep_columns\tspecifies\tthe\tname\tof\tthe\tcolumns\tyou\twant\tto\tkeep\tin\tthe\tresult.\tIf\tyou\ndon\u2019t\tsupply\tit,\tthe\tresult\tcontains\tall\tthe\tcolumns.\n\nadditional_columns\tis\ta\tdictionary\twhose\tkeys\tare\tnew\tcolumn\tnames\tand\twhose\nvalues\tare\tfunctions\tspecifying\thow\tto\tcompute\tthe\tvalues\tof\tthe\tnew\tcolumns.\n\nIf\tyou\twere\tto\tsupply\tneither\tof\tthem,\tyou\u2019d\tsimply\tget\tback\ta\tcopy\tof\tthe\ttable:\n\n\t\t\t\tdef\tselect(self,\tkeep_columns=None,\tadditional_columns=None):\n\n\t\t\t\t\t\t\t\tif\tkeep_columns\tis\tNone:\t\t\t\t\t\t\t\t\t#\tif\tno\tcolumns\tspecified,\n\t\t\t\t\t\t\t\t\t\t\t\tkeep_columns\t=\tself.columns\t\t#\treturn\tall\tcolumns\n\n\t\t\t\t\t\t\t\tif\tadditional_columns\tis\tNone:\n\t\t\t\t\t\t\t\t\t\t\t\tadditional_columns\t=\t{}\n\n\t\t\t\t\t\t\t\t#\tnew\ttable\tfor\tresults\n\t\t\t\t\t\t\t\tresult_table\t=\tTable(keep_columns\t+\tadditional_columns.keys())\n\n\t\t\t\t\t\t\t\tfor\trow\tin\tself.rows:\n\t\t\t\t\t\t\t\t\t\t\t\tnew_row\t=\t[row[column]\tfor\tcolumn\tin\tkeep_columns]\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tcolumn_name,\tcalculation\tin\tadditional_columns.iteritems():\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnew_row.append(calculation(row))\n\t\t\t\t\t\t\t\t\t\t\t\tresult_table.insert(new_row)\n\n\t\t\t\t\t\t\t\treturn\tresult_table\n\nOur\tselect()\treturns\ta\tnew\tTable,\twhile\tthe\ttypical\tSQL\tSELECT\tjust\tproduces\tsome\tsort\nof\ttransient\tresult\tset\t(unless\tyou\texplicitly\tinsert\tthe\tresults\tinto\ta\ttable).\n\nWe\u2019ll\talso\tneed\twhere()\tand\tlimit()\tmethods.\tBoth\tare\tpretty\tsimple:\n\n\t\t\t\tdef\twhere(self,\tpredicate=lambda\trow:\tTrue):\n\t\t\t\t\t\t\t\t\"\"\"return\tonly\tthe\trows\tthat\tsatisfy\tthe\tsupplied\tpredicate\"\"\"\n\t\t\t\t\t\t\t\twhere_table\t=\tTable(self.columns)\n\t\t\t\t\t\t\t\twhere_table.rows\t=\tfilter(predicate,\tself.rows)\n\t\t\t\t\t\t\t\treturn\twhere_table\n\n\t\t\t\tdef\tlimit(self,\tnum_rows):\n\t\t\t\t\t\t\t\t\"\"\"return\tonly\tthe\tfirst\tnum_rows\trows\"\"\"\n\t\t\t\t\t\t\t\tlimit_table\t=\tTable(self.columns)\n\t\t\t\t\t\t\t\tlimit_table.rows\t=\tself.rows[:num_rows]",
    "381": "return\tlimit_table\n\nafter\twhich\twe\tcan\teasily\tconstruct\tNotQuiteABase\tequivalents\tto\tthe\tpreceding\tSQL\nstatements:\n\n#\tSELECT\t*\tFROM\tusers;\nusers.select()\n\n#\tSELECT\t*\tFROM\tusers\tLIMIT\t2;\nusers.limit(2)\n\n#\tSELECT\tuser_id\tFROM\tusers;\nusers.select(keep_columns=[\"user_id\"])\n\n#\tSELECT\tuser_id\tFROM\tusers\tWHERE\tname\t=\t'Dunn';\nusers.where(lambda\trow:\trow[\"name\"]\t==\t\"Dunn\")\t\\\n\t\t\t\t\t.select(keep_columns=[\"user_id\"])\n\n#\tSELECT\tLENGTH(name)\tAS\tname_length\tFROM\tusers;\ndef\tname_length(row):\treturn\tlen(row[\"name\"])\n\nusers.select(keep_columns=[],\n\t\t\t\t\t\t\t\t\t\t\t\t\tadditional_columns\t=\t{\t\"name_length\"\t:\tname_length\t})\n\nNotice\tthat\t\u2014\tunlike\tin\tthe\trest\tof\tthe\tbook\t\u2014\there\tI\tuse\tbackslash\t\\\tto\tcontinue\nstatements\tacross\tmultiple\tlines.\tI\tfind\tit\tmakes\tthe\tchained-together\tNotQuiteABase\nqueries\tmore\treadable\tthan\tany\tother\tway\tof\twriting\tthem.",
    "382": "GROUP\tBY\n\nAnother\tcommon\tSQL\toperation\tis\tGROUP\tBY,\twhich\tgroups\ttogether\trows\twith\tidentical\nvalues\tin\tspecified\tcolumns\tand\tproduces\taggregate\tvalues\tlike\tMIN\tand\tMAX\tand\tCOUNT\nand\tSUM.\t(This\tshould\tremind\tyou\tof\tthe\tgroup_by\tfunction\tfrom\t\u201cManipulating\tData\u201d.)\n\nFor\texample,\tyou\tmight\twant\tto\tfind\tthe\tnumber\tof\tusers\tand\tthe\tsmallest\tuser_id\tfor\neach\tpossible\tname\tlength:\n\nSELECT\tLENGTH(name)\tas\tname_length,\n\tMIN(user_id)\tAS\tmin_user_id,\n\tCOUNT(*)\tAS\tnum_users\nFROM\tusers\nGROUP\tBY\tLENGTH(name);\n\nEvery\tfield\twe\tSELECT\tneeds\tto\tbe\teither\tin\tthe\tGROUP\tBY\tclause\t(which\tname_length\tis)\nor\tan\taggregate\tcomputation\t(which\tmin_user_id\tand\tnum_users\tare).\n\nSQL\talso\tsupports\ta\tHAVING\tclause\tthat\tbehaves\tsimilarly\tto\ta\tWHERE\tclause\texcept\tthat\tits\nfilter\tis\tapplied\tto\tthe\taggregates\t(whereas\ta\tWHERE\twould\tfilter\tout\trows\tbefore\naggregation\teven\ttook\tplace).\n\nYou\tmight\twant\tto\tknow\tthe\taverage\tnumber\tof\tfriends\tfor\tusers\twhose\tnames\tstart\twith\nspecific\tletters\tbut\tonly\tsee\tthe\tresults\tfor\tletters\twhose\tcorresponding\taverage\tis\tgreater\nthan\t1.\t(Yes,\tsome\tof\tthese\texamples\tare\tcontrived.)\n\nSELECT\tSUBSTR(name,\t1,\t1)\tAS\tfirst_letter,\n\tAVG(num_friends)\tAS\tavg_num_friends\nFROM\tusers\nGROUP\tBY\tSUBSTR(name,\t1,\t1)\nHAVING\tAVG(num_friends)\t>\t1;\n\n(Functions\tfor\tworking\twith\tstrings\tvary\tacross\tSQL\timplementations;\tsome\tdatabases\nmight\tinstead\tuse\tSUBSTRING\tor\tsomething\telse.)\n\nYou\tcan\talso\tcompute\toverall\taggregates.\tIn\tthat\tcase,\tyou\tleave\toff\tthe\tGROUP\tBY:\n\nSELECT\tSUM(user_id)\tas\tuser_id_sum\nFROM\tusers\nWHERE\tuser_id\t>\t1;\n\nTo\tadd\tthis\tfunctionality\tto\tNotQuiteABase\tTables,\twe\u2019ll\tadd\ta\tgroup_by()\tmethod.\tIt\ntakes\tthe\tnames\tof\tthe\tcolumns\tyou\twant\tto\tgroup\tby,\ta\tdictionary\tof\tthe\taggregation\nfunctions\tyou\twant\tto\trun\tover\teach\tgroup,\tand\tan\toptional\tpredicate\thaving\tthat\toperates\non\tmultiple\trows.\n\nThen\tit\tdoes\tthe\tfollowing\tsteps:\n\n1.\t Creates\ta\tdefaultdict\tto\tmap\ttuples\t(of\tthe\tgroup-by-values)\tto\trows\t(containing\nthe\tgroup-by-values).\tRecall\tthat\tyou\tcan\u2019t\tuse\tlists\tas\tdict\tkeys;\tyou\thave\tto\tuse\ntuples.",
    "383": "2.\t Iterates\tover\tthe\trows\tof\tthe\ttable,\tpopulating\tthe\tdefaultdict.\n\n3.\t Creates\ta\tnew\ttable\twith\tthe\tcorrect\toutput\tcolumns.\n\n4.\t Iterates\tover\tthe\tdefaultdict\tand\tpopulates\tthe\toutput\ttable,\tapplying\tthe\thaving\n\nfilter\tif\tany.\n\n(An\tactual\tdatabase\twould\talmost\tcertainly\tdo\tthis\tin\ta\tmore\tefficient\tmanner.)\n\n\t\t\t\tdef\tgroup_by(self,\tgroup_by_columns,\taggregates,\thaving=None):\n\n\t\t\t\t\t\t\t\tgrouped_rows\t=\tdefaultdict(list)\n\n\t\t\t\t\t\t\t\t#\tpopulate\tgroups\n\t\t\t\t\t\t\t\tfor\trow\tin\tself.rows:\n\t\t\t\t\t\t\t\t\t\t\t\tkey\t=\ttuple(row[column]\tfor\tcolumn\tin\tgroup_by_columns)\n\t\t\t\t\t\t\t\t\t\t\t\tgrouped_rows[key].append(row)\n\n\t\t\t\t\t\t\t\t#\tresult\ttable\tconsists\tof\tgroup_by\tcolumns\tand\taggregates\n\t\t\t\t\t\t\t\tresult_table\t=\tTable(group_by_columns\t+\taggregates.keys())\n\n\t\t\t\t\t\t\t\tfor\tkey,\trows\tin\tgrouped_rows.iteritems():\n\t\t\t\t\t\t\t\t\t\t\t\tif\thaving\tis\tNone\tor\thaving(rows):\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnew_row\t=\tlist(key)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\taggregate_name,\taggregate_fn\tin\taggregates.iteritems():\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnew_row.append(aggregate_fn(rows))\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tresult_table.insert(new_row)\n\n\t\t\t\t\t\t\t\treturn\tresult_table\n\nAgain,\tlet\u2019s\tsee\thow\twe\twould\tdo\tthe\tequivalent\tof\tthe\tpreceding\tSQL\tstatements.\tThe\nname_length\tmetrics\tare:\n\ndef\tmin_user_id(rows):\treturn\tmin(row[\"user_id\"]\tfor\trow\tin\trows)\n\nstats_by_length\t=\tusers\t\\\n\t\t\t\t.select(additional_columns={\"name_length\"\t:\tname_length})\t\\\n\t\t\t\t.group_by(group_by_columns=[\"name_length\"],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\taggregates={\t\"min_user_id\"\t:\tmin_user_id,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"num_users\"\t:\tlen\t})\n\nThe\tfirst_letter\tmetrics:\n\ndef\tfirst_letter_of_name(row):\n\t\t\t\treturn\trow[\"name\"][0]\tif\trow[\"name\"]\telse\t\"\"\n\ndef\taverage_num_friends(rows):\n\t\t\t\treturn\tsum(row[\"num_friends\"]\tfor\trow\tin\trows)\t/\tlen(rows)\n\ndef\tenough_friends(rows):\n\t\t\t\treturn\taverage_num_friends(rows)\t>\t1\n\navg_friends_by_letter\t=\tusers\t\\\n\t\t\t\t.select(additional_columns={'first_letter'\t:\tfirst_letter_of_name})\t\\\n\t\t\t\t.group_by(group_by_columns=['first_letter'],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\taggregates={\t\"avg_num_friends\"\t:\taverage_num_friends\t},\n\t\t\t\t\t\t\t\t\t\t\t\t\t\thaving=enough_friends)\n\nand\tthe\tuser_id_sum\tis:\n\ndef\tsum_user_ids(rows):\treturn\tsum(row[\"user_id\"]\tfor\trow\tin\trows)",
    "384": "user_id_sum\t=\tusers\t\\\n\t\t\t\t.where(lambda\trow:\trow[\"user_id\"]\t>\t1)\t\\\n\t\t\t\t.group_by(group_by_columns=[],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\taggregates={\t\"user_id_sum\"\t:\tsum_user_ids\t})",
    "385": "ORDER\tBY\n\nFrequently,\tyou\u2019ll\twant\tto\tsort\tyour\tresults.\tFor\texample,\tyou\tmight\twant\tto\tknow\tthe\n(alphabetically)\tfirst\ttwo\tnames\tof\tyour\tusers:\n\nSELECT\t*\tFROM\tusers\nORDER\tBY\tname\nLIMIT\t2;\n\nThis\tis\teasy\tto\timplement\tby\tgiving\tour\tTable\tan\torder_by()\tmethod\tthat\ttakes\tan\torder\nfunction:\n\n\t\t\t\tdef\torder_by(self,\torder):\n\t\t\t\t\t\t\t\tnew_table\t=\tself.select()\t\t\t\t\t\t\t#\tmake\ta\tcopy\n\t\t\t\t\t\t\t\tnew_table.rows.sort(key=order)\n\t\t\t\t\t\t\t\treturn\tnew_table\n\nwhich\twe\tcan\tthen\tuse\tas\tfollows:\n\nfriendliest_letters\t=\tavg_friends_by_letter\t\\\n\t\t\t\t.order_by(lambda\trow:\t-row[\"avg_num_friends\"])\t\\\n\t\t\t\t.limit(4)\n\nThe\tSQL\tORDER\tBY\tlets\tyou\tspecify\tASC\t(ascending)\tor\tDESC\t(descending)\tfor\teach\tsort\nfield;\there\twe\u2019d\thave\tto\tbake\tthat\tinto\tour\torder\tfunction.",
    "386": "JOIN\n\nRelational\tdatabase\ttables\tare\toften\tnormalized,\twhich\tmeans\tthat\tthey\u2019re\torganized\tto\nminimize\tredundancy.\tFor\texample,\twhen\twe\twork\twith\tour\tusers\u2019\tinterests\tin\tPython\twe\ncan\tjust\tgive\teach\tuser\ta\tlist\tcontaining\this\tinterests.\n\nSQL\ttables\tcan\u2019t\ttypically\tcontain\tlists,\tso\tthe\ttypical\tsolution\tis\tto\tcreate\ta\tsecond\ttable\nuser_interests\tcontaining\tthe\tone-to-many\trelationship\tbetween\tuser_ids\tand\tinterests.\nIn\tSQL\tyou\tmight\tdo:\n\nCREATE\tTABLE\tuser_interests\t(\n\t\t\t\tuser_id\tINT\tNOT\tNULL,\n\t\t\t\tinterest\tVARCHAR(100)\tNOT\tNULL\n);\n\nwhereas\tin\tNotQuiteABase\tyou\u2019d\tcreate\tthe\ttable:\n\nuser_interests\t=\tTable([\"user_id\",\t\"interest\"])\nuser_interests.insert([0,\t\"SQL\"])\nuser_interests.insert([0,\t\"NoSQL\"])\nuser_interests.insert([2,\t\"SQL\"])\nuser_interests.insert([2,\t\"MySQL\"])\n\nNOTE\n\nThere\u2019s\tstill\tplenty\tof\tredundancy\t\u2014\tthe\tinterest\t\u201cSQL\u201d\tis\tstored\tin\ttwo\tdifferent\tplaces.\tIn\ta\treal\tdatabase\nyou\tmight\tstore\tuser_id\tand\tinterest_id\tin\tthe\tuser_interests\ttable\tand\tthen\tcreate\ta\tthird\ttable\ninterests\tmapping\tinterest_id\tto\tinterest\tso\tyou\tcould\tstore\tthe\tinterest\tnames\tonly\tonce\teach.\tHere\nthat\twould\tjust\tmake\tour\texamples\tmore\tcomplicated\tthan\tthey\tneed\tto\tbe.\n\nWhen\tour\tdata\tlives\tacross\tdifferent\ttables,\thow\tdo\twe\tanalyze\tit?\tBy\tJOINing\tthe\ttables\ntogether.\tA\tJOIN\tcombines\trows\tin\tthe\tleft\ttable\twith\tcorresponding\trows\tin\tthe\tright\ntable,\twhere\tthe\tmeaning\tof\t\u201ccorresponding\u201d\tis\tbased\ton\thow\twe\tspecify\tthe\tjoin.\n\nFor\texample,\tto\tfind\tthe\tusers\tinterested\tin\tSQL\tyou\u2019d\tquery:\n\nSELECT\tusers.name\nFROM\tusers\nJOIN\tuser_interests\nON\tusers.user_id\t=\tuser_interests.user_id\nWHERE\tuser_interests.interest\t=\t'SQL'\n\nThe\tJOIN\tsays\tthat,\tfor\teach\trow\tin\tusers,\twe\tshould\tlook\tat\tthe\tuser_id\tand\tassociate\nthat\trow\twith\tevery\trow\tin\tuser_interests\tcontaining\tthe\tsame\tuser_id.\n\nNotice\twe\thad\tto\tspecify\twhich\ttables\tto\tJOIN\tand\talso\twhich\tcolumns\tto\tjoin\tON.\tThis\tis\nan\tINNER\tJOIN,\twhich\treturns\tthe\tcombinations\tof\trows\t(and\tonly\tthe\tcombinations\tof\nrows)\tthat\tmatch\taccording\tto\tthe\tspecified\tjoin\tcriteria.\n\nThere\tis\talso\ta\tLEFT\tJOIN,\twhich\t\u2014\tin\taddition\tto\tthe\tcombinations\tof\tmatching\trows\t\u2014\nreturns\ta\trow\tfor\teach\tleft-table\trow\twith\tno\tmatching\trows\t(in\twhich\tcase,\tthe\tfields\tthat\nwould\thave\tcome\tfrom\tthe\tright\ttable\tare\tall\tNULL).",
    "387": "Using\ta\tLEFT\tJOIN,\tit\u2019s\teasy\tto\tcount\tthe\tnumber\tof\tinterests\teach\tuser\thas:\n\nSELECT\tusers.id,\tCOUNT(user_interests.interest)\tAS\tnum_interests\nFROM\tusers\nLEFT\tJOIN\tuser_interests\nON\tusers.user_id\t=\tuser_interests.user_id\n\nThe\tLEFT\tJOIN\tensures\tthat\tusers\twith\tno\tinterests\twill\tstill\thave\trows\tin\tthe\tjoined\tdata\nset\t(with\tNULL\tvalues\tfor\tthe\tfields\tcoming\tfrom\tuser_interests),\tand\tCOUNT\tonly\tcounts\nvalues\tthat\tare\tnon-NULL.\n\nThe\tNotQuiteABase\tjoin()\timplementation\twill\tbe\tmore\trestrictive\t\u2014\tit\tsimply\tjoins\ntwo\ttables\ton\twhatever\tcolumns\tthey\thave\tin\tcommon.\tEven\tso,\tit\u2019s\tnot\ttrivial\tto\twrite:\n\n\t\t\t\tdef\tjoin(self,\tother_table,\tleft_join=False):\n\n\t\t\t\t\t\t\t\tjoin_on_columns\t=\t[c\tfor\tc\tin\tself.columns\t\t\t\t\t\t\t\t\t\t\t#\tcolumns\tin\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tc\tin\tother_table.columns]\t\t\t\t\t\t#\tboth\ttables\n\n\t\t\t\t\t\t\t\tadditional_columns\t=\t[c\tfor\tc\tin\tother_table.columns\t#\tcolumns\tonly\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tc\tnot\tin\tjoin_on_columns]\t\t\t#\tin\tright\ttable\n\n\t\t\t\t\t\t\t\t#\tall\tcolumns\tfrom\tleft\ttable\t+\tadditional_columns\tfrom\tright\ttable\n\t\t\t\t\t\t\t\tjoin_table\t=\tTable(self.columns\t+\tadditional_columns)\n\n\t\t\t\t\t\t\t\tfor\trow\tin\tself.rows:\n\t\t\t\t\t\t\t\t\t\t\t\tdef\tis_join(other_row):\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treturn\tall(other_row[c]\t==\trow[c]\tfor\tc\tin\tjoin_on_columns)\n\n\t\t\t\t\t\t\t\t\t\t\t\tother_rows\t=\tother_table.where(is_join).rows\n\n\t\t\t\t\t\t\t\t\t\t\t\t#\teach\tother\trow\tthat\tmatches\tthis\tone\tproduces\ta\tresult\trow\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tother_row\tin\tother_rows:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tjoin_table.insert([row[c]\tfor\tc\tin\tself.columns]\t+\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[other_row[c]\tfor\tc\tin\tadditional_columns])\n\n\t\t\t\t\t\t\t\t\t\t\t\t#\tif\tno\trows\tmatch\tand\tit's\ta\tleft\tjoin,\toutput\twith\tNones\n\t\t\t\t\t\t\t\t\t\t\t\tif\tleft_join\tand\tnot\tother_rows:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tjoin_table.insert([row[c]\tfor\tc\tin\tself.columns]\t+\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[None\tfor\tc\tin\tadditional_columns])\n\n\t\t\t\t\t\t\t\treturn\tjoin_table\n\nSo,\twe\tcould\tfind\tusers\tinterested\tin\tSQL\twith:\n\nsql_users\t=\tusers\t\\\n\t\t\t\t.join(user_interests)\t\\\n\t\t\t\t.where(lambda\trow:\trow[\"interest\"]\t==\t\"SQL\")\t\\\n\t\t\t\t.select(keep_columns=[\"name\"])\n\nAnd\twe\tcould\tget\tthe\tinterest\tcounts\twith:\n\ndef\tcount_interests(rows):\n\t\t\t\t\"\"\"counts\thow\tmany\trows\thave\tnon-None\tinterests\"\"\"\n\t\t\t\treturn\tlen([row\tfor\trow\tin\trows\tif\trow[\"interest\"]\tis\tnot\tNone])\n\nuser_interest_counts\t=\tusers\t\\\n\t\t\t\t.join(user_interests,\tleft_join=True)\t\\\n\t\t\t\t.group_by(group_by_columns=[\"user_id\"],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\taggregates={\"num_interests\"\t:\tcount_interests\t})\n\nIn\tSQL,\tthere\tis\talso\ta\tRIGHT\tJOIN,\twhich\tkeeps\trows\tfrom\tthe\tright\ttable\tthat\thave\tno",
    "388": "matches,\tand\ta\tFULL\tOUTER\tJOIN,\twhich\tkeeps\trows\tfrom\tboth\ttables\tthat\thave\tno\nmatches.\tWe\twon\u2019t\timplement\teither\tof\tthose.",
    "389": "Subqueries\n\nIn\tSQL,\tyou\tcan\tSELECT\tfrom\t(and\tJOIN)\tthe\tresults\tof\tqueries\tas\tif\tthey\twere\ttables.\tSo\tif\nyou\twanted\tto\tfind\tthe\tsmallest\tuser_id\tof\tanyone\tinterested\tin\tSQL,\tyou\tcould\tuse\ta\nsubquery.\t(Of\tcourse,\tyou\tcould\tdo\tthe\tsame\tcalculation\tusing\ta\tJOIN,\tbut\tthat\twouldn\u2019t\nillustrate\tsubqueries.)\n\nSELECT\tMIN(user_id)\tAS\tmin_user_id\tFROM\n(SELECT\tuser_id\tFROM\tuser_interests\tWHERE\tinterest\t=\t'SQL')\tsql_interests;\n\nGiven\tthe\tway\twe\u2019ve\tdesigned\tNotQuiteABase,\twe\tget\tthis\tfor\tfree.\t(Our\tquery\tresults\tare\nactual\ttables.)\n\nlikes_sql_user_ids\t=\tuser_interests\t\\\n\t\t\t\t.where(lambda\trow:\trow[\"interest\"]\t==\t\"SQL\")\t\\\n\t\t\t\t.select(keep_columns=['user_id'])\n\nlikes_sql_user_ids.group_by(group_by_columns=[],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taggregates={\t\"min_user_id\"\t:\tmin_user_id\t})",
    "390": "Indexes\n\nTo\tfind\trows\tcontaining\ta\tspecific\tvalue\t(say,\twhere\tname\tis\t\u201cHero\u201d),\tNotQuiteABase\thas\nto\tinspect\tevery\trow\tin\tthe\ttable.\tIf\tthe\ttable\thas\ta\tlot\tof\trows,\tthis\tcan\ttake\ta\tvery\tlong\ntime.\n\nSimilarly,\tour\tjoin\talgorithm\tis\textremely\tinefficient.\tFor\teach\trow\tin\tthe\tleft\ttable,\tit\ninspects\tevery\trow\tin\tthe\tright\ttable\tto\tsee\tif\tit\u2019s\ta\tmatch.\tWith\ttwo\tlarge\ttables\tthis\tcould\ntake\tapproximately\tforever.\n\nAlso,\tyou\u2019d\toften\tlike\tto\tapply\tconstraints\tto\tsome\tof\tyour\tcolumns.\tFor\texample,\tin\tyour\nusers\ttable\tyou\tprobably\tdon\u2019t\twant\tto\tallow\ttwo\tdifferent\tusers\tto\thave\tthe\tsame\nuser_id.\n\nIndexes\tsolve\tall\tthese\tproblems.\tIf\tthe\tuser_interests\ttable\thad\tan\tindex\ton\tuser_id,\ta\nsmart\tjoin\talgorithm\tcould\tfind\tmatches\tdirectly\trather\tthan\tscanning\tthe\twhole\ttable.\tIf\nthe\tusers\ttable\thad\ta\t\u201cunique\u201d\tindex\ton\tuser_id,\tyou\u2019d\tget\tan\terror\tif\tyou\ttried\tto\tinsert\ta\nduplicate.\n\nEach\ttable\tin\ta\tdatabase\tcan\thave\tone\tor\tmore\tindexes,\twhich\tallow\tyou\tto\tquickly\tlook\nup\trows\tby\tkey\tcolumns,\tefficiently\tjoin\ttables\ttogether,\tand\tenforce\tunique\tconstraints\ton\ncolumns\tor\tcombinations\tof\tcolumns.\n\nDesigning\tand\tusing\tindexes\twell\tis\tsomewhat\tof\ta\tblack\tart\t(which\tvaries\tsomewhat\ndepending\ton\tthe\tspecific\tdatabase),\tbut\tif\tyou\tend\tup\tdoing\ta\tlot\tof\tdatabase\twork\tit\u2019s\nworth\tlearning\tabout.",
    "391": "Query\tOptimization\n\nRecall\tthe\tquery\tto\tfind\tall\tusers\twho\tare\tinterested\tin\tSQL:\n\nSELECT\tusers.name\nFROM\tusers\nJOIN\tuser_interests\nON\tusers.user_id\t=\tuser_interests.user_id\nWHERE\tuser_interests.interest\t=\t'SQL'\n\nIn\tNotQuiteABase\tthere\tare\t(at\tleast)\ttwo\tdifferent\tways\tto\twrite\tthis\tquery.\tYou\tcould\nfilter\tthe\tuser_interests\ttable\tbefore\tperforming\tthe\tjoin:\n\nuser_interests\t\\\n\t\t\t\t.where(lambda\trow:\trow[\"interest\"]\t==\t\"SQL\")\t\\\n\t\t\t\t.join(users)\t\\\n\t\t\t\t.select([\"name\"])\n\nOr\tyou\tcould\tfilter\tthe\tresults\tof\tthe\tjoin:\n\nuser_interests\t\\\n\t\t\t\t.join(users)\t\\\n\t\t\t\t.where(lambda\trow:\trow[\"interest\"]\t==\t\"SQL\")\t\\\n\t\t\t\t.select([\"name\"])\n\nYou\u2019ll\tend\tup\twith\tthe\tsame\tresults\teither\tway,\tbut\tfilter-before-join\tis\talmost\tcertainly\nmore\tefficient,\tsince\tin\tthat\tcase\tjoin\thas\tmany\tfewer\trows\tto\toperate\ton.\n\nIn\tSQL,\tyou\tgenerally\twouldn\u2019t\tworry\tabout\tthis.\tYou\t\u201cdeclare\u201d\tthe\tresults\tyou\twant\tand\nleave\tit\tup\tto\tthe\tquery\tengine\tto\texecute\tthem\t(and\tuse\tindexes\tefficiently).",
    "392": "NoSQL\n\nA\trecent\ttrend\tin\tdatabases\tis\ttoward\tnonrelational\t\u201cNoSQL\u201d\tdatabases,\twhich\tdon\u2019t\nrepresent\tdata\tin\ttables.\tFor\tinstance,\tMongoDB\tis\ta\tpopular\tschema-less\tdatabase\twhose\nelements\tare\tarbitrarily\tcomplex\tJSON\tdocuments\trather\tthan\trows.\n\nThere\tare\tcolumn\tdatabases\tthat\tstore\tdata\tin\tcolumns\tinstead\tof\trows\t(good\twhen\tdata\nhas\tmany\tcolumns\tbut\tqueries\tneed\tfew\tof\tthem),\tkey-value\tstores\tthat\tare\toptimized\tfor\nretrieving\tsingle\t(complex)\tvalues\tby\ttheir\tkeys,\tdatabases\tfor\tstoring\tand\ttraversing\ngraphs,\tdatabases\tthat\tare\toptimized\tto\trun\tacross\tmultiple\tdatacenters,\tdatabases\tthat\tare\ndesigned\tto\trun\tin\tmemory,\tdatabases\tfor\tstoring\ttime-series\tdata,\tand\thundreds\tmore.\n\nTomorrow\u2019s\tflavor\tof\tthe\tday\tmight\tnot\teven\texist\tnow,\tso\tI\tcan\u2019t\tdo\tmuch\tmore\tthan\tlet\nyou\tknow\tthat\tNoSQL\tis\ta\tthing.\tSo\tnow\tyou\tknow.\tIt\u2019s\ta\tthing.",
    "393": "For\tFurther\tExploration\n\nIf\tyou\u2019d\tlike\tto\tdownload\ta\trelational\tdatabase\tto\tplay\twith,\tSQLite\tis\tfast\tand\ttiny,\nwhile\tMySQL\tand\tPostgreSQL\tare\tlarger\tand\tfeatureful.\tAll\tare\tfree\tand\thave\tlots\tof\ndocumentation.\n\nIf\tyou\twant\tto\texplore\tNoSQL,\tMongoDB\tis\tvery\tsimple\tto\tget\tstarted\twith,\twhich\tcan\nbe\tboth\ta\tblessing\tand\tsomewhat\tof\ta\tcurse.\tIt\talso\thas\tpretty\tgood\tdocumentation.\n\nThe\tWikipedia\tarticle\ton\tNoSQL\talmost\tcertainly\tnow\tcontains\tlinks\tto\tdatabases\tthat\ndidn\u2019t\teven\texist\twhen\tthis\tbook\twas\twritten.",
    "394": "",
    "395": "Chapter\t24.\tMapReduce\n\nThe\tfuture\thas\talready\tarrived.\tIt\u2019s\tjust\tnot\tevenly\tdistributed\tyet.\n\nWilliam\tGibson\n\nMapReduce\tis\ta\tprogramming\tmodel\tfor\tperforming\tparallel\tprocessing\ton\tlarge\tdata\tsets.\nAlthough\tit\tis\ta\tpowerful\ttechnique,\tits\tbasics\tare\trelatively\tsimple.\n\nImagine\twe\thave\ta\tcollection\tof\titems\twe\u2019d\tlike\tto\tprocess\tsomehow.\tFor\tinstance,\tthe\nitems\tmight\tbe\twebsite\tlogs,\tthe\ttexts\tof\tvarious\tbooks,\timage\tfiles,\tor\tanything\telse.\tA\nbasic\tversion\tof\tthe\tMapReduce\talgorithm\tconsists\tof\tthe\tfollowing\tsteps:\n\n1.\t Use\ta\tmapper\tfunction\tto\tturn\teach\titem\tinto\tzero\tor\tmore\tkey-value\tpairs.\t(Often\n\nthis\tis\tcalled\tthe\tmap\tfunction,\tbut\tthere\tis\talready\ta\tPython\tfunction\tcalled\tmap\tand\nwe\tdon\u2019t\tneed\tto\tconfuse\tthe\ttwo.)\n\n2.\t Collect\ttogether\tall\tthe\tpairs\twith\tidentical\tkeys.\n\n3.\t Use\ta\treducer\tfunction\ton\teach\tcollection\tof\tgrouped\tvalues\tto\tproduce\toutput\n\nvalues\tfor\tthe\tcorresponding\tkey.\n\nThis\tis\tall\tsort\tof\tabstract,\tso\tlet\u2019s\tlook\tat\ta\tspecific\texample.\tThere\tare\tfew\tabsolute\trules\nof\tdata\tscience,\tbut\tone\tof\tthem\tis\tthat\tyour\tfirst\tMapReduce\texample\thas\tto\tinvolve\ncounting\twords.",
    "396": "Example:\tWord\tCount\n\nDataSciencester\thas\tgrown\tto\tmillions\tof\tusers!\tThis\tis\tgreat\tfor\tyour\tjob\tsecurity,\tbut\tit\nmakes\troutine\tanalyses\tslightly\tmore\tdifficult.\n\nFor\texample,\tyour\tVP\tof\tContent\twants\tto\tknow\twhat\tsorts\tof\tthings\tpeople\tare\ttalking\nabout\tin\ttheir\tstatus\tupdates.\tAs\ta\tfirst\tattempt,\tyou\tdecide\tto\tcount\tthe\twords\tthat\tappear,\nso\tthat\tyou\tcan\tprepare\ta\treport\ton\tthe\tmost\tfrequent\tones.\n\nWhen\tyou\thad\ta\tfew\thundred\tusers\tthis\twas\tsimple\tto\tdo:\n\ndef\tword_count_old(documents):\n\t\t\t\t\"\"\"word\tcount\tnot\tusing\tMapReduce\"\"\"\n\t\t\t\treturn\tCounter(word\n\t\t\t\t\t\t\t\tfor\tdocument\tin\tdocuments\n\t\t\t\t\t\t\t\tfor\tword\tin\ttokenize(document))\n\nWith\tmillions\tof\tusers\tthe\tset\tof\tdocuments\t(status\tupdates)\tis\tsuddenly\ttoo\tbig\tto\tfit\ton\nyour\tcomputer.\tIf\tyou\tcan\tjust\tfit\tthis\tinto\tthe\tMapReduce\tmodel,\tyou\tcan\tuse\tsome\t\u201cbig\ndata\u201d\tinfrastructure\tthat\tyour\tengineers\thave\timplemented.\n\nFirst,\twe\tneed\ta\tfunction\tthat\tturns\ta\tdocument\tinto\ta\tsequence\tof\tkey-value\tpairs.\tWe\u2019ll\nwant\tour\toutput\tto\tbe\tgrouped\tby\tword,\twhich\tmeans\tthat\tthe\tkeys\tshould\tbe\twords.\tAnd\nfor\teach\tword,\twe\u2019ll\tjust\temit\tthe\tvalue\t1\tto\tindicate\tthat\tthis\tpair\tcorresponds\tto\tone\noccurrence\tof\tthe\tword:\n\ndef\twc_mapper(document):\n\t\t\t\t\"\"\"for\teach\tword\tin\tthe\tdocument,\temit\t(word,1)\"\"\"\n\t\t\t\tfor\tword\tin\ttokenize(document):\n\t\t\t\t\t\t\t\tyield\t(word,\t1)\n\nSkipping\tthe\t\u201cplumbing\u201d\tstep\t2\tfor\tthe\tmoment,\timagine\tthat\tfor\tsome\tword\twe\u2019ve\ncollected\ta\tlist\tof\tthe\tcorresponding\tcounts\twe\temitted.\tThen\tto\tproduce\tthe\toverall\tcount\nfor\tthat\tword\twe\tjust\tneed:\n\ndef\twc_reducer(word,\tcounts):\n\t\t\t\t\"\"\"sum\tup\tthe\tcounts\tfor\ta\tword\"\"\"\n\t\t\t\tyield\t(word,\tsum(counts))\n\nReturning\tto\tstep\t2,\twe\tnow\tneed\tto\tcollect\tthe\tresults\tfrom\twc_mapper\tand\tfeed\tthem\tto\nwc_reducer.\tLet\u2019s\tthink\tabout\thow\twe\twould\tdo\tthis\ton\tjust\tone\tcomputer:\n\ndef\tword_count(documents):\n\t\t\t\t\"\"\"count\tthe\twords\tin\tthe\tinput\tdocuments\tusing\tMapReduce\"\"\"\n\n\t\t\t\t#\tplace\tto\tstore\tgrouped\tvalues\n\t\t\t\tcollector\t=\tdefaultdict(list)\n\n\t\t\t\tfor\tdocument\tin\tdocuments:\n\t\t\t\t\t\t\t\tfor\tword,\tcount\tin\twc_mapper(document):\n\t\t\t\t\t\t\t\t\t\t\t\tcollector[word].append(count)\n\n\t\t\t\treturn\t[output\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tword,\tcounts\tin\tcollector.iteritems()\n\t\t\t\t\t\t\t\t\t\t\t\tfor\toutput\tin\twc_reducer(word,\tcounts)]",
    "397": "Imagine\tthat\twe\thave\tthree\tdocuments\t[\"data\tscience\",\t\"big\tdata\",\t\"science\nfiction\"].\n\nThen\twc_mapper\tapplied\tto\tthe\tfirst\tdocument\tyields\tthe\ttwo\tpairs\t(\"data\",\t1)\tand\n(\"science\",\t1).\tAfter\twe\u2019ve\tgone\tthrough\tall\tthree\tdocuments,\tthe\tcollector\tcontains\n\n{\t\"data\"\t:\t[1,\t1],\n\t\t\"science\"\t:\t[1,\t1],\n\t\t\"big\"\t:\t[1],\n\t\t\"fiction\"\t:\t[1]\t}\n\nThen\twc_reducer\tproduces\tthe\tcount\tfor\teach\tword:\n\n[(\"data\",\t2),\t(\"science\",\t2),\t(\"big\",\t1),\t(\"fiction\",\t1)]",
    "398": "Why\tMapReduce?\n\nAs\tmentioned\tearlier,\tthe\tprimary\tbenefit\tof\tMapReduce\tis\tthat\tit\tallows\tus\tto\tdistribute\ncomputations\tby\tmoving\tthe\tprocessing\tto\tthe\tdata.\tImagine\twe\twant\tto\tword-count\nacross\tbillions\tof\tdocuments.\n\nOur\toriginal\t(non-MapReduce)\tapproach\trequires\tthe\tmachine\tdoing\tthe\tprocessing\tto\nhave\taccess\tto\tevery\tdocument.\tThis\tmeans\tthat\tthe\tdocuments\tall\tneed\tto\teither\tlive\ton\nthat\tmachine\tor\telse\tbe\ttransferred\tto\tit\tduring\tprocessing.\tMore\timportant,\tit\tmeans\tthat\nthe\tmachine\tcan\tonly\tprocess\tone\tdocument\tat\ta\ttime.\n\nPossibly\tit\tcan\tprocess\tup\tto\ta\tfew\tat\ta\ttime\tif\tit\thas\tmultiple\tcores\tand\tif\tthe\tcode\tis\trewritten\tto\ttake\nadvantage\tof\tthem.\tBut\teven\tso,\tall\tthe\tdocuments\tstill\thave\tto\tget\tto\tthat\tmachine.\n\nNOTE\n\nImagine\tnow\tthat\tour\tbillions\tof\tdocuments\tare\tscattered\tacross\t100\tmachines.\tWith\tthe\nright\tinfrastructure\t(and\tglossing\tover\tsome\tof\tthe\tdetails),\twe\tcan\tdo\tthe\tfollowing:\n\nHave\teach\tmachine\trun\tthe\tmapper\ton\tits\tdocuments,\tproducing\tlots\tof\t(key,\tvalue)\npairs.\n\nDistribute\tthose\t(key,\tvalue)\tpairs\tto\ta\tnumber\tof\t\u201creducing\u201d\tmachines,\tmaking\tsure\nthat\tthe\tpairs\tcorresponding\tto\tany\tgiven\tkey\tall\tend\tup\ton\tthe\tsame\tmachine.\n\nHave\teach\treducing\tmachine\tgroup\tthe\tpairs\tby\tkey\tand\tthen\trun\tthe\treducer\ton\teach\nset\tof\tvalues.\n\nReturn\teach\t(key,\toutput)\tpair.\n\nWhat\tis\tamazing\tabout\tthis\tis\tthat\tit\tscales\thorizontally.\tIf\twe\tdouble\tthe\tnumber\tof\nmachines,\tthen\t(ignoring\tcertain\tfixed-costs\tof\trunning\ta\tMapReduce\tsystem)\tour\ncomputation\tshould\trun\tapproximately\ttwice\tas\tfast.\tEach\tmapper\tmachine\twill\tonly\tneed\nto\tdo\thalf\tas\tmuch\twork,\tand\t(assuming\tthere\tare\tenough\tdistinct\tkeys\tto\tfurther\tdistribute\nthe\treducer\twork)\tthe\tsame\tis\ttrue\tfor\tthe\treducer\tmachines.",
    "399": "MapReduce\tMore\tGenerally\n\nIf\tyou\tthink\tabout\tit\tfor\ta\tminute,\tall\tof\tthe\tword-count-specific\tcode\tin\tthe\tprevious\nexample\tis\tcontained\tin\tthe\twc_mapper\tand\twc_reducer\tfunctions.\tThis\tmeans\tthat\twith\ta\ncouple\tof\tchanges\twe\thave\ta\tmuch\tmore\tgeneral\tframework\t(that\tstill\truns\ton\ta\tsingle\nmachine):\n\ndef\tmap_reduce(inputs,\tmapper,\treducer):\n\t\t\t\t\"\"\"runs\tMapReduce\ton\tthe\tinputs\tusing\tmapper\tand\treducer\"\"\"\n\t\t\t\tcollector\t=\tdefaultdict(list)\n\n\t\t\t\tfor\tinput\tin\tinputs:\n\t\t\t\t\t\t\t\tfor\tkey,\tvalue\tin\tmapper(input):\n\t\t\t\t\t\t\t\t\t\t\t\tcollector[key].append(value)\n\n\t\t\t\treturn\t[output\n\t\t\t\t\t\t\t\t\t\t\t\tfor\tkey,\tvalues\tin\tcollector.iteritems()\n\t\t\t\t\t\t\t\t\t\t\t\tfor\toutput\tin\treducer(key,values)]\n\nAnd\tthen\twe\tcan\tcount\twords\tsimply\tby\tusing:\n\nword_counts\t=\tmap_reduce(documents,\twc_mapper,\twc_reducer)\n\nThis\tgives\tus\tthe\tflexibility\tto\tsolve\ta\twide\tvariety\tof\tproblems.\n\nBefore\twe\tproceed,\tobserve\tthat\twc_reducer\tis\tjust\tsumming\tthe\tvalues\tcorresponding\tto\neach\tkey.\tThis\tkind\tof\taggregation\tis\tcommon\tenough\tthat\tit\u2019s\tworth\tabstracting\tit\tout:\n\ndef\treduce_values_using(aggregation_fn,\tkey,\tvalues):\n\t\t\t\t\"\"\"reduces\ta\tkey-values\tpair\tby\tapplying\taggregation_fn\tto\tthe\tvalues\"\"\"\n\t\t\t\tyield\t(key,\taggregation_fn(values))\n\ndef\tvalues_reducer(aggregation_fn):\n\t\t\t\t\"\"\"turns\ta\tfunction\t(values\t->\toutput)\tinto\ta\treducer\n\t\t\t\tthat\tmaps\t(key,\tvalues)\t->\t(key,\toutput)\"\"\"\n\t\t\t\treturn\tpartial(reduce_values_using,\taggregation_fn)\n\nafter\twhich\twe\tcan\teasily\tcreate:\n\nsum_reducer\t=\tvalues_reducer(sum)\nmax_reducer\t=\tvalues_reducer(max)\nmin_reducer\t=\tvalues_reducer(min)\ncount_distinct_reducer\t=\tvalues_reducer(lambda\tvalues:\tlen(set(values)))\n\nand\tso\ton.",
    "400": "Example:\tAnalyzing\tStatus\tUpdates\n\nThe\tcontent\tVP\twas\timpressed\twith\tthe\tword\tcounts\tand\tasks\twhat\telse\tyou\tcan\tlearn\nfrom\tpeople\u2019s\tstatus\tupdates.\tYou\tmanage\tto\textract\ta\tdata\tset\tof\tstatus\tupdates\tthat\tlook\nlike:\n\n{\"id\":\t1,\n\t\"username\"\t:\t\"joelgrus\",\n\t\"text\"\t:\t\"Is\tanyone\tinterested\tin\ta\tdata\tscience\tbook?\",\n\t\"created_at\"\t:\tdatetime.datetime(2013,\t12,\t21,\t11,\t47,\t0),\n\t\"liked_by\"\t:\t[\"data_guy\",\t\"data_gal\",\t\"mike\"]\t}\n\nLet\u2019s\tsay\twe\tneed\tto\tfigure\tout\twhich\tday\tof\tthe\tweek\tpeople\ttalk\tthe\tmost\tabout\tdata\nscience.\tIn\torder\tto\tfind\tthis,\twe\u2019ll\tjust\tcount\thow\tmany\tdata\tscience\tupdates\tthere\tare\ton\neach\tday\tof\tthe\tweek.\tThis\tmeans\twe\u2019ll\tneed\tto\tgroup\tby\tthe\tday\tof\tweek,\tso\tthat\u2019s\tour\nkey.\tAnd\tif\twe\temit\ta\tvalue\tof\t1\tfor\teach\tupdate\tthat\tcontains\t\u201cdata\tscience,\u201d\twe\tcan\nsimply\tget\tthe\ttotal\tnumber\tusing\tsum:\n\ndef\tdata_science_day_mapper(status_update):\n\t\t\t\t\"\"\"yields\t(day_of_week,\t1)\tif\tstatus_update\tcontains\t\"data\tscience\"\t\"\"\"\n\t\t\t\tif\t\"data\tscience\"\tin\tstatus_update[\"text\"].lower():\n\t\t\t\t\t\t\t\tday_of_week\t=\tstatus_update[\"created_at\"].weekday()\n\t\t\t\t\t\t\t\tyield\t(day_of_week,\t1)\n\ndata_science_days\t=\tmap_reduce(status_updates,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata_science_day_mapper,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsum_reducer)\n\nAs\ta\tslightly\tmore\tcomplicated\texample,\timagine\twe\tneed\tto\tfind\tout\tfor\teach\tuser\tthe\nmost\tcommon\tword\tthat\tshe\tputs\tin\ther\tstatus\tupdates.\tThere\tare\tthree\tpossible\napproaches\tthat\tspring\tto\tmind\tfor\tthe\tmapper:\n\nPut\tthe\tusername\tin\tthe\tkey;\tput\tthe\twords\tand\tcounts\tin\tthe\tvalues.\n\nPut\tthe\tword\tin\tkey;\tput\tthe\tusernames\tand\tcounts\tin\tthe\tvalues.\n\nPut\tthe\tusername\tand\tword\tin\tthe\tkey;\tput\tthe\tcounts\tin\tthe\tvalues.\n\nIf\tyou\tthink\tabout\tit\ta\tbit\tmore,\twe\tdefinitely\twant\tto\tgroup\tby\tusername,\tbecause\twe\nwant\tto\tconsider\teach\tperson\u2019s\twords\tseparately.\tAnd\twe\tdon\u2019t\twant\tto\tgroup\tby\tword,\nsince\tour\treducer\twill\tneed\tto\tsee\tall\tthe\twords\tfor\teach\tperson\tto\tfind\tout\twhich\tis\tthe\nmost\tpopular.\tThis\tmeans\tthat\tthe\tfirst\toption\tis\tthe\tright\tchoice:\n\ndef\twords_per_user_mapper(status_update):\n\t\t\t\tuser\t=\tstatus_update[\"username\"]\n\t\t\t\tfor\tword\tin\ttokenize(status_update[\"text\"]):\n\t\t\t\t\t\t\t\tyield\t(user,\t(word,\t1))\n\ndef\tmost_popular_word_reducer(user,\twords_and_counts):\n\t\t\t\t\"\"\"given\ta\tsequence\tof\t(word,\tcount)\tpairs,\n\t\t\t\treturn\tthe\tword\twith\tthe\thighest\ttotal\tcount\"\"\"\n\n\t\t\t\tword_counts\t=\tCounter()\n\t\t\t\tfor\tword,\tcount\tin\twords_and_counts:\n\t\t\t\t\t\t\t\tword_counts[word]\t+=\tcount",
    "401": "word,\tcount\t=\tword_counts.most_common(1)[0]\n\n\t\t\t\tyield\t(user,\t(word,\tcount))\n\nuser_words\t=\tmap_reduce(status_updates,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\twords_per_user_mapper,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmost_popular_word_reducer)\n\nOr\twe\tcould\tfind\tout\tthe\tnumber\tof\tdistinct\tstatus-likers\tfor\teach\tuser:\n\ndef\tliker_mapper(status_update):\n\t\t\t\tuser\t=\tstatus_update[\"username\"]\n\t\t\t\tfor\tliker\tin\tstatus_update[\"liked_by\"]:\n\t\t\t\t\t\t\t\tyield\t(user,\tliker)\n\ndistinct_likers_per_user\t=\tmap_reduce(status_updates,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tliker_mapper,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcount_distinct_reducer)",
    "402": "Example:\tMatrix\tMultiplication\n\nRecall\tfrom\t\u201cMatrix\tMultiplication\u201d\tthat\tgiven\ta\t\nB,\twe\tcan\tmultiply\tthem\tto\tform\ta\t\ncolumn\tj\tis\tgiven\tby:\n\n\tmatrix\n\tmatrix\tA\tand\ta\t\n\tmatrix\tC,\twhere\tthe\telement\tof\tC\tin\trow\ti\tand\n\nAs\twe\u2019ve\tseen,\ta\t\u201cnatural\u201d\tway\tto\trepresent\ta\t\n\n\tmatrix\tis\twith\ta\tlist\tof\tlists,\n\nwhere\tthe\telement\t\n\n\tis\tthe\tjth\telement\tof\tthe\tith\tlist.\n\nBut\tlarge\tmatrices\tare\tsometimes\tsparse,\twhich\tmeans\tthat\tmost\tof\ttheir\telements\tequal\nzero.\tFor\tlarge\tsparse\tmatrices,\ta\tlist\tof\tlists\tcan\tbe\ta\tvery\twasteful\trepresentation.\tA\tmore\ncompact\trepresentation\tis\ta\tlist\tof\ttuples\t(name,\ti,\tj,\tvalue)\twhere\tname\tidentifies\tthe\nmatrix,\tand\twhere\ti,\tj,\tvalue\tindicates\ta\tlocation\twith\tnonzero\tvalue.\n\nFor\texample,\ta\tbillion\t\u00d7\tbillion\tmatrix\thas\ta\tquintillion\tentries,\twhich\twould\tnot\tbe\teasy\nto\tstore\ton\ta\tcomputer.\tBut\tif\tthere\tare\tonly\ta\tfew\tnonzero\tentries\tin\teach\trow,\tthis\nalternative\trepresentation\tis\tmany\torders\tof\tmagnitude\tsmaller.\n\nGiven\tthis\tsort\tof\trepresentation,\tit\tturns\tout\tthat\twe\tcan\tuse\tMapReduce\tto\tperform\nmatrix\tmultiplication\tin\ta\tdistributed\tmanner.\n\nTo\tmotivate\tour\talgorithm,\tnotice\tthat\teach\telement\t\n\n\tis\tonly\tused\tto\tcompute\tthe\n\nelements\tof\tC\tin\trow\ti,\tand\teach\telement\t\n\tis\tonly\tused\tto\tcompute\tthe\telements\tof\tC\tin\ncolumn\tj.\tOur\tgoal\twill\tbe\tfor\teach\toutput\tof\tour\treducer\tto\tbe\ta\tsingle\tentry\tof\tC,\twhich\nmeans\twe\u2019ll\tneed\tour\tmapper\tto\temit\tkeys\tidentifying\ta\tsingle\tentry\tof\tC.\tThis\tsuggests\nthe\tfollowing:\n\ndef\tmatrix_multiply_mapper(m,\telement):\n\t\t\t\t\"\"\"m\tis\tthe\tcommon\tdimension\t(columns\tof\tA,\trows\tof\tB)\n\t\t\t\telement\tis\ta\ttuple\t(matrix_name,\ti,\tj,\tvalue)\"\"\"\n\t\t\t\tname,\ti,\tj,\tvalue\t=\telement\n\n\t\t\t\tif\tname\t==\t\"A\":\n\t\t\t\t\t\t\t\t#\tA_ij\tis\tthe\tjth\tentry\tin\tthe\tsum\tfor\teach\tC_ik,\tk=1..m\n\t\t\t\t\t\t\t\tfor\tk\tin\trange(m):\n\t\t\t\t\t\t\t\t\t\t\t\t#\tgroup\twith\tother\tentries\tfor\tC_ik\n\t\t\t\t\t\t\t\t\t\t\t\tyield((i,\tk),\t(j,\tvalue))\n\t\t\t\telse:\n\t\t\t\t\t\t\t\t#\tB_ij\tis\tthe\ti-th\tentry\tin\tthe\tsum\tfor\teach\tC_kj\n\t\t\t\t\t\t\t\tfor\tk\tin\trange(m):\n\t\t\t\t\t\t\t\t\t\t\t\t#\tgroup\twith\tother\tentries\tfor\tC_kj\n\t\t\t\t\t\t\t\t\t\t\t\tyield((k,\tj),\t(i,\tvalue))\n\ndef\tmatrix_multiply_reducer(m,\tkey,\tindexed_values):\n\t\t\t\tresults_by_index\t=\tdefaultdict(list)\n\t\t\t\tfor\tindex,\tvalue\tin\tindexed_values:\n\t\t\t\t\t\t\t\tresults_by_index[index].append(value)\n\n\t\t\t\t#\tsum\tup\tall\tthe\tproducts\tof\tthe\tpositions\twith\ttwo\tresults\n\t\t\t\tsum_product\t=\tsum(results[0]\t*\tresults[1]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tresults\tin\tresults_by_index.values()\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tlen(results)\t==\t2)",
    "403": "if\tsum_product\t!=\t0.0:\n\t\t\t\t\t\t\t\tyield\t(key,\tsum_product)\n\nFor\texample,\tif\tyou\thad\tthe\ttwo\tmatrices\n\nA\t=\t[[3,\t2,\t0],\n\t\t\t\t\t[0,\t0,\t0]]\n\nB\t=\t[[4,\t-1,\t0],\n\t\t\t\t\t[10,\t0,\t0],\n\t\t\t\t\t[0,\t0,\t0]]\n\nyou\tcould\trewrite\tthem\tas\ttuples:\n\nentries\t=\t[(\"A\",\t0,\t0,\t3),\t(\"A\",\t0,\t1,\t\t2),\n\t\t\t\t\t\t\t\t\t\t\t(\"B\",\t0,\t0,\t4),\t(\"B\",\t0,\t1,\t-1),\t(\"B\",\t1,\t0,\t10)]\nmapper\t=\tpartial(matrix_multiply_mapper,\t3)\nreducer\t=\tpartial(matrix_multiply_reducer,\t3)\n\nmap_reduce(entries,\tmapper,\treducer)\t#\t[((0,\t1),\t-3),\t((0,\t0),\t32)]\n\nThis\tisn\u2019t\tterribly\tinteresting\ton\tsuch\tsmall\tmatrices,\tbut\tif\tyou\thad\tmillions\tof\trows\tand\nmillions\tof\tcolumns,\tit\tcould\thelp\tyou\ta\tlot.",
    "404": "An\tAside:\tCombiners\n\nOne\tthing\tyou\thave\tprobably\tnoticed\tis\tthat\tmany\tof\tour\tmappers\tseem\tto\tinclude\ta\tbunch\nof\textra\tinformation.\tFor\texample,\twhen\tcounting\twords,\trather\tthan\temitting\t(word,\t1)\nand\tsumming\tover\tthe\tvalues,\twe\tcould\thave\temitted\t(word,\tNone)\tand\tjust\ttaken\tthe\nlength.\n\nOne\treason\twe\tdidn\u2019t\tdo\tthis\tis\tthat,\tin\tthe\tdistributed\tsetting,\twe\tsometimes\twant\tto\tuse\ncombiners\tto\treduce\tthe\tamount\tof\tdata\tthat\thas\tto\tbe\ttransferred\taround\tfrom\tmachine\tto\nmachine.\tIf\tone\tof\tour\tmapper\tmachines\tsees\tthe\tword\t\u201cdata\u201d\t500\ttimes,\twe\tcan\ttell\tit\tto\ncombine\tthe\t500\tinstances\tof\t(\"data\",\t1)\tinto\ta\tsingle\t(\"data\",\t500)\tbefore\thanding\noff\tto\tthe\treducing\tmachine.\tThis\tresults\tin\ta\tlot\tless\tdata\tgetting\tmoved\taround,\twhich\ncan\tmake\tour\talgorithm\tsubstantially\tfaster\tstill.\n\nBecause\tof\tthe\tway\twe\twrote\tour\treducer,\tit\twould\thandle\tthis\tcombined\tdata\tcorrectly.\t(If\nwe\u2019d\twritten\tit\tusing\tlen\tit\twould\tnot\thave.)",
    "405": "For\tFurther\tExploration\n\nThe\tmost\twidely\tused\tMapReduce\tsystem\tis\tHadoop,\twhich\titself\tmerits\tmany\tbooks.\nThere\tare\tvarious\tcommercial\tand\tnoncommercial\tdistributions\tand\ta\thuge\tecosystem\nof\tHadoop-related\ttools.\t\nIn\torder\tto\tuse\tit,\tyou\thave\tto\tset\tup\tyour\town\tcluster\t(or\tfind\tsomeone\tto\tlet\tyou\tuse\ntheirs),\twhich\tis\tnot\tnecessarily\ta\ttask\tfor\tthe\tfaint-hearted.\tHadoop\tmappers\tand\nreducers\tare\tcommonly\twritten\tin\tJava,\talthough\tthere\tis\ta\tfacility\tknown\tas\t\u201cHadoop\nstreaming\u201d\tthat\tallows\tyou\tto\twrite\tthem\tin\tother\tlanguages\t(including\tPython).\n\nAmazon.com\toffers\tan\tElastic\tMapReduce\tservice\tthat\tcan\tprogrammatically\tcreate\nand\tdestroy\tclusters,\tcharging\tyou\tonly\tfor\tthe\tamount\tof\ttime\tthat\tyou\u2019re\tusing\tthem.\n\nmrjob\tis\ta\tPython\tpackage\tfor\tinterfacing\twith\tHadoop\t(or\tElastic\tMapReduce).\n\nHadoop\tjobs\tare\ttypically\thigh-latency,\twhich\tmakes\tthem\ta\tpoor\tchoice\tfor\t\u201creal-\ntime\u201d\tanalytics.\tThere\tare\tvarious\t\u201creal-time\u201d\ttools\tbuilt\ton\ttop\tof\tHadoop,\tbut\tthere\nare\talso\tseveral\talternative\tframeworks\tthat\tare\tgrowing\tin\tpopularity.\tTwo\tof\tthe\tmost\npopular\tare\tSpark\tand\tStorm.\n\nAll\tthat\tsaid,\tby\tnow\tit\u2019s\tquite\tlikely\tthat\tthe\tflavor\tof\tthe\tday\tis\tsome\thot\tnew\ndistributed\tframework\tthat\tdidn\u2019t\teven\texist\twhen\tthis\tbook\twas\twritten.\tYou\u2019ll\thave\tto\nfind\tthat\tone\tyourself.",
    "406": "",
    "407": "Chapter\t25.\tGo\tForth\tand\tDo\tData\nScience\n\nAnd\tnow,\tonce\tagain,\tI\tbid\tmy\thideous\tprogeny\tgo\tforth\tand\tprosper.\n\nMary\tShelley\n\nWhere\tdo\tyou\tgo\tfrom\there?\tAssuming\tI\thaven\u2019t\tscared\tyou\toff\tof\tdata\tscience,\tthere\tare\na\tnumber\tof\tthings\tyou\tshould\tlearn\tnext.",
    "408": "IPython\n\nWe\tmentioned\tIPython\tearlier\tin\tthe\tbook.\tIt\tprovides\ta\tshell\twith\tfar\tmore\tfunctionality\nthan\tthe\tstandard\tPython\tshell,\tand\tit\tadds\t\u201cmagic\tfunctions\u201d\tthat\tallow\tyou\tto\t(among\nother\tthings)\teasily\tcopy\tand\tpaste\tcode\t(which\tis\tnormally\tcomplicated\tby\tthe\ncombination\tof\tblank\tlines\tand\twhitespace\tformatting)\tand\trun\tscripts\tfrom\twithin\tthe\nshell.\n\nMastering\tIPython\twill\tmake\tyour\tlife\tfar\teasier.\t(Even\tlearning\tjust\ta\tlittle\tbit\tof\tIPython\nwill\tmake\tyour\tlife\ta\tlot\teasier.)\n\nAdditionally,\tit\tallows\tyou\tto\tcreate\t\u201cnotebooks\u201d\tcombining\ttext,\tlive\tPython\tcode,\tand\nvisualizations\tthat\tyou\tcan\tshare\twith\tother\tpeople,\tor\tjust\tkeep\taround\tas\ta\tjournal\tof\nwhat\tyou\tdid\t(Figure\t25-1).\n\nFigure\t25-1.\tAn\tIPython\tnotebook",
    "409": "Mathematics\n\nThroughout\tthis\tbook,\twe\tdabbled\tin\tlinear\talgebra\t(Chapter\t4),\tstatistics\t(Chapter\t5),\nprobability\t(Chapter\t6),\tand\tvarious\taspects\tof\tmachine\tlearning.\n\nTo\tbe\ta\tgood\tdata\tscientist,\tyou\tshould\tknow\tmuch\tmore\tabout\tthese\ttopics,\tand\tI\nencourage\tyou\tto\tgive\teach\tof\tthem\ta\tmore\tin-depth\tstudy,\tusing\tthe\ttextbooks\nrecommended\tat\tthe\tend\tof\tthe\tchapters,\tyour\town\tpreferred\ttextbooks,\tonline\tcourses,\tor\neven\treal-life\tcourses.",
    "410": "Not\tfrom\tScratch\n\nImplementing\tthings\t\u201cfrom\tscratch\u201d\tis\tgreat\tfor\tunderstanding\thow\tthey\twork.\tBut\tit\u2019s\ngenerally\tnot\tgreat\tfor\tperformance\t(unless\tyou\u2019re\timplementing\tthem\tspecifically\twith\nperformance\tin\tmind),\tease\tof\tuse,\trapid\tprototyping,\tor\terror\thandling.\n\nIn\tpractice,\tyou\u2019ll\twant\tto\tuse\twell-designed\tlibraries\tthat\tsolidly\timplement\tthe\nfundamentals.\t(My\toriginal\tproposal\tfor\tthis\tbook\tinvolved\ta\tsecond\t\u201cnow\tlet\u2019s\tlearn\tthe\nlibraries\u201d\thalf\tthat\tO\u2019Reilly,\tthankfully,\tvetoed.)",
    "411": "NumPy\n\nNumPy\t(for\t\u201cNumeric\tPython\u201d)\tprovides\tfacilities\tfor\tdoing\t\u201creal\u201d\tscientific\tcomputing.\nIt\tfeatures\tarrays\tthat\tperform\tbetter\tthan\tour\tlist-vectors,\tmatrices\tthat\tperform\tbetter\nthan\tour\tlist-of-list-matrices,\tand\tlots\tof\tnumeric\tfunctions\tfor\tworking\twith\tthem.\n\nNumPy\tis\ta\tbuilding\tblock\tfor\tmany\tother\tlibraries,\twhich\tmakes\tit\tespecially\tvaluable\tto\nknow.",
    "412": "pandas\n\npandas\tprovides\tadditional\tdata\tstructures\tfor\tworking\twith\tdata\tsets\tin\tPython.\tIts\nprimary\tabstraction\tis\tthe\tDataFrame,\twhich\tis\tconceptually\tsimilar\tto\tthe\tNotQuiteABase\nTable\tclass\twe\tconstructed\tin\tChapter\t23,\tbut\twith\tmuch\tmore\tfunctionality\tand\tbetter\nperformance.\n\nIf\tyou\u2019re\tgoing\tto\tuse\tPython\tto\tmunge,\tslice,\tgroup,\tand\tmanipulate\tdata\tsets,\tpandas\tis\nan\tinvaluable\ttool.",
    "413": "scikit-learn\n\nscikit-learn\tis\tprobably\tthe\tmost\tpopular\tlibrary\tfor\tdoing\tmachine\tlearning\tin\tPython.\tIt\ncontains\tall\tthe\tmodels\twe\u2019ve\timplemented\tand\tmany\tmore\tthat\twe\thaven\u2019t.\tOn\ta\treal\nproblem,\tyou\u2019d\tnever\tbuild\ta\tdecision\ttree\tfrom\tscratch;\tyou\u2019d\tlet\tscikit-learn\tdo\tthe\nheavy\tlifting.\tOn\ta\treal\tproblem,\tyou\u2019d\tnever\twrite\tan\toptimization\talgorithm\tby\thand;\nyou\u2019d\tcount\ton\tscikit-learn\tto\tbe\talready\tusing\ta\treally\tgood\tone.\n\nIts\tdocumentation\tcontains\tmany,\tmany\texamples\tof\twhat\tit\tcan\tdo\t(and,\tmore\tgenerally,\nwhat\tmachine\tlearning\tcan\tdo).",
    "414": "Visualization\n\nThe\tmatplotlib\tcharts\twe\u2019ve\tbeen\tcreating\thave\tbeen\tclean\tand\tfunctional\tbut\tnot\nparticularly\tstylish\t(and\tnot\tat\tall\tinteractive).\tIf\tyou\twant\tto\tget\tdeeper\tinto\tdata\nvisualization,\tyou\thave\tseveral\toptions.\n\nThe\tfirst\tis\tto\tfurther\texplore\tmatplotlib,\tonly\ta\thandful\tof\twhose\tfeatures\twe\u2019ve\tactually\ncovered.\tIts\twebsite\tcontains\tmany\texamples\tof\tits\tfunctionality\tand\ta\tGallery\tof\tsome\tof\nthe\tmore\tinteresting\tones.\tIf\tyou\twant\tto\tcreate\tstatic\tvisualizations\t(say,\tfor\tprinting\tin\ta\nbook),\tthis\tis\tprobably\tyour\tbest\tnext\tstep.\n\nYou\tshould\talso\tcheck\tout\tseaborn,\twhich\tis\ta\tlibrary\tthat\t(among\tother\tthings)\tmakes\nmatplotlib\tmore\tattractive.\n\nIf\tyou\u2019d\tlike\tto\tcreate\tinteractive\tvisualizations\tthat\tyou\tcan\tshare\ton\tthe\tWeb,\tthe\tobvious\nchoice\tis\tprobably\tD3.js,\ta\tJavaScript\tlibrary\tfor\tcreating\t\u201cData\tDriven\tDocuments\u201d\n(those\tare\tthe\tthree\tDs).\tEven\tif\tyou\tdon\u2019t\tknow\tmuch\tJavaScript,\tit\u2019s\toften\tpossible\tto\ncrib\texamples\tfrom\tthe\tD3\tgallery\tand\ttweak\tthem\tto\twork\twith\tyour\tdata.\t(Good\tdata\nscientists\tcopy\tfrom\tthe\tD3\tgallery;\tgreat\tdata\tscientists\tsteal\tfrom\tthe\tD3\tgallery.)\n\nEven\tif\tyou\thave\tno\tinterest\tin\tD3,\tjust\tbrowsing\tthe\tgallery\tis\titself\ta\tpretty\tincredible\neducation\tin\tdata\tvisualization.\n\nBokeh\tis\ta\tproject\tthat\tbrings\tD3-style\tfunctionality\tinto\tPython.",
    "415": "R\n\nAlthough\tyou\tcan\ttotally\tget\taway\twith\tnot\tlearning\tR,\ta\tlot\tof\tdata\tscientists\tand\tdata\nscience\tprojects\tuse\tit,\tso\tit\u2019s\tworth\tgetting\tat\tleast\tfamiliar\twith\tit.\n\nIn\tpart,\tthis\tis\tso\tthat\tyou\tcan\tunderstand\tpeople\u2019s\tR-based\tblog\tposts\tand\texamples\tand\ncode;\tin\tpart,\tthis\tis\tto\thelp\tyou\tbetter\tappreciate\tthe\t(comparatively)\tclean\telegance\tof\nPython;\tand\tin\tpart,\tthis\tis\tto\thelp\tyou\tbe\ta\tmore\tinformed\tparticipant\tin\tthe\tnever-ending\n\u201cR\tversus\tPython\u201d\tflamewars.\n\nThe\tworld\thas\tno\tshortage\tof\tR\ttutorials,\tR\tcourses,\tand\tR\tbooks.\tI\thear\tgood\tthings\tabout\nHands-On\tProgramming\twith\tR,\tand\tnot\tjust\tbecause\tit\u2019s\talso\tan\tO\u2019Reilly\tbook.\t(OK,\nmostly\tbecause\tit\u2019s\talso\tan\tO\u2019Reilly\tbook.)",
    "416": "Find\tData\n\nIf\tyou\u2019re\tdoing\tdata\tscience\tas\tpart\tof\tyour\tjob,\tyou\u2019ll\tmost\tlikely\tget\tthe\tdata\tas\tpart\tof\nyour\tjob\t(although\tnot\tnecessarily).\tWhat\tif\tyou\u2019re\tdoing\tdata\tscience\tfor\tfun?\tData\tis\neverywhere,\tbut\there\tare\tsome\tstarting\tpoints:\n\nData.gov\tis\tthe\tgovernment\u2019s\topen\tdata\tportal.\tIf\tyou\twant\tdata\ton\tanything\tthat\thas\tto\ndo\twith\tthe\tgovernment\t(which\tseems\tto\tbe\tmost\tthings\tthese\tdays)\tit\u2019s\ta\tgood\tplace\tto\nstart.\n\nreddit\thas\ta\tcouple\tof\tforums,\tr/datasets\tand\tr/data,\tthat\tare\tplaces\tto\tboth\task\tfor\tand\ndiscover\tdata.\n\nAmazon.com\tmaintains\ta\tcollection\tof\tpublic\tdata\tsets\tthat\tthey\u2019d\tlike\tyou\tto\tanalyze\nusing\ttheir\tproducts\t(but\tthat\tyou\tcan\tanalyze\twith\twhatever\tproducts\tyou\twant).\n\nRobb\tSeaton\thas\ta\tquirky\tlist\tof\tcurated\tdata\tsets\ton\this\tblog.\n\nKaggle\tis\ta\tsite\tthat\tholds\tdata\tscience\tcompetitions.\tI\tnever\tmanaged\tto\tget\tinto\tit\t(I\ndon\u2019t\thave\tmuch\tof\ta\tcompetitive\tnature\twhen\tit\tcomes\tto\tdata\tscience),\tbut\tyou\tmight.",
    "417": "Do\tData\tScience\n\nLooking\tthrough\tdata\tcatalogs\tis\tfine,\tbut\tthe\tbest\tprojects\t(and\tproducts)\tare\tones\tthat\ntickle\tsome\tsort\tof\titch.\tHere\tare\ta\tfew\tthat\tI\u2019ve\tdone.",
    "418": "Hacker\tNews\n\nHacker\tNews\tis\ta\tnews\taggregation\tand\tdiscussion\tsite\tfor\ttechnology-related\tnews.\tIt\ncollects\tlots\tand\tlots\tof\tarticles,\tmany\tof\twhich\taren\u2019t\tinteresting\tto\tme.\n\nAccordingly,\tseveral\tyears\tago,\tI\tset\tout\tto\tbuild\ta\tHacker\tNews\tstory\tclassifier\tto\tpredict\nwhether\tI\twould\tor\twould\tnot\tbe\tinterested\tin\tany\tgiven\tstory.\tThis\tdid\tnot\tgo\tover\tso\twell\nwith\tthe\tusers\tof\tHacker\tNews,\twho\tresented\tthe\tidea\tthat\tsomeone\tmight\tnot\tbe\ninterested\tin\tevery\tstory\ton\tthe\tsite.\n\nThis\tinvolved\thand-labeling\ta\tlot\tof\tstories\t(in\torder\tto\thave\ta\ttraining\tset),\tchoosing\tstory\nfeatures\t(for\texample,\twords\tin\tthe\ttitle,\tand\tdomains\tof\tthe\tlinks),\tand\ttraining\ta\tNaive\nBayes\tclassifier\tnot\tunlike\tour\tspam\tfilter.\n\nFor\treasons\tnow\tlost\tto\thistory,\tI\tbuilt\tit\tin\tRuby.\tLearn\tfrom\tmy\tmistakes.",
    "419": "Fire\tTrucks\n\nI\tlive\ton\ta\tmajor\tstreet\tin\tdowntown\tSeattle,\thalfway\tbetween\ta\tfire\tstation\tand\tmost\tof\nthe\tcity\u2019s\tfires\t(or\tso\tit\tseems).\tAccordingly,\tover\tthe\tyears,\tI\thave\tdeveloped\ta\nrecreational\tinterest\tin\tthe\tSeattle\tFire\tDepartment.\n\nLuckily\t(from\ta\tdata\tperspective)\tthey\tmaintain\ta\tRealtime\t911\tsite\tthat\tlists\tevery\tfire\nalarm\talong\twith\tthe\tfire\ttrucks\tinvolved.\n\nAnd\tso,\tto\tindulge\tmy\tinterest,\tI\tscraped\tmany\tyears\u2019\tworth\tof\tfire\talarm\tdata\tand\nperformed\ta\tsocial\tnetwork\tanalysis\tof\tthe\tfire\ttrucks.\tAmong\tother\tthings,\tthis\trequired\nme\tto\tinvent\ta\tfire-truck-specific\tnotion\tof\tcentrality,\twhich\tI\tcalled\tTruckRank.",
    "420": "T-shirts\n\nI\thave\ta\tyoung\tdaughter,\tand\tan\tincessant\tsource\tof\tfrustration\tto\tme\tthroughout\ther\nchildhood\thas\tbeen\tthat\tmost\t\u201cgirls\tshirts\u201d\tare\tquite\tboring,\twhile\tmany\t\u201cboys\tshirts\u201d\tare\na\tlot\tof\tfun.\n\nIn\tparticular,\tit\tfelt\tclear\tto\tme\tthat\tthere\twas\ta\tdistinct\tdifference\tbetween\tthe\tshirts\nmarketed\tto\ttoddler\tboys\tand\ttoddler\tgirls.\tAnd\tso\tI\tasked\tmyself\tif\tI\tcould\ttrain\ta\tmodel\nto\trecognize\tthese\tdifferences.\n\nSpoiler:\tI\tcould.\n\nThis\tinvolved\tdownloading\tthe\timages\tof\thundreds\tof\tshirts,\tshrinking\tthem\tall\tto\tthe\nsame\tsize,\tturning\tthem\tinto\tvectors\tof\tpixel\tcolors,\tand\tusing\tlogistic\tregression\tto\tbuild\na\tclassifier.\n\nOne\tapproach\tlooked\tsimply\tat\twhich\tcolors\twere\tpresent\tin\teach\tshirt;\ta\tsecond\tfound\nthe\tfirst\t10\tprincipal\tcomponents\tof\tthe\tshirt\timage\tvectors\tand\tclassified\teach\tshirt\tusing\nits\tprojections\tinto\tthe\t10-dimensional\tspace\tspanned\tby\tthe\t\u201ceigenshirts\u201d\t(Figure\t25-2).\n\nFigure\t25-2.\tEigenshirts\tcorresponding\tto\tthe\tfirst\tprincipal\tcomponent",
    "421": "And\tYou?\n\nWhat\tinterests\tyou?\tWhat\tquestions\tkeep\tyou\tup\tat\tnight?\tLook\tfor\ta\tdata\tset\t(or\tscrape\nsome\twebsites)\tand\tdo\tsome\tdata\tscience.\n\nLet\tme\tknow\twhat\tyou\tfind!\tEmail\tme\tat\tjoelgrus@gmail.com\tor\tfind\tme\ton\tTwitter\tat\n@joelgrus.",
    "422": "",
    "423": "Index\n\nA\n\nA/B\ttest,\tExample:\tRunning\tan\tA/B\tTest\n\naccuracy,\tCorrectness\n\nof\tmodel\tperformance,\tCorrectness\n\nall\tfunction\t(Python),\tTruthiness\n\nAnaconda\tdistribution\tof\tPython,\tGetting\tPython\n\nany\tfunction\t(Python),\tTruthiness\n\nAPIs,\tusing\tto\tget\tdata,\tUsing\tAPIs-Using\tTwython\n\nexample,\tusing\tTwitter\tAPIs,\tExample:\tUsing\tthe\tTwitter\tAPIs-Using\tTwython\n\ngetting\tcredentials,\tGetting\tCredentials\n\nusing\ttwython,\tUsing\tTwython\n\nfinding\tAPIs,\tFinding\tAPIs\n\nJSON\t(and\tXML),\tJSON\t(and\tXML)\n\nunauthenticated\tAPI,\tUsing\tan\tUnauthenticated\tAPI\n\nargs\tand\tkwargs\t(Python),\targs\tand\tkwargs\n\nargument\tunpacking,\tzip\tand\tArgument\tUnpacking\n\narithmetic\n\nin\tPython,\tArithmetic\n\nperforming\ton\tvectors,\tVectors\n\nartificial\tneural\tnetworks,\tNeural\tNetworks\n\n(see\talso\tneural\tnetworks)\n\nassignment,\tmultiple,\tin\tPython,\tTuples",
    "424": "B\n\nbackpropagation,\tBackpropagation\n\nbagging,\tRandom\tForests\n\nbar\tcharts,\tBar\tCharts-Line\tCharts\n\nBayes\u2019s\tTheorem,\tBayes\u2019s\tTheorem,\tA\tReally\tDumb\tSpam\tFilter\n\nBayesian\tInference,\tBayesian\tInference\n\nBeautiful\tSoup\tlibrary,\tHTML\tand\tthe\tParsing\tThereof,\tn-gram\tModels\n\nusing\twith\tXML\tdata,\tJSON\t(and\tXML)\n\nBernoulli\ttrial,\tExample:\tFlipping\ta\tCoin\n\nBeta\tdistributions,\tBayesian\tInference\n\nbetweenness\tcentrality,\tBetweenness\tCentrality-Betweenness\tCentrality\n\nbias,\tThe\tBias-Variance\tTrade-off\n\nadditional\tdata\tand,\tThe\tBias-Variance\tTrade-off\n\nbigram\tmodel,\tn-gram\tModels\n\nbinary\trelationships,\trepresenting\twith\tmatrices,\tMatrices\n\nbinomial\trandom\tvariables,\tThe\tCentral\tLimit\tTheorem,\tExample:\tFlipping\ta\tCoin\n\nBokeh\tproject,\tVisualization\n\nbooleans\t(Python),\tTruthiness\n\nbootstrap\taggregating,\tRandom\tForests\n\nbootstrapping\tdata,\tDigression:\tThe\tBootstrap\n\nbottom-up\thierarchical\tclustering,\tBottom-up\tHierarchical\tClustering-Bottom-up\nHierarchical\tClustering\n\nbreak\tstatement\t(Python),\tControl\tFlow\n\nbuckets,\tgrouping\tdata\tinto,\tExploring\tOne-Dimensional\tData",
    "425": "business\tmodels,\tModeling\n\nC\n\nCAPTCHA,\tdefeating\twith\ta\tneural\tnetwork,\tExample:\tDefeating\ta\tCAPTCHA-\nExample:\tDefeating\ta\tCAPTCHA\n\ncausation,\tcorrelation\tand,\tCorrelation\tand\tCausation,\tThe\tModel\n\ncdf\t(see\tcumulative\tdistribtion\tfunction)\n\ncentral\tlimit\ttheorem,\tThe\tCentral\tLimit\tTheorem,\tConfidence\tIntervals\n\ncentral\ttendencies\n\nmean,\tCentral\tTendencies\n\nmedian,\tCentral\tTendencies\n\nmode,\tCentral\tTendencies\n\nquantile,\tCentral\tTendencies\n\ncentrality\n\nbetweenness,\tBetweenness\tCentrality-Betweenness\tCentrality\n\ncloseness,\tBetweenness\tCentrality\n\ndegree,\tFinding\tKey\tConnectors,\tBetweenness\tCentrality\n\neigenvector,\tEigenvector\tCentrality-Centrality\n\nclasses\t(Python),\tObject-Oriented\tProgramming\n\nclassification\ttrees,\tWhat\tIs\ta\tDecision\tTree?\n\ncloseness\tcentrality,\tBetweenness\tCentrality\n\nclustering,\tClustering-For\tFurther\tExploration\n\nbottom-up\thierarchical\tclustering,\tBottom-up\tHierarchical\tClustering-Bottom-up\nHierarchical\tClustering\n\nchoosing\tk,\tChoosing\tk",
    "426": "example,\tclustering\tcolors,\tExample:\tClustering\tColors\n\nexample,\tmeetups,\tExample:\tMeetups-Example:\tMeetups\n\nk-means\tclustering,\tThe\tModel\n\nclusters,\tRescaling,\tThe\tIdea\n\ndistance\tbetween,\tBottom-up\tHierarchical\tClustering\n\ncode\texamples\tfrom\tthis\tbook,\tUsing\tCode\tExamples\n\ncoefficient\tof\tdetermination,\tThe\tModel\n\ncombiners\t(in\tMapReduce),\tAn\tAside:\tCombiners\n\ncomma-separated\tvalues\tfiles,\tDelimited\tFiles\n\ncleaning\tcomma-delimited\tstock\tprices,\tCleaning\tand\tMunging\n\ncommand\tline,\trunning\tPython\tscripts\tat,\tstdin\tand\tstdout\n\nconditional\tprobability,\tConditional\tProbability\n\nrandom\tvariables\tand,\tRandom\tVariables\n\nconfidence\tintervals,\tConfidence\tIntervals\n\nconfounding\tvariables,\tSimpson\u2019s\tParadox\n\nconfusion\tmatrix,\tCorrectness\n\ncontinue\tstatement\t(Python),\tControl\tFlow\n\ncontinuity\tcorrection,\tExample:\tFlipping\ta\tCoin\n\ncontinuous\tdistributions,\tContinuous\tDistributions\n\ncontrol\tflow\t(in\tPython),\tControl\tFlow\n\ncorrectness,\tCorrectness\n\ncorrelation,\tCorrelation\n\nand\tcausation,\tCorrelation\tand\tCausation",
    "427": "in\tsimple\tlinear\tregression,\tThe\tModel\n\nother\tcaveats,\tSome\tOther\tCorrelational\tCaveats\n\noutliers\tand,\tCorrelation\n\nSimpson\u2019s\tParadox\tand,\tSimpson\u2019s\tParadox\n\ncorrelation\tfunction,\tSimple\tLinear\tRegression\n\ncosine\tsimilarity,\tUser-Based\tCollaborative\tFiltering,\tItem-Based\tCollaborative\nFiltering\n\nCounter\t(Python),\tCounter\n\ncovariance,\tCorrelation\n\nCREATE\tTABLE\tstatement\t(SQL),\tCREATE\tTABLE\tand\tINSERT\n\ncumulative\tdistribution\tfunction\t(cdf),\tContinuous\tDistributions\n\ncurrying\t(Python),\tFunctional\tTools\n\ncurse\tof\tdimensionality,\tThe\tCurse\tof\tDimensionality-The\tCurse\tof\tDimensionality,\nUser-Based\tCollaborative\tFiltering\n\nD\n\nD3.js\tlibrary,\tVisualization\n\ndata\n\ncleaning\tand\tmunging,\tCleaning\tand\tMunging\n\nexploring,\tExploring\tYour\tData-Many\tDimensions\n\nfinding,\tFind\tData\n\ngetting,\tGetting\tData-For\tFurther\tExploration\n\nreading\tfiles,\tReading\tFiles-Delimited\tFiles\n\nscraping\tfrom\tweb\tpages,\tScraping\tthe\tWeb-Example:\tO\u2019Reilly\tBooks\tAbout\nData",
    "428": "using\tAPIs,\tUsing\tAPIs-Using\tTwython\n\nusing\tstdin\tand\tstdout,\tstdin\tand\tstdout\n\nmanipulating,\tManipulating\tData-Manipulating\tData\n\nrescaling,\tRescaling-Rescaling\n\ndata\tmining,\tWhat\tIs\tMachine\tLearning?\n\ndata\tscience\n\nabout,\tData\tScience\n\ndefined,\tWhat\tIs\tData\tScience?\n\ndoing,\tprojects\tof\tthe\tauthor,\tDo\tData\tScience\n\nfrom\tscratch,\tFrom\tScratch\n\nlearning\tmore\tabout,\tGo\tForth\tand\tDo\tData\tScience-And\tYou?\n\nskills\tneeded\tfor,\tData\tScience\n\nusing\tlibraries,\tNot\tfrom\tScratch\n\ndata\tvisualization,\tVisualizing\tData-For\tFurther\tExploration\n\nbar\tcharts,\tBar\tCharts-Line\tCharts\n\nfurther\texploration\tof,\tVisualization\n\nline\tcharts,\tLine\tCharts\n\nmatplotlib,\tmatplotlib\n\nscatterplots,\tScatterplots-Scatterplots\n\ndatabases\tand\tSQL,\tDatabases\tand\tSQL-For\tFurther\tExploration\n\nCREATE\tTABLE\tand\tINSERT\tstatements,\tCREATE\tTABLE\tand\tINSERT-\nUPDATE\n\nDELETE\tstatement,\tDELETE",
    "429": "GROUP\tBY\tstatement,\tGROUP\tBY-GROUP\tBY\n\nJOIN\tstatement,\tJOIN\n\nNoSQL,\tNoSQL\n\nORDER\tBY\tstatement,\tORDER\tBY\n\nquery\toptimization,\tQuery\tOptimization\n\nSELECT\tstatement,\tSELECT-SELECT\n\nsubqueries,\tSubqueries\n\nUPDATE\tstatement,\tUPDATE\n\ndecision\ttrees,\tDecision\tTrees-For\tFurther\tExploration\n\ncreating,\tCreating\ta\tDecision\tTree\n\ndefined,\tWhat\tIs\ta\tDecision\tTree?\n\nentropy,\tEntropy\n\nentropy\tof\ta\tpartition,\tThe\tEntropy\tof\ta\tPartition\n\nhiring\ttree\timplementation\t(example),\tPutting\tIt\tAll\tTogether\n\nrandom\tforests,\tRandom\tForests\n\ndegree\tcentrality,\tFinding\tKey\tConnectors,\tBetweenness\tCentrality\n\nDELETE\tstatement\t(SQL),\tDELETE\n\ndelimited\tfiles,\tDelimited\tFiles\n\ndependence,\tDependence\tand\tIndependence\n\nderivatives,\tapproximating\twith\tdifference\tquotients,\tEstimating\tthe\tGradient\n\ndictionaries\t(Python),\tDictionaries\n\ndefaultdict,\tdefaultdict\n\nitems\tand\titeritems\tmethods,\tGenerators\tand\tIterators",
    "430": "dimensionality\treduction,\tDimensionality\tReduction-Dimensionality\tReduction\n\nusing\tprincipal\tcomponent\tanalysis,\tDimensionality\tReduction\n\ndimensionality,\tcurse\tof,\tThe\tCurse\tof\tDimensionality-The\tCurse\tof\tDimensionality,\nUser-Based\tCollaborative\tFiltering\n\ndiscrete\tdistribution,\tContinuous\tDistributions\n\ndispersion,\tDispersion\n\nrange,\tDispersion\n\nstandard\tdeviation,\tDispersion\n\nvariance,\tDispersion\n\ndistance,\tThe\tModel\n\n(see\talso\tnearest\tneighbors\tclassification)\n\nbetween\tclusters,\tBottom-up\tHierarchical\tClustering\n\ndistance\tfunction,\tRescaling,\tThe\tModel\n\ndistribution\n\nbernoulli,\tThe\tCentral\tLimit\tTheorem,\tExample:\tFlipping\ta\tCoin\n\nbeta,\tBayesian\tInference\n\nbinomial,\tThe\tCentral\tLimit\tTheorem,\tExample:\tFlipping\ta\tCoin\n\ncontinuous,\tContinuous\tDistributions\n\nnormal,\tThe\tNormal\tDistribution\n\ndot\tproduct,\tVectors,\tMatrix\tMultiplication\n\ndummy\tvariables,\tMultiple\tRegression\n\nE\n\nedges,\tNetwork\tAnalysis\n\neigenshirts\tproject,\tT-shirts",
    "431": "eigenvector\tcentrality,\tEigenvector\tCentrality-Centrality\n\nensemble\tlearning,\tRandom\tForests\n\nentropy,\tEntropy\n\nof\ta\tpartition,\tThe\tEntropy\tof\ta\tPartition\n\nenumerate\tfunction\t(Python),\tenumerate\n\nerrors\n\nin\tclustering,\tChoosing\tk\n\nin\tmultiple\tlinear\tregression\tmodel,\tFurther\tAssumptions\tof\tthe\tLeast\tSquares\nModel\n\nin\tsimple\tlinear\tregression\tmodel,\tThe\tModel,\tMaximum\tLikelihood\tEstimation\n\nminimizing\tin\tmodels,\tGradient\tDescent-For\tFurther\tExploration\n\nstandard\terrors\tof\tregression\tcoefficients,\tStandard\tErrors\tof\tRegression\nCoefficients-Standard\tErrors\tof\tRegression\tCoefficients\n\nEuclidean\tdistance\tfunction,\tRescaling\n\nexceptions\tin\tPython,\tExceptions\n\nexperience\toptimization,\tExample:\tRunning\tan\tA/B\tTest\n\nF\n\nF1\tscore,\tCorrectness\n\nfalse\tpositives,\tExample:\tFlipping\ta\tCoin\n\nfarness,\tBetweenness\tCentrality\n\nfeatures,\tFeature\tExtraction\tand\tSelection\n\nchoosing,\tFeature\tExtraction\tand\tSelection\n\nextracting,\tFeature\tExtraction\tand\tSelection\n\nfeed-forward\tneural\tnetworks,\tFeed-Forward\tNeural\tNetworks",
    "432": "files,\treading,\tReading\tFiles\n\ndelimited\tfiles,\tDelimited\tFiles\n\ntext\tfiles,\tThe\tBasics\tof\tText\tFiles\n\nfilter\tfunction\t(Python),\tFunctional\tTools\n\nfire\ttrucks\tproject,\tFire\tTrucks\n\nfor\tcomprehensions\t(Python),\tGenerators\tand\tIterators\n\nfor\tloops\t(Python),\tControl\tFlow\n\nin\tlist\tcomprehensions,\tList\tComprehensions\n\nfull\touter\tjoins,\tJOIN\n\nfunctions\t(Python),\tFunctions\n\nG\n\ngenerators\t(Python),\tGenerators\tand\tIterators\n\ngetting\tdata\t(see\tdata,\tgetting)\n\nGibbs\tsampling,\tAn\tAside:\tGibbs\tSampling-An\tAside:\tGibbs\tSampling\n\nGithub\u2019s\tAPI,\tUsing\tan\tUnauthenticated\tAPI\n\ngradient,\tThe\tIdea\tBehind\tGradient\tDescent\n\ngradient\tdescent,\tGradient\tDescent-For\tFurther\tExploration\n\nchoosing\tthe\tright\tstep\tsize,\tChoosing\tthe\tRight\tStep\tSize\n\nestimating\tthe\tgradient,\tEstimating\tthe\tGradient\n\nexample,\tminimize_batch\tfunction,\tPutting\tIt\tAll\tTogether\n\nstochastic,\tStochastic\tGradient\tDescent\n\nusing\tfor\tmultiple\tregression\tmodel,\tFitting\tthe\tModel\n\nusing\tin\tsimple\tlinear\tregression,\tUsing\tGradient\tDescent",
    "433": "grammars,\tGrammars-Grammars\n\ngreedy\talgorithms,\tCreating\ta\tDecision\tTree\n\nGROUP\tBY\tstatement\t(SQL),\tGROUP\tBY-GROUP\tBY\n\nH\n\nHacker\tNews,\tHacker\tNews\n\nharmonic\tmean,\tCorrectness\n\nhierarchical\tclustering,\tBottom-up\tHierarchical\tClustering-Bottom-up\tHierarchical\nClustering\n\nhistograms\n\nof\tfriend\tcounts\t(example),\tDescribing\ta\tSingle\tSet\tof\tData\n\nplotting\tusing\tbar\tcharts,\tBar\tCharts\n\nHTML,\tparsing,\tHTML\tand\tthe\tParsing\tThereof\n\nexample,\tO\u2019Reilly\tbooks\tabout\tdata,\tExample:\tO\u2019Reilly\tBooks\tAbout\tData-\nExample:\tO\u2019Reilly\tBooks\tAbout\tData\n\nusing\tBeautiful\tSoup\tlibrary,\tHTML\tand\tthe\tParsing\tThereof\n\nhypotheses,\tHypothesis\tand\tInference\n\nhypothesis\ttesting,\tStatistical\tHypothesis\tTesting\n\nexample,\tan\tA/B\ttest,\tExample:\tRunning\tan\tA/B\tTest\n\nexample,\tflipping\ta\tcoin,\tExample:\tFlipping\ta\tCoin-Example:\tFlipping\ta\tCoin\n\np-hacking,\tP-hacking\n\nregression\tcoefficients,\tStandard\tErrors\tof\tRegression\tCoefficients-Standard\nErrors\tof\tRegression\tCoefficients\n\nusing\tconfidence\tintervals,\tConfidence\tIntervals\n\nusing\tp-values,\tExample:\tFlipping\ta\tCoin\n\nI",
    "434": "if\tstatements\t(Python),\tControl\tFlow\n\nif-then-else\tstatements\t(Python),\tControl\tFlow\n\nin\toperator\t(Python),\tLists,\tDictionaries\n\nin\tfor\tloops,\tControl\tFlow\n\nusing\ton\tsets,\tSets\n\nindependence,\tDependence\tand\tIndependence\n\nindexes\t(database\ttables),\tIndexes\n\ninference\n\nBayesian\tInference,\tBayesian\tInference\n\nstatistical,\tin\tA/B\ttest,\tExample:\tRunning\tan\tA/B\tTest\n\ninner\tjoins,\tJOIN\n\nINSERT\tstatement\t(SQL),\tCREATE\tTABLE\tand\tINSERT\n\ninteractive\tvisualizations,\tVisualization\n\ninverse\tnormal\tcumulative\tdistribution\tfunction,\tThe\tNormal\tDistribution\n\nIPython,\tGetting\tPython,\tIPython\n\nitem-based\tcollaborative\tfiltering,\tItem-Based\tCollaborative\tFiltering-For\tFurther\nExploration\n\nJ\n\nJavaScript,\tD3.js\tlibrary,\tVisualization\n\nJOIN\tstatement\t(SQL),\tJOIN\n\nJSON\t(JavaScript\tObject\tNotation),\tJSON\t(and\tXML)\n\nK\n\nk-means\tclustering,\tThe\tModel\n\nchoosing\tk,\tChoosing\tk",
    "435": "k-nearest\tneighbors\tclassification\t(see\tnearest\tneighbors\tclassification)\n\nkernel\ttrick,\tSupport\tVector\tMachines\n\nkey/value\tpairs\t(in\tPython\tdictionaries),\tDictionaries\n\nkwargs\t(Python),\targs\tand\tkwargs\n\nL\n\nLasso\tregression,\tRegularization\n\nLatent\tDirichlet\tAnalysis\t(LDA),\tTopic\tModeling\n\nlayers\t(neural\tnetwork),\tFeed-Forward\tNeural\tNetworks\n\nleast\tsquares\tmodel\n\nassumptions,\tFurther\tAssumptions\tof\tthe\tLeast\tSquares\tModel\n\nin\tsimple\tlinear\tregression,\tThe\tModel\n\nleft\tjoins,\tJOIN\n\nlikelihood,\tMaximum\tLikelihood\tEstimation,\tThe\tLogistic\tFunction\n\nline\tcharts\n\ncreating\twith\tmatplotlib,\tmatplotlib\n\nshowing\ttrends,\tLine\tCharts\n\nlinear\talgebra,\tLinear\tAlgebra-For\tFurther\tExploration,\tMathematics\n\nmatrices,\tMatrices-Matrices\n\nvectors,\tVectors-Vectors\n\nlinear\tregression\n\nmultiple,\tMultiple\tRegression-For\tFurther\tExploration\n\nassumptions\tof\tleast\tsquares\tmodel,\tFurther\tAssumptions\tof\tthe\tLeast\tSquares\nModel\n\nbootstrapping\tnew\tdata\tsets,\tDigression:\tThe\tBootstrap",
    "436": "goodness\tof\tfit,\tGoodness\tof\tFit\n\ninterpreting\tthe\tmodel,\tInterpreting\tthe\tModel\n\nmodel,\tThe\tModel\n\nregularization,\tRegularization\n\nstandard\terrors\tof\tregression\tcoefficients,\tStandard\tErrors\tof\tRegression\nCoefficients-Standard\tErrors\tof\tRegression\tCoefficients\n\nsimple,\tSimple\tLinear\tRegression-For\tFurther\tExploration\n\nmaximum\tlikelihood\testimation,\tMaximum\tLikelihood\tEstimation\n\nmodel,\tThe\tModel\n\nusing\tgradient\tdescent,\tUsing\tGradient\tDescent\n\nusing\tto\tpredict\tpaid\taccounts,\tThe\tProblem\n\nlist\tcomprehensions\t(Python),\tList\tComprehensions\n\nlists\t(in\tPython),\tLists\n\nrepresenting\tmatrices\tas,\tMatrices\n\nsort\tmethod,\tSorting\n\nusing\tto\trepresent\tvectors,\tVectors\n\nzipping\tand\tunzipping,\tzip\tand\tArgument\tUnpacking\n\nlog\tlikelihood,\tThe\tLogistic\tFunction\n\nlogistic\tregression,\tLogistic\tRegression-For\tFurther\tInvestigation\n\napplying\tthe\tmodel,\tApplying\tthe\tModel\n\ngoodness\tof\tfit,\tGoodness\tof\tFit\n\nlogistic\tfunction,\tThe\tLogistic\tFunction\n\nproblem,\tpredicting\tpaid\tuser\taccounts,\tThe\tProblem\n\nM",
    "437": "machine\tlearning,\tMachine\tLearning-For\tFurther\tExploration\n\nbias-variance\ttrade-off,\tThe\tBias-Variance\tTrade-off\n\ncorrectness,\tCorrectness\n\ndefined,\tWhat\tIs\tMachine\tLearning?\n\nfeature\textraction\tand\tselection,\tFeature\tExtraction\tand\tSelection\n\nmodeling\tdata,\tModeling\n\noverfitting\tand\tunderfitting,\tOverfitting\tand\tUnderfitting\n\nscikit-learn\tlibrary\tfor,\tscikit-learn\n\nmagnitude\tof\ta\tvector,\tVectors\n\nmanipulating\tdata,\tManipulating\tData-Manipulating\tData\n\nmap\tfunction\t(Python),\tFunctional\tTools\n\nMapReduce,\tMapReduce-For\tFurther\tExploration\n\nbasic\talgorithm,\tMapReduce\n\nbenefits\tof\tusing,\tWhy\tMapReduce?\n\ncombiners,\tAn\tAside:\tCombiners\n\nexample,\tanalyzing\tstatus\tupdates,\tExample:\tAnalyzing\tStatus\tUpdates\n\nexample,\tmatrix\tmultiplication,\tExample:\tMatrix\tMultiplication-Example:\tMatrix\nMultiplication\n\nexample,\tword\tcount,\tExample:\tWord\tCount-Why\tMapReduce?\n\nmath.erf\tfunction\t(Python),\tThe\tNormal\tDistribution\n\nmatplotlib,\tmatplotlib,\tVisualization\n\nmatrices,\tMatrices-Matrices\n\nimportance\tof,\tMatrices",
    "438": "matrix\tmultiplication,\tMatrix\tMultiplication\n\nusing\tMapReduce,\tExample:\tMatrix\tMultiplication-Example:\tMatrix\nMultiplication\n\nscatterplot\tmatrix,\tMany\tDimensions\n\nmaximum\tlikelihood\testimation,\tMaximum\tLikelihood\tEstimation\n\nmaximum,\tfinding\tusing\tgradient\tdescent,\tThe\tIdea\tBehind\tGradient\tDescent,\nPutting\tIt\tAll\tTogether\n\nmean\n\ncomputing,\tCentral\tTendencies\n\nremoving\tfrom\tPCA\tdata,\tDimensionality\tReduction\n\nmedian,\tCentral\tTendencies\n\nmeetups\t(example),\tExample:\tMeetups-Example:\tMeetups\n\nmember\tfunctions,\tObject-Oriented\tProgramming\n\nmerged\tclusters,\tBottom-up\tHierarchical\tClustering\n\nminimum,\tfinding\tusing\tgradient\tdescent,\tThe\tIdea\tBehind\tGradient\tDescent\n\nmode,\tCentral\tTendencies\n\nmodels,\tModeling\n\nbias-variance\ttrade-off,\tThe\tBias-Variance\tTrade-off\n\nin\tmachine\tlearning,\tWhat\tIs\tMachine\tLearning?\n\nmodules\t(Python),\tModules\n\nmultiple\tassignment\t(Python),\tTuples\n\nN\n\nn-gram\tmodels,\tn-gram\tModels-n-gram\tModels\n\nbigram,\tn-gram\tModels",
    "439": "trigrams,\tn-gram\tModels\n\nn-grams,\tn-gram\tModels\n\nNaive\tBayes\talgorithm,\tNaive\tBayes-For\tFurther\tExploration\n\nexample,\tfiltering\tspam,\tA\tReally\tDumb\tSpam\tFilter-A\tMore\tSophisticated\tSpam\nFilter\n\nimplementation,\tImplementation\n\nnatural\tlanguage\tprocessing\t(NLP),\tNatural\tLanguage\tProcessing-For\tFurther\nExploration\n\ngrammars,\tGrammars-Grammars\n\ntopic\tmodeling,\tTopic\tModeling-Topic\tModeling\n\ntopics\tof\tinterest,\tfinding,\tTopics\tof\tInterest\n\nword\tclouds,\tWord\tClouds-Word\tClouds\n\nnearest\tneighbors\tclassification,\tk-Nearest\tNeighbors-For\tFurther\tExploration\n\ncurse\tof\tdimensionality,\tThe\tCurse\tof\tDimensionality-The\tCurse\tof\tDimensionality\n\nexample,\tfavorite\tprogramming\tlanguages,\tExample:\tFavorite\tLanguages-\nExample:\tFavorite\tLanguages\n\nmodel,\tThe\tModel\n\nnetwork\tanalysis,\tNetwork\tAnalysis-For\tFurther\tExploration\n\nbetweenness\tcentrality,\tBetweenness\tCentrality-Betweenness\tCentrality\n\ncloseness\tcentrality,\tBetweenness\tCentrality\n\ndegree\tcentrality,\tFinding\tKey\tConnectors,\tBetweenness\tCentrality\n\ndirected\tgraphs\tand\tPageRank,\tDirected\tGraphs\tand\tPageRank-Directed\tGraphs\nand\tPageRank\n\neigenvector\tcentrality,\tEigenvector\tCentrality-Centrality\n\nnetworks,\tNetwork\tAnalysis",
    "440": "neural\tnetworks,\tNeural\tNetworks-For\tFurther\tExploration\n\nbackpropagation,\tBackpropagation\n\nexample,\tdefeating\ta\tCAPTCHA,\tExample:\tDefeating\ta\tCAPTCHA-Example:\nDefeating\ta\tCAPTCHA\n\nfeed-forward,\tFeed-Forward\tNeural\tNetworks\n\nperceptrons,\tPerceptrons\n\nneurons,\tNeural\tNetworks\n\nNLP\t(see\tnatural\tlanguage\tprocessing)\n\nnodes,\tNetwork\tAnalysis\n\nnoise,\tRescaling\n\nin\tmachine\tlearning,\tOverfitting\tand\tUnderfitting\n\nNone\t(Python),\tTruthiness\n\nnormal\tdistribution,\tThe\tNormal\tDistribution\n\nand\tp-value\tcomputation,\tExample:\tFlipping\ta\tCoin\n\ncentral\tlimit\ttheorem\tand,\tThe\tCentral\tLimit\tTheorem\n\nin\tcoin\tflip\texample,\tExample:\tFlipping\ta\tCoin\n\nstandard,\tThe\tNormal\tDistribution\n\nnormalized\ttables,\tJOIN\n\nNoSQL\tdatabases,\tNoSQL\n\nNotQuiteABase,\tDatabases\tand\tSQL\n\nnull\thypothesis,\tStatistical\tHypothesis\tTesting\n\ntesting\tin\tA/B\ttest,\tExample:\tRunning\tan\tA/B\tTest\n\nNumPy,\tNumPy\n\nO",
    "441": "one-sided\ttests,\tExample:\tFlipping\ta\tCoin\n\nORDER\tBY\tstatement\t(SQL),\tORDER\tBY\n\noverfitting,\tOverfitting\tand\tUnderfitting,\tThe\tBias-Variance\tTrade-off\n\nP\n\np-hacking,\tP-hacking\n\np-values,\tExample:\tFlipping\ta\tCoin\n\nPageRank\talgorithm,\tDirected\tGraphs\tand\tPageRank\n\npaid\taccounts,\tpredicting,\tPaid\tAccounts\n\npandas,\tFor\tFurther\tExploration,\tFor\tFurther\tExploration,\tpandas\n\nparameterized\tmodels,\tWhat\tIs\tMachine\tLearning?\n\nparameters,\tprobability\tjudgments\tabout,\tBayesian\tInference\n\npartial\tderivatives,\tEstimating\tthe\tGradient\n\npartial\tfunctions\t(Python),\tFunctional\tTools\n\nPCA\t(see\tprincipal\tcomponent\tanalysis)\n\nperceptrons,\tPerceptrons\n\npip\t(Python\tpackage\tmanager),\tGetting\tPython\n\npipe\toperator\t(|),\tstdin\tand\tstdout\n\npiping\tdata\tthrough\tPython\tscripts,\tstdin\tand\tstdout\n\nposterior\tdistributions,\tBayesian\tInference\n\nprecision\tand\trecall,\tCorrectness\n\npredicate\tfunctions,\tDELETE\n\npredictive\tmodeling,\tWhat\tIs\tMachine\tLearning?\n\nprincipal\tcomponent\tanalysis,\tDimensionality\tReduction",
    "442": "probability,\tProbability-For\tFurther\tExploration,\tMathematics\n\nBayes\u2019s\tTheorem,\tBayes\u2019s\tTheorem\n\ncentral\tlimit\ttheorem,\tThe\tCentral\tLimit\tTheorem\n\nconditional,\tConditional\tProbability\n\ncontinuous\tdistributions,\tContinuous\tDistributions\n\ndefined,\tProbability\n\ndependence\tand\tindependence,\tDependence\tand\tIndependence\n\nnormal\tdistribution,\tThe\tNormal\tDistribution\n\nrandom\tvariables,\tRandom\tVariables\n\nprobability\tdensity\tfunction,\tContinuous\tDistributions\n\nprogramming\tlanguages\tfor\tlearning\tdata\tscience,\tFrom\tScratch\n\nPython,\tA\tCrash\tCourse\tin\tPython-For\tFurther\tExploration\n\nargs\tand\tkwargs,\targs\tand\tkwargs\n\narithmetic,\tArithmetic\n\nbenefits\tof\tusing\tfor\tdata\tscience,\tFrom\tScratch\n\nBooleans,\tTruthiness\n\ncontrol\tflow,\tControl\tFlow\n\nCounter,\tCounter\n\ndictionaries,\tDictionaries-defaultdict\n\nenumerate\tfunction,\tenumerate\n\nexceptions,\tExceptions\n\nfunctional\ttools,\tFunctional\tTools\n\nfunctions,\tFunctions",
    "443": "generators\tand\titerators,\tGenerators\tand\tIterators\n\nlist\tcomprehensions,\tList\tComprehensions\n\nlists,\tLists\n\nobject-oriented\tprogramming,\tObject-Oriented\tProgramming\n\npiping\tdata\tthrough\tscripts\tusing\tstdin\tand\tstdout,\tstdin\tand\tstdout\n\nrandom\tnumbers,\tgenerating,\tRandomness\n\nregular\texpressions,\tRegular\tExpressions\n\nsets,\tSets\n\nsorting\tin,\tThe\tNot-So-Basics\n\nstrings,\tStrings\n\ntuples,\tTuples\n\nwhitespace\tformatting,\tWhitespace\tFormatting\n\nzip\tfunction\tand\targument\tunpacking,\tzip\tand\tArgument\tUnpacking\n\nQ\n\nquantile,\tcomputing,\tCentral\tTendencies\n\nquery\toptimization\t(SQL),\tQuery\tOptimization\n\nR\n\nR\t(programming\tlanguage),\tFrom\tScratch,\tR\n\nrandom\tforests,\tRandom\tForests\n\nrandom\tmodule\t(Python),\tRandomness\n\nrandom\tvariables,\tRandom\tVariables\n\nBernoulli,\tThe\tCentral\tLimit\tTheorem\n\nbinomial,\tThe\tCentral\tLimit\tTheorem",
    "444": "conditioned\ton\tevents,\tRandom\tVariables\n\nexpected\tvalue,\tRandom\tVariables\n\nnormal,\tThe\tNormal\tDistribution-The\tCentral\tLimit\tTheorem\n\nuniform,\tContinuous\tDistributions\n\nrange,\tDispersion\n\nrange\tfunction\t(Python),\tGenerators\tand\tIterators\n\nreading\tfiles\t(see\tfiles,\treading)\n\nrecall,\tCorrectness\n\nrecommendations,\tRecommender\tSystems\n\nrecommender\tsystems,\tRecommender\tSystems-For\tFurther\tExploration\n\nData\tScientists\tYou\tMay\tKnow\t(example),\tData\tScientists\tYou\tMay\tKnow\n\nitem-based\tcollaborative\tfiltering,\tItem-Based\tCollaborative\tFiltering-For\tFurther\nExploration\n\nmanual\tcuration,\tManual\tCuration\n\nrecommendations\tbased\ton\tpopularity,\tRecommending\tWhat\u2019s\tPopular\n\nuser-based\tcollaborative\tfiltering,\tUser-Based\tCollaborative\tFiltering-User-Based\nCollaborative\tFiltering\n\nreduce\tfunction\t(Python),\tFunctional\tTools\n\nusing\twith\tvectors,\tVectors\n\nregression\t(see\tlinear\tregression;\tlogistic\tregression)\n\nregression\ttrees,\tWhat\tIs\ta\tDecision\tTree?\n\nregular\texpressions,\tRegular\tExpressions\n\nregularization,\tRegularization\n\nrelational\tdatabases,\tDatabases\tand\tSQL",
    "445": "rescaling\tdata,\tRescaling-Rescaling,\tRegularization\n\nridge\tregression,\tRegularization\n\nright\tjoins,\tJOIN\n\nS\n\nscalars,\tVectors\n\nscale\tof\tdata,\tRescaling\n\nscatterplot\tmatrices,\tMany\tDimensions\n\nscatterplots,\tScatterplots-Scatterplots\n\nschema,\tCREATE\tTABLE\tand\tINSERT\n\nscikit-learn,\tscikit-learn\n\nscraping\tdata\tfrom\tweb\tpages,\tScraping\tthe\tWeb-Example:\tO\u2019Reilly\tBooks\tAbout\nData\n\nHTML,\tparsing,\tHTML\tand\tthe\tParsing\tThereof\n\nexample,\tO\u2019Reilly\tbooks\tabout\tdata,\tExample:\tO\u2019Reilly\tBooks\tAbout\tData-\nExample:\tO\u2019Reilly\tBooks\tAbout\tData\n\nSELECT\tstatement\t(SQL),\tSELECT-SELECT\n\nsets\t(Python),\tSets\n\nsigmoid\tfunction,\tFeed-Forward\tNeural\tNetworks\n\nSimpson\u2019s\tParadox,\tSimpson\u2019s\tParadox\n\nsmooth\tfunctions,\tFeed-Forward\tNeural\tNetworks\n\nsocial\tnetwork\tanalysis\t(fire\ttrucks),\tFire\tTrucks\n\nsorting\t(in\tPython),\tSorting\n\nspam\tfilters\t(see\tNaive\tBayes\talgorithm)\n\nsparse\tmatrices,\tExample:\tMatrix\tMultiplication",
    "446": "SQL\t(Structured\tQuery\tLanguage),\tDatabases\tand\tSQL\n\n(see\talso\tdatabases\tand\tSQL)\n\nsquare\tbrackets\t([]),\tworking\twith\tlists\tin\tPython,\tLists\n\nstandard\tdeviation,\tDispersion\n\nstandard\terrors\tof\tcoefficients,\tGoodness\tof\tFit,\tStandard\tErrors\tof\tRegression\nCoefficients-Regularization\n\nstandard\tnormal\tdistribution,\tThe\tNormal\tDistribution\n\nstatistics,\tStatistics-For\tFurther\tExploration,\tMathematics\n\ncorrelation,\tCorrelation\n\nand\tcausation,\tCorrelation\tand\tCausation\n\nother\tcaveats,\tSome\tOther\tCorrelational\tCaveats\n\nSimpson\u2019s\tParadox,\tSimpson\u2019s\tParadox\n\ndescribing\ta\tsingle\tdataset,\tDescribing\ta\tSingle\tSet\tof\tData\n\ncentral\ttendencies,\tCentral\tTendencies\n\ndispersion,\tDispersion\n\ntesting\thypotheses\twith,\tStatistical\tHypothesis\tTesting\n\nstdin\tand\tstdout,\tstdin\tand\tstdout\n\nstemming\twords,\tTesting\tOur\tModel\n\nstochastic\tgradient\tdescent,\tStochastic\tGradient\tDescent\n\nusing\tto\tfind\toptimal\tbeta\tin\tmultiple\tregression\tmodel,\tFitting\tthe\tModel\n\nusing\twith\tPCA\tdata,\tDimensionality\tReduction\n\nstrings\t(in\tPython),\tStrings\n\nStructured\tQuery\tLanguage\t(see\tdatabases\tand\tSQL;\tSQL)\n\nsubqueries,\tSubqueries",
    "447": "sum\tof\tsquares,\tcomputing\tfor\ta\tvector,\tVectors\n\nsupervised\tlearning,\tClustering\n\nsupervised\tmodels,\tWhat\tIs\tMachine\tLearning?\n\nsupport\tvector\tmachines,\tSupport\tVector\tMachines\n\nT\n\nt-shirts\tproject,\tT-shirts\n\ntab-separated\tvalues\tfiles,\tDelimited\tFiles\n\ntables\t(database),\tCREATE\tTABLE\tand\tINSERT\n\nindexes,\tIndexes\n\nnormalized,\tJOIN\n\ntext\tfiles,\tworking\twith,\tThe\tBasics\tof\tText\tFiles\n\ntokenization,\tGrammars\n\nfor\tNaive\tBayes\tspam\tfilter,\tImplementation\n\ntopic\tmodeling,\tTopic\tModeling-Topic\tModeling\n\ntransforming\tdata\t(dimensionality\treduction),\tDimensionality\tReduction\n\ntrends,\tshowing\twith\tline\tcharts,\tLine\tCharts\n\ntrigrams,\tn-gram\tModels\n\ntruthiness\t(in\tPython),\tTruthiness\n\ntuples\t(Python),\tTuples\n\nTwenty\tQuestions,\tWhat\tIs\ta\tDecision\tTree?\n\nTwitter\tAPIs,\tusing\tto\tget\tdata,\tExample:\tUsing\tthe\tTwitter\tAPIs-Using\tTwython\n\ngetting\tcredentials,\tGetting\tCredentials\n\nusing\ttwython,\tUsing\tTwython",
    "448": "U\n\nunderfitting,\tOverfitting\tand\tUnderfitting,\tThe\tBias-Variance\tTrade-off\n\nuniform\tdistribution,\tContinuous\tDistributions\n\ncumulative\tdistribution\tfunction\tfor,\tContinuous\tDistributions\n\nunsupervised\tlearning,\tClustering\n\nunsupervised\tmodels,\tWhat\tIs\tMachine\tLearning?\n\nUPDATE\tstatement\t(SQL),\tUPDATE\n\nuser-based\tcollaborative\tfiltering\n\nV\n\nvariance,\tDispersion,\tThe\tBias-Variance\tTrade-off\n\ncovariance\tversus,\tCorrelation\n\nreducing\twith\tmore\tdata,\tThe\tBias-Variance\tTrade-off\n\nvectors,\tVectors-Vectors\n\nadding,\tVectors\n\ndataset\tof\tmultiple\tvectors,\trepresenting\tas\tmatrix,\tMatrices\n\ndistance\tbetween,\tcomputing,\tVectors\n\ndot\tproduct\tof,\tVectors\n\nmultiplying\tby\ta\tscalar,\tVectors\n\nsubtracting,\tVectors\n\nsum\tof\tsquares\tand\tmagnitude,\tcomputing,\tVectors\n\nvisualizing\tdata\t(see\tdata\tvisualization)\n\nW\n\nWHERE\tclause\t(SQL),\tDELETE\n\nwhile\tloops\t(Python),\tControl\tFlow",
    "449": "whitespace\tin\tPython\tcode,\tWhitespace\tFormatting\n\nword\tclouds,\tWord\tClouds-Word\tClouds\n\nX\n\nXML\tdata\tfrom\tAPIs,\tJSON\t(and\tXML)\n\nxrange\tfunction\t(Python),\tGenerators\tand\tIterators\n\nY\n\nyield\toperator\t(Python),\tGenerators\tand\tIterators\n\nZ\n\nzip\tfunction\t(Python),\tzip\tand\tArgument\tUnpacking\n\nusing\twith\tvectors,\tVectors",
    "450": "",
    "451": "About\tthe\tAuthor\n\nJoel\tGrus\tis\ta\tsoftware\tengineer\tat\tGoogle.\tPreviously\the\tworked\tas\ta\tdata\tscientist\tat\nseveral\tstartups.\tHe\tlives\tin\tSeattle,\twhere\the\tregularly\tattends\tdata\tscience\thappy\thours.\nHe\tblogs\tinfrequently\tat\tjoelgrus.com\tand\ttweets\tall\tday\tlong\tat\t@joelgrus.",
    "452": "",
    "453": "Colophon\n\nThe\tanimal\ton\tthe\tcover\tof\tData\tScience\tfrom\tScratch\tis\ta\tRock\tPtarmigan\t(Lagopus\nmuta).\tThis\tmedium-sized\tgamebird\tof\tthe\tgrouse\tfamily\tis\tcalled\tsimply\t\u201cptarmigan\u201d\tin\nthe\tUK\tand\tCanada,\tand\t\u201csnow\tchicken\u201d\tin\tthe\tUnited\tStates.\tThe\trock\tptarmigan\tis\nsedentary,\tand\tbreeds\tacross\tarctic\tand\tsubarctic\tEurasia\tas\twell\tas\tNorth\tAmerica\tas\tfar\nas\tGreenland.\tIt\tprefers\tbarren,\tisolated\thabitats,\tsuch\tas\tScotland\u2019s\tmountains,\tthe\nPyrenees,\tthe\tAlps,\tthe\tUrals,\tthe\tPamir\tMountains,\tBulgaria,\tthe\tAltay\tMountains,\tand\nthe\tJapan\tAlps.\tIt\teats\tprimarily\tbirch\tand\twillow\tbuds,\tbut\talso\tfeeds\ton\tseeds,\tflowers,\nleaves,\tand\tberries.\tDeveloping\tyoung\trock\tptarmigans\teat\tinsects.\n\nMale\trock\tptarmigans\tdon\u2019t\thave\tthe\ttypical\tornaments\tof\ta\tgrouse,\taside\tfrom\tthe\tcomb,\nwhich\tis\tused\tfor\tcourtship\tdisplay\tor\taltercations\tbetween\tmales.\tMany\tstudies\thave\nshown\ta\tcorrelation\tbetween\tcomb\tsize\tand\ttestosterone\tlevels\tin\tmales.\tIts\tfeathers\tmoult\nfrom\twinter\tto\tspring\tand\tsummer,\tchanging\tfrom\twhite\tto\tbrown,\tproviding\tit\ta\tsort\tof\nseasonal\tcamouflage.\tBreeding\tmales\thave\twhite\twings\tand\tgrey\tupper\tparts\texcept\tin\nwinter,\twhen\tits\tplumage\tis\tcompletely\twhite\tsave\tfor\tits\tblack\ttail.\n\nAt\tsix\tmonths\tof\tage,\tthe\tptarmigan\tbecomes\tsexually\tmature;\ta\tbreeding\trate\tof\tsix\nchicks\tper\tbreeding\tseason\tis\tcommon,\twhich\thelps\tprotect\tthe\tpopulation\tfrom\toutside\nfactors\tsuch\tas\thunting.\tIt\u2019s\talso\tspared\tmany\tpredators\tbecause\tof\tits\tremote\thabitat,\tand\nis\thunted\tmainly\tby\tgolden\teagles.\n\nRock\tptarmigan\tmeat\tis\ta\tpopular\tstaple\tin\tIcelandic\tfestive\tmeals.\tHunting\tof\trock\nptarmigans\twas\tbanned\tin\t2003\tand\t2004\tbecause\tof\tdeclining\tpopulation.\tIn\t2005,\nhunting\twas\tallowed\tagain\twith\trestrictions\tto\tcertain\tdays.\tAll\trock\tptarmigan\ttrade\tis\nillegal.\n\nMany\tof\tthe\tanimals\ton\tO\u2019Reilly\tcovers\tare\tendangered;\tall\tof\tthem\tare\timportant\tto\tthe\nworld.\tTo\tlearn\tmore\tabout\thow\tyou\tcan\thelp,\tgo\tto\tanimals.oreilly.com.\n\nThe\tcover\timage\tis\tfrom\tCassell\u2019s\tNatural\tHistory.\tThe\tcover\tfonts\tare\tURW\tTypewriter\nand\tGuardian\tSans.\tThe\ttext\tfont\tis\tAdobe\tMinion\tPro;\tthe\theading\tfont\tis\tAdobe\tMyriad\nCondensed;\tand\tthe\tcode\tfont\tis\tDalton\tMaag\u2019s\tUbuntu\tMono.",
    "454": "Preface\n\nData\tScience\n\nFrom\tScratch\n\nConventions\tUsed\tin\tThis\tBook\n\nUsing\tCode\tExamples\n\nSafari\u00ae\tBooks\tOnline\n\nHow\tto\tContact\tUs\n\nAcknowledgments\n\n1.\tIntroduction\n\nThe\tAscendance\tof\tData\n\nWhat\tIs\tData\tScience?\n\nMotivating\tHypothetical:\tDataSciencester\n\nFinding\tKey\tConnectors\n\nData\tScientists\tYou\tMay\tKnow\n\nSalaries\tand\tExperience\n\nPaid\tAccounts\n\nTopics\tof\tInterest\n\nOnward\n\n2.\tA\tCrash\tCourse\tin\tPython\n\nThe\tBasics\n\nGetting\tPython\n\nThe\tZen\tof\tPython\n\nWhitespace\tFormatting\n\nModules\n\nArithmetic\n\nFunctions",
    "455": "Strings\n\nExceptions\n\nLists\n\nTuples\n\nDictionaries\n\nSets\n\nControl\tFlow\n\nTruthiness\n\nThe\tNot-So-Basics\n\nSorting\n\nList\tComprehensions\n\nGenerators\tand\tIterators\n\nRandomness\n\nRegular\tExpressions\n\nObject-Oriented\tProgramming\n\nFunctional\tTools\n\nenumerate\n\nzip\tand\tArgument\tUnpacking\n\nargs\tand\tkwargs\n\nWelcome\tto\tDataSciencester!\n\nFor\tFurther\tExploration\n\n3.\tVisualizing\tData\n\nmatplotlib\n\nBar\tCharts\n\nLine\tCharts",
    "456": "Scatterplots\n\nFor\tFurther\tExploration\n\n4.\tLinear\tAlgebra\n\nVectors\n\nMatrices\n\nFor\tFurther\tExploration\n\n5.\tStatistics\n\nDescribing\ta\tSingle\tSet\tof\tData\n\nCentral\tTendencies\n\nDispersion\n\nCorrelation\n\nSimpson\u2019s\tParadox\n\nSome\tOther\tCorrelational\tCaveats\n\nCorrelation\tand\tCausation\n\nFor\tFurther\tExploration\n\n6.\tProbability\n\nDependence\tand\tIndependence\n\nConditional\tProbability\n\nBayes\u2019s\tTheorem\n\nRandom\tVariables\n\nContinuous\tDistributions\n\nThe\tNormal\tDistribution\n\nThe\tCentral\tLimit\tTheorem\n\nFor\tFurther\tExploration\n\n7.\tHypothesis\tand\tInference\n\nStatistical\tHypothesis\tTesting",
    "457": "Example:\tFlipping\ta\tCoin\n\nConfidence\tIntervals\n\nP-hacking\n\nExample:\tRunning\tan\tA/B\tTest\n\nBayesian\tInference\n\nFor\tFurther\tExploration\n\n8.\tGradient\tDescent\n\nThe\tIdea\tBehind\tGradient\tDescent\n\nEstimating\tthe\tGradient\n\nUsing\tthe\tGradient\n\nChoosing\tthe\tRight\tStep\tSize\n\nPutting\tIt\tAll\tTogether\n\nStochastic\tGradient\tDescent\n\nFor\tFurther\tExploration\n\n9.\tGetting\tData\n\nstdin\tand\tstdout\n\nReading\tFiles\n\nThe\tBasics\tof\tText\tFiles\n\nDelimited\tFiles\n\nScraping\tthe\tWeb\n\nHTML\tand\tthe\tParsing\tThereof\n\nExample:\tO\u2019Reilly\tBooks\tAbout\tData\n\nUsing\tAPIs\n\nJSON\t(and\tXML)\n\nUsing\tan\tUnauthenticated\tAPI\n\nFinding\tAPIs",
    "458": "Example:\tUsing\tthe\tTwitter\tAPIs\n\nGetting\tCredentials\n\nFor\tFurther\tExploration\n\n10.\tWorking\twith\tData\n\nExploring\tYour\tData\n\nExploring\tOne-Dimensional\tData\n\nTwo\tDimensions\n\nMany\tDimensions\n\nCleaning\tand\tMunging\n\nManipulating\tData\n\nRescaling\n\nDimensionality\tReduction\n\nFor\tFurther\tExploration\n\n11.\tMachine\tLearning\n\nModeling\n\nWhat\tIs\tMachine\tLearning?\n\nOverfitting\tand\tUnderfitting\n\nCorrectness\n\nThe\tBias-Variance\tTrade-off\n\nFeature\tExtraction\tand\tSelection\n\nFor\tFurther\tExploration\n\n12.\tk-Nearest\tNeighbors\n\nThe\tModel\n\nExample:\tFavorite\tLanguages\n\nThe\tCurse\tof\tDimensionality\n\nFor\tFurther\tExploration",
    "459": "13.\tNaive\tBayes\n\nA\tReally\tDumb\tSpam\tFilter\n\nA\tMore\tSophisticated\tSpam\tFilter\n\nImplementation\n\nTesting\tOur\tModel\n\nFor\tFurther\tExploration\n\n14.\tSimple\tLinear\tRegression\n\nThe\tModel\n\nUsing\tGradient\tDescent\n\nMaximum\tLikelihood\tEstimation\n\nFor\tFurther\tExploration\n\n15.\tMultiple\tRegression\n\nThe\tModel\n\nFurther\tAssumptions\tof\tthe\tLeast\tSquares\tModel\n\nFitting\tthe\tModel\n\nInterpreting\tthe\tModel\n\nGoodness\tof\tFit\n\nDigression:\tThe\tBootstrap\n\nStandard\tErrors\tof\tRegression\tCoefficients\n\nRegularization\n\nFor\tFurther\tExploration\n\n16.\tLogistic\tRegression\n\nThe\tProblem\n\nThe\tLogistic\tFunction\n\nApplying\tthe\tModel\n\nGoodness\tof\tFit",
    "460": "Support\tVector\tMachines\n\nFor\tFurther\tInvestigation\n\n17.\tDecision\tTrees\n\nWhat\tIs\ta\tDecision\tTree?\n\nEntropy\n\nThe\tEntropy\tof\ta\tPartition\n\nCreating\ta\tDecision\tTree\n\nPutting\tIt\tAll\tTogether\n\nRandom\tForests\n\nFor\tFurther\tExploration\n\n18.\tNeural\tNetworks\n\nPerceptrons\n\nFeed-Forward\tNeural\tNetworks\n\nBackpropagation\n\nExample:\tDefeating\ta\tCAPTCHA\n\nFor\tFurther\tExploration\n\n19.\tClustering\n\nThe\tIdea\n\nThe\tModel\n\nExample:\tMeetups\n\nChoosing\tk\n\nExample:\tClustering\tColors\n\nBottom-up\tHierarchical\tClustering\n\nFor\tFurther\tExploration\n\n20.\tNatural\tLanguage\tProcessing\n\nWord\tClouds",
    "461": "n-gram\tModels\n\nGrammars\n\nAn\tAside:\tGibbs\tSampling\n\nTopic\tModeling\n\nFor\tFurther\tExploration\n\n21.\tNetwork\tAnalysis\n\nBetweenness\tCentrality\n\nEigenvector\tCentrality\n\nMatrix\tMultiplication\n\nCentrality\n\nDirected\tGraphs\tand\tPageRank\n\nFor\tFurther\tExploration\n\n22.\tRecommender\tSystems\n\nManual\tCuration\n\nRecommending\tWhat\u2019s\tPopular\n\nUser-Based\tCollaborative\tFiltering\n\nItem-Based\tCollaborative\tFiltering\n\nFor\tFurther\tExploration\n\n23.\tDatabases\tand\tSQL\n\nCREATE\tTABLE\tand\tINSERT\n\nUPDATE\n\nDELETE\n\nSELECT\n\nGROUP\tBY\n\nORDER\tBY\n\nJOIN",
    "462": "Subqueries\n\nIndexes\n\nQuery\tOptimization\n\nNoSQL\n\nFor\tFurther\tExploration\n\n24.\tMapReduce\n\nExample:\tWord\tCount\n\nWhy\tMapReduce?\n\nMapReduce\tMore\tGenerally\n\nExample:\tAnalyzing\tStatus\tUpdates\n\nExample:\tMatrix\tMultiplication\n\nAn\tAside:\tCombiners\n\nFor\tFurther\tExploration\n\n25.\tGo\tForth\tand\tDo\tData\tScience\n\nIPython\n\nMathematics\n\nNot\tfrom\tScratch\n\nNumPy\n\npandas\n\nscikit-learn\n\nVisualization\n\nR\n\nFind\tData\n\nDo\tData\tScience\n\nHacker\tNews\n\nFire\tTrucks",
    "463": "T-shirts\n\nAnd\tYou?\n\nIndex",
    "464": ""
}